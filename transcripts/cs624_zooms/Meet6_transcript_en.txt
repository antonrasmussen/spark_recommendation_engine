Afternoon, professor. Hello, Professor. We'll get started in five minutes. Okay. Hi everyone. Welcome back. The couple of announcements here, first thing is based on certain students feedback. A couple, you know, I'm trying to organize the course material in a much more easy to understand and organized way and I appreciate that feedback. However, you know, everything cannot be arranged within a week. So the way it's little, the feedback is given. So I'm trying my best to do here and the process will go through this week as well. It's just that reorganizing the content a little bit so that we, we kind of streamline what we are learning in the class and everything is a little bit clear when we talk about the canvas. So for example, if I go to modules, let me start with syllabus. So first things, I want to make it clear again, if you are able to see my screen, I just want to make sure the first thing is again, you know, this is a synchronous class. I mean it's always been that, right? Online through Zoom using Coach collaboration tool. So if you look at the silvers, it's always been that it's, it's never been asynchronous. So it's synchronous. Of course it's not mandatory. So you're supposed to join if you can, if you cannot. Because a lot of people are working online. We have given the flexibility just for, you know, some, some, some valid reasons. You could see the recording if it's absolutely needed. But it's generally expected that you join the, the class online. Again, there is no grade for it and there is. It's not mandatory. It's up to you. But that's the format from the start in itself, right? So I hope that's very clear to everyone because there might be some confusion that this is asynchronous class and. No, it's not. That is not asynchronous class. Okay. The course content will be delivered fully online asynchronous via weekly modules. Now this is where there might be some confusion, as you know. So the modules is something that is available in online and you know, the material is posted on online. Sometimes it's done weekly, but in my classes I generally try to post everything before the semester starts. So if you have, maybe this was the reason for some confusion that it's asynchronous class. So all presentations generally I try to ask students to do online. The reason is like you have the, you know, you can look at each of these other presentations. Sometimes I've asked Students to do record the presentations and upload. Now the problem with that is like students do not see how other students are building. And I believe that that's more useful. That's most important part of the class because once you learn how students are implementing the project, you learn from each other. And that's the only reason I ask you to present it. But when we have time crunch, I do ask to upload recording so that we focus more on the lecture than the presentation. And that might happen in this course as well. For your final project, I might ask you to just upload the recording. But it's my always. The intent is try to have that opportunity so that everybody can share their work. Now, from the core structure, there was a big course structure out there. I had to update it to make it. Make it, to be honest, to make it look very simple and not too much detail oriented where that could create confusion. So here, if you look at here, right, The Intro to Big Data, right? Intro to Big Data is. Yeah, Intro to Big Data is where we start Spark Intro and setup as something we did. Then we went into Spark SQL, right? That's the third one. Then we went into Spark SQL Transformations. Then we went into Spark complex data types. This was the JSON. So this is all we have been covering. But I think I tried to break it down to match what we are doing in the class to the syllabus so that students don't feel that there is a misalignment or a misconnection here. So just making this simple, right? Then we did Spark streaming. We are right here at Spark Machine Learning. So this is a very, very big module now. So it's going to take a little bit time because it has supervised learning, unsupervised machine learning. We are dealing with big data. We start with data pre processing. So we are doing a lot of things and it's the main meat of the course because till here you're working on Spark, understanding SPARK tools. But here suddenly we go into the machine learning and then we try to apply these tools. Now once this is done, this is, this is all these three topics are more on once you know, Spark, how to optimize Spark. Right? So now there is no assignments on this, right? There is no project on this. The only thing you have here is labs related to this and most of you have done it. But I will go through the code just when we hit these topics. But to do this we need to understand basics of spark. We need to be good at Spark to actually go and start to understand what is performance tuning. And these topics are a little bit important because suppose you write Spark code and tomorrow you deploy in production how to optimize it, how to make it run faster is something experience programmer could only do it. So knowing this is very, very important. But now, now after Spark machine learning mostly what is going to happen is I'm going to take these three things and put it into the end or based on if I have time, I might just do it. But we have to finish Recommendation Engine because Recommendation Engine is the, is the. Is the main topic for your final project, right? So. So we have to cover Recommendation Engine. So as soon as we finish machine learning, I will go for Recommendation Engine just so that you have enough time for your final project. You know what you're doing, what is the topic, you know any lab, how it can help you with the project, right? And generative, a very, very small topic. We are not. We are not. It is a big data topic, right? We are dealing with big data. But here I just had to bring in generative AI, you know, just to. Just to tell you how will be the impact of generative on big data and what are the opportunities and you know, for big data. But it's not a course by itself, right? So we have a project on this just to introduce how it works with big data. But I will give a lecture about what are the opportunities of generative AI and big Data. So that's, that's pretty much the big, you know, the syllabus from a syllabus point always if there is time, I. I do want to teach graphs or some other thing. But you don't have any assignments, you don't have any labs. It's just like additional knowledge that you get out of it. But it all depends on the time and the pace of the course, right? But this is certainly the topics I have to cover and I intend to cover on time. There is no change on the grading. This always been the case and again, one thing I've been trying to tell is there are going to be three assignments. This was a Spark Fundamentals in ui, the grading. So the grading would be done within this week. This was due on July 5, so it will be out. Assignment two, grading has been already done. Maybe some of you are missing the files, have not graded it. But otherwise if you have submitted everything, the grading is already out for this. So if you have any issues or concerns, please let me know. Please communicate. If you don't communicate, I don't know. I'm not saying that I'm 100% perfect. But unless you give me feedback, there is no way to know it. So I will, I will try to fix it. Then there is a building a predictive machine learning using Spark. This is where you are focusing on. This is this assignment three. And this is I will go through because I'm getting a lot of questions on assignment three. So I will go through that in the class. Project one was, you know, due earlier and we had given enough time for this. But I students have requested me to have additional time. So that's the only reason I'm extending it. It's not because I want to just keep extending. It's because then it gets impacted for your other projects. But students are asking for extension and that's the only reason I'm extending it. It's not coming from me, it's coming from the class and I'm just accommodating it. The Project 2 is building a scalable data lake recommendation system. And this is, we need to have. So most probably within this week or definitely next lecture you will have all the material you required to start this get started with this project. And so this was always there, right? This was your Assignment 1, Assignment 2, Assignment 3, Project 1, Project 2. This husband was there and class was 10%. This was always there. The only thing that was not there was for 20%. And I've been telling that, well, I will add an assignment. But you know, what I've decided is I don't want to add anything more because we are already. This is a summer session and it's going to be extremely difficult to solve everything. I want to still focus everyone to focus on assignment three. You know, very comfortably finished project one and focus on the recommendation engine. Right. I do not want to add anything more to it, but this one is a one extended lab session. Now what it means is, see generally you are doing labs and sometimes, you know, my intention for labs was always to try to explain you the code. Whenever it's possible, I'm trying to do it. But with the lecture, sometimes it's not possible. So I leave it to you to understand the code. But what I'm going to do is one extended lab session assignment that I wanted to build to match to industry standards some of the things I wanted to discuss hands on, I'll turn it to into a lab. So all you need to do is come into the class, just execute the material with me or you don't have to write any code. It's going to be super easy. But all I'm Trying to do here is just getting us both an opportunity to discuss some code in detail and that's it. So you don't have to worry too much what's going to be on it because I'm not expecting you to write any code here. All you need to come is come here and execute the code with main class and that will give us an opportunity to work on some of the most challenging parts with big data. Right. So I will announce make an announcement when this is expected within this week. So this will be. I will tell you the details but this is the only addition that is one extended lab. It's not an assignment again. Right. So rest of the things were already done. This was the only thing that was left out. I was still deciding what to do here because I was trying to look at the AI space but now, sorry, apologies. There is some network connection issue. I hope you are able to hear me back. I'm going to share my screen back again. Yeah, so I was talking about the modules here. So if you look at this just to match the syllabus rightly and the modules. We started with the big data analysis using Spark. That was the first lecture and then so let me just go here. This is. Yes, James, please. Yes sir. I noticed on your project one as you were scrolling down that you had something about rag models and medical devices or something on your description. The project 1. Yes here or no. When you were in the syllabus. Yeah. Yes. Okay. Yeah. When I went to project one and started doing this, it was came up totally different than that description. No, that's right. So initially we thought of. I'll change this. Thanks for that. So there were multiple applications. I was thinking to build a gen AI and Rack based application. So instead of medical we took financial. But it's still the same thing. I mean it's the technology wise, it's still a gen AI model. It's still taking the rag. But you're focused more on the financial application rather than the medical diagnostic application. I'll make a change on that. Thank you. Thank you very much. Scared me to death. Sorry about that. Let me change it right here. Okay. This was module one and then you had module two that was intro to Spark setup. I'm just trying to make it a little bit simple and to match all the PDFs so it's way better organized. The third one was a Spark SQL. Then we had like Spark complex data types went into Spark streaming. We are here, Spark Machine learning. And as I said, these three modules will be in one Go. Right. It will be in one go because everything is related, connected. So within four hours we should be able to finish. Whenever we do this, all these three things, it's all about how to improve Spark queries. Now, I have not added anything in the last two modules. This will be coming up next week. This will be the recommendation engine. And what do we intend to cover in the generative way? So this is, this is all the modules that we want to cover. Anything. In addition, it has nothing to do with your labs, assignments or something. I might add some material just, just to discuss with you guys. But other than that, this is it again. You have any issues with canvas, you have any issues with modules, please let me know. I have a full intention to make it the best possible, give you the best possible experience for the remaining of the course. Okay. Okay. Then we will get started with where we left last time. And before that, let me actually. So let me ask you this question, right? Assignment three is building predictive machine learning pipeline using Spark. It is a. You're basically building a module here. Now, before I talk more about it, just. Yes. So what do you think? You really like doing it online or you want a recording presentation? What do you, what do you prefer more? I won't really. Any feedback would be great. I think from my perspective I was. I mean, I probably prefer to record, but it is useful to see maybe a handful of other students present. But watching, I don't know, 15 to 20 presentations was kind of a lot also. So okay. For me, I. I like the live presentation so that I don't have to necessarily put in the time to preparing a video. I can just make the slides, you know, which I would do for a video anyways. But it's a lot of editing and re recording and whatnot that comes along with that. Doing it live, I get the presentation practice. And then also, you know, it's just, I think it's simpler where you don't have to record something like that. So that's me per se. Okay. Okay. So it looks like there is going to be a mix of live and recording. So let me think for assignment three, let's go with live. What we will do is for project, we will remove, you know, let's do that way so that we satisfy both the preferences. For Assignment 3, we will go still with live, right? For recommendation engine, whatever the project, I think, you know, you can just record it so. So you don't have to present it live. Anyway, that's the end of the course. So you have already gone Through a lot of material. So that's fine. And yeah, access to PSP presentation code, that helped me. Yes. So I will try to ask if anybody would like to share. I will create a discussion forum where students can share their code if they want to because that's their code and they own it. So I will give a platform if anybody wants to share their code. You're welcome to do that so that your peers can look and maybe learn from that. Okay, so now assignment three. See here the focus here is, requires building a supervised classification model using various machine learning algorithm discussing classes we are still discussing. So you know, we'll be discussing a couple of algorithms at very, very top level. And here all you need to do is build a entire machine learning pipeline using it where you actually do the data exploration, model evaluation, provide clear explanations and visualization. Your records need to be 10,000 records. Now what I'm going to do is see your record based on students emails and because I'm getting multiple emails and it's summer sessions so I understand that it's a little bit faster, fast track. So what I'm going to do here is if, if just to make this a little bit simple, if you want it, I'm giving an option here to. This is. I'm making it optional. So if you want to use Spark code, you can. If you don't want to use Spark code, that's also fine. Right? It's up to you. You decide whether you want. So that should help a little bit. But I, I'm not changing any time. I'm still if. Unless you guys really want another week. But yeah, so all I'm trying to do is like removing the spark. If it's too much, if you feel it's too much, it's. It's up to you to decide. I would recommend Youth Spark. You know, it's a way to learn because anyways, I'm not putting any, you know, conditions on your accuracy. I'm not saying that, you know, your performance should be really high. All I'm trying to say is build a pipeline and how long you discuss the pipeline and final results, whether you got the results working or is the partial results or. I'm not worried about that. As long as you show me the pipeline that you implemented so you can go with Spot and if something doesn't run, you can explain why it didn't run. And that's fine for this assignment. Right? But don't try to lose an opportunity. But if again if you don't want to work with Spot that's fine. I will tell you an alternative in this class. Okay, so it. Okay, so what, what we will try to do is we will go through some theory and in the end I will come and discuss this assignment that will make it much more easy to understand and what are the expectations. So let me go through the remaining slide. Let's see again, just a quick recovery with what we were trying to do. We were trying to learn machine learning. We started with something called as collecting data. We went through some subtopics there, some data scrapping libraries. I left it to you. Then we went into the data annotation. I explained you why data annotation is so important and why data quality is so important. Then we went into data pre processing. This is a way where you take the raw data and start preparing it for the machine learning models. I did explain that there was like almost 50% of the time is spent on data pre processing. Right. You see, the data scientists almost spent huge amount of time on data pre processing. And I did say that different based on the domain, a marine data scientist, the expertise required is different from a cyber data scientist where they might need to know more on the cyber security data. Right. So both are two different things. But the algorithms, in general, the math doesn't change. I mean, you know, the algorithms don't change. I mean you might tweak it a little bit, but the intuition all are right there. So whatever the marine data scientist is using at surface level, the algorithms, most of the algorithms are going to get reused at the cybersecurity data scientists as well. But the data, the way the data is mapped to this algorithm, that's, that's the, that's the innovation or that's the tricky part. Right? So again, data, we talk about data processing, it's all about missing values, fixing outliers. Right? And doing a couple of things, how to clean the data. There are different techniques, how do you remove the data, what are the different ways. All this is given up here, right? So a lot of things here and outliers right now, outliers, I explained you what is the importance of outliers and how to deal with outliers. Assignment to excellent presentation. A lot of, lot of presentation. They showed up. You know, how to find what are the outliers, what are the issues with the data. But now the problem with assignment three is you need to fix this. Not only you need to fix this, you need to go ahead and start putting the map that data to the machine learning model and build the whole pipeline. So you are going to go do the data visual analysis. You are doing the data pre processing at least these two steps. So if you're going back, look at this. What I'm trying to ask here is data loading this anyways you're going to do and I don't think there is any issue or question you need to have in data loading. But if you still have, let me know. Initial data analysis is something you already know from your assignment too. So there is nothing again. And I've been getting questions that hey, you know, I. I've used my assignment day two data set. Can I reuse it in assignment three? Yes, you can. If you think that you can build a predictive problem using it, you, you know, a classification problem using it, then go ahead and use it. I'm not. So maybe there is some overlap. What you did in Assignment 2, you could use it in Assignment 3 as well. Yes, please do it. As long as you give me all these things, I'm not worried about the data set. All I care is like you know, at least 10,000 records. And if you're even close to this 10,000 records, please let me know. But don't come to me with the 500 record or 200 records. It's not just we are in a big data course, even 10,000 is not ideal. But because of infrastructure issues and a lot of other problems, I'm okay with 10,000 because we also need the infrastructure and a super infrastructure to deal with a very very huge data. And maybe that's where the extended lab which I'm planning, I'm trying to bring everything together now in the data cleaning. This is also we discussed in the slides how to do it, how to handle null value layer inconsistent data. Data visualization you already know because you did it in your assignment too. So 1, 2, 3, 4, up to 4, you shouldn't have any problem. It's already you are, you know, a mix of Assignment 2 and based on our lecture, these things should be already clean to you now. Now next is feature engineering. Now this should be also little bit, you know. So discuss any feature engineering techniques used like one hot encoding, scaling, feature transformation, feature selection. Now see, whenever you have data, you know, you, you. You identify your data to map to machine learning. Sometimes you have categorical variables, right? You might want to do one not encoding. You might want to rescale the data. And I'm going to discuss, I'm going to give you a, show you a library, right? So let me show you here. So I'm sharing a link in the chat. Please bookmark this link. Extremely good resource to have. This is called feature engine. Now if you look at the feature engine, what it has is. See, feature engineering is a process of domain knowledge article to create features of machine learning algorithms. Now, now it's just like a wrapper to do certain activities. Now what I will do is I will quickly show you some code here. So for example, let us see what, how does feature engine help us, right? So if you want to fix missing data, right? You have missing data in your code and you want to fix missing data, right? So there are different techniques to fix missing data. And this is what we discussed in the lab also, right? So like we look at here, you know, basically if I go here, what are the missing, what are the different ways? One is deletion, dummy substitution means there could be always multiple ways to fix minting data. So there's a sample code, the sample code, how, how in this library is used to fix missing data. So what you do, what you see here is, look at this. They are basically using pandas, right? Okay, so feature engine is a library. So basically what they're trying to do is load some data, right? This is data. So in the X1 there is some data. X2 there is some data. But if you clearly look at, there is some nans. Nans are the missing values, right? There are some nans here, nans here. Now based on the median, so what they do is like they apply median imputer. Now this is a wrapper, this is a function that this library is already built in. What it will do is it will look your data and wherever the NANS is possibly it's going to, you know, fill up those nands using this, this fit this median imputer. This is the code for that. But if, if you don't want to use median imputer, there is arbitrary imputer, right? So you could use that and it will, it will fix the nands in that way. So these are some solutions, just codes to look at it. Now one of the problem here is, see this is something you could use to do, you know in a standard feature engine is something that you could use to work on a single machine, right? But if you have to do with Apache Spark, right? If you have like very, very, very big file and you are, you want to do median imputation, you have to break it down. Now I don't know whether feature, I don't think that feature engine is something that you could use with Spark. You can use it without Spark. So you have to search how to do this, you know, these kind of things with Spark, if you're using Spark library, if you are not using Spark, then you can just go and learn how to do this. Use feature engine or some other libraries to clean your data. But I would recommend, if you're not using Spark, then try to make use of feature engine. Because what will happen is you are learning how to fix data, how to build a machine learning model, and this will at least make you okay. This is how a machine learning model needs to be fixed, built, right? You learn in this assignment and tomorrow when you are again going for the recommendation engine project, there you have to anyways use Spark. There is no option, you have to use Spark. So at that place you might learn, okay, this is how we do it without Spark. But with Spark, how should we do it? You still have to fix missing values, you still have to do feature engineering, you still have to do machine learning build machine learning model. But how would you do that with Spark, right? So this is like learning the concept and applying it on the big data is something with Spark you have to do it a little bit different, right? So Maybe in Assignment 2 or Assignment 3, you can just go with feature engine but not use Spark. Focus more on learning how to build a good model. And in recommendation engine you can choose to use, I mean you have to anyways use Spark. So there's, you can find more libraries, how to build all this using in a distributed fashion, right? So now if, if students already know how to build machine learning models and everything, then I recommend start building using Spark here itself. You don't have to wait, right? But if you're doing it for the first time, maybe then, then you just do it without Spark. Okay, so now this was a feature engine I was discussing. And then there is, there is, you know, this is about all missing data. Then there are something like one or encoder. So let's take a look at one or encoder. So the one or encoder category where a set of binary will be representing each one of the unique categories in the variable. Now this is very likely to get used. So for example, now you, you take a look at this, right? Look at this data. What do you have? You have 1, 2, 3, 4, right? And X1, that's fine. But X2, what do you have? AABBC. Now these are all string. This is all categorical data, right? This is all strings basically categorical data. And when you give categorical data to machine learning, right? It's, it's not going to work. So you have to change it to numbers. You have to encode it into some kind of numbers. So what? One way to encode it to numbers is. See you have data like A, A, B, bc. So maybe you can put for a one key like one, you can transform B as two, you can transform C as three. So basically you're putting an identifier to the categorical data, right? But it's more advantageous instead of just putting 1, B, C. If you put it like this, where you are saying that, you know, you take all the unique, unique categorical variables in this data. There are three unique, one is A, B, C. Of course there's four data items, but unique is only three, right? So you take all the three and whenever you have a right for, suppose for A, you can represent A using like this one zero zero where it means basically one is, is, is because it's A, right? And zero and zero is because it. There is no B and C. When you have B, right, something like here, right? You put one here and for the rest of the things you put 00. So this way of changing, you know, this way of turning the category variable into some kind of a binary representation which is a combination of one and zeros, where you put one where the actual variable is. And for the rest of the elements you put 0 that is called category 1 not encoding. You're taking this data and you're hot 1, 1 hot encoding it. You're transforming it to some other encoding and then you give it to machine learning algorithm. And that way the algorithm learns much more faster, right? And it runs, learns in the right way. So 100 encoding is extremely important. Again, it's part of data pre processing. See what, what is, what is again important of your, you know, you, you fix that. You let me go to the feature engine, right? You basically first fix the missing data, clean the data, find the outliers. There are a lot of things you do drop the missing data and then you have to encode your data or you have to change your data so that it is understood by machine learning, right? And one of the techniques is to, if you have categorical data, you want to change that category data using one hot encoding. There are multiple encoding techniques and it's just not possible for us for in this class to go through all the encoding turn techniques, learn everything that is out there. Because it's a feature engineering learning data pre processing. It will take months, right? So we cannot do everything. That's why I'm not putting anything mandatory. I'm all asking Is if you feel that your data has to be encoded, then do the encoding. If you have categorical variables, you can do encoding and I'm just passing you some code as a reference which you can use. And then here you have some code where basically it shows you how to remove outliers. So for example, in this code you have all this data and using this arbitrary outlier wrapper function, it will identify the outlier or remove the outlier. So this is just if you want to use feature engine, you can Google if you have outlier in your Pandas course, how to remove, you will find a lot of code out there, right? So you don't have to use feature engine to do all these things. It's one of the techniques, right? It's a library that is getting pretty much used now. You also have feature creation. Sometimes you have different type of features. You want to create new features for that. Also you could use this library if you have date functions, you have to create new features from the date. Basically extract month, year, semester and transform that. You could use date features. So this basically feature engine is mostly going to help you. Yes please, using feature engine make it easier or shall. It will make it easier. It will make it easier because already the functions are built in. So all you do is pass on your data and you know it will give you the output. So otherwise you have to write the code. So it's basically giving you the built in functions, right? For outlier detection, how would you remove outlier? You have to do interquartile range and then write some code and remove the outlier. But if you directly use a feature engine outlier handling, it will remove because it's a built in code, right? Now I'm not saying that it's the only way to do it because you can find the code nowadays in many places through AI, Google references. But this is all well organized. So if you can just take a look or the worst thing you could do is, I mean the other thing you could do is like you can come here, hey, how to fix missing data, right? So there are different ways of missing data. At least you know, these are the different ways of fixing missing data. How missing data is fixed and then you can search in Google or somewhere like or find references how other people have done it. But at least you know these are the different ways to fix missing data. And instead of using their built in functions, you try to write your own code, right? Similarly you can come here and you say that okay, I have to fix outlier. You see here There are different types of outlier handling. So at least you know these subtopics. And from here you go and say that okay, can I get some similar code to do it directly in Pandas instead of using feature in general, right? So that's how this could be helpful, right? For example, scaling. Scaling is like how to normalize your data. So if you have data, how do you do normalization? And right. So if you look at the data here, Assignment 3 I talk about in feature engineering multiple things. One thing I tell is you can do one hot encoding. One hot encoding is something when you have see you have univariate data, you have category in universe data. You could have categorical data. But you cannot give categorical data to machine learning model. You have to change it to numbers, right? You have to encode it. And one of the best ways to do is to do one hot encoding, right? There are other ways to do it. You can search how to change your categorical data for machine learning. Then you have to do scaling. Scaling is something where you know, I told you, you have to do scaling. If to basically normalize your data, sometimes the numbers are out of range. So you have to bring all your values between 0 to 1 or mean at least between 0 to 1. So you have functions in the feature engine or to do that if you are not using feature engine, then you have to search out to do scaling on your data and you can apply that code feature transformation. It depends totally on your right? How would you transform your feature? So earlier I was giving you some examples here. Let me see if we still have those examples here. See, I was trying to tell you this. This could be your feature in your date. This is just an example. You could have some date as a feature and you want to extract this meaningful information. Now why would you extract this meaningful information? Because it will give more information to your machine learning model. You just put in data as a string. It's not going to give enough information to your model. That's why you are breaking this down intuitively deriving new features and that features. You are giving it to the machine learning algorithm so that it has more information about your data. And you end up building a good model, right? So that is feature extraction and then we have feature selection. If you don't want to use all the features, it's up to you whether to drop certain features or select pseudo features. You don't have to use PCA for now. You just just ignore pca. If somebody knows pca, you can go ahead and it's just an algorithm that helps you to select certain features. You just need to plug it in. But since we have not discussed it, it's up to you whether you want to use PC or not. You can try to search for it, but all I'm trying to say here is feature engineering. All these things, right, where you are basically trying to transform your categorical data, you're doing some scaling, you're doing feature transformation. And all these 10 points is not like you apply everything here. No, it's. It's up to you. You decide. At least you show me some of them based on the context. One or two. Right. It's okay. You don't. Generally, you know, you would have to do everything in a pipeline, but as long as you are, you're showing me something, I, I understand because it's, you know, doing the complete pipeline is sometimes tricky. So again, I, I just want to, you know, motivate you to at least use one or two of them so that you know how to connect these dots. Right. Okay. We haven't discussed anything for the training. Neither we have discussed anything for the performance evaluation. So that's. I'm going to discuss next. Now, before I do that, I want to really ask the class, are you comfortable to present next week, or do you think I need to push it by one more week around 21 or 22nd? Do we have more time? If possible? Yes. Yeah. Let me, let me look at all students because I don't want to be in a place where students saying that I'm just changing the deadlines. But it's, it's. It's basically coming from the class. So one more time. Yeah, Yeah. I really need everyone. So, I mean, please let me know because if I really understand. Okay. Yeah, yeah. Please put it in the chat. Okay. Okay. Okay. Okay. Perfect. I'm going with 21st. There are enough students who have told about 21st. I'm going with 21st. Right. And let me change this right here. Okay. Okay. This has been changed. Now what we are focused here on is the training and performance evaluation. Right. So now, now, as I said, you know, I believe that a lot of students are doing. Because I don't have that information always, but I'm looking at the class. It looks like a lot of students are doing this for the first time. Right. So. So don't use spark if you're, if you're not comfortable with this, don't use spark right now. You know, do it without spark. And we will discuss a lot of things in today's class that should Help you, right. Focus on how to build a machine learning model. And once you know it, when you are going for recommendation engine, then you can use Spark, right? That, that can make things very simple. And students who have started using Spark, it's good enough, you already know things, so that's better. You learn something new. Okay, so we were discussing over the feature engine and I was trying to tell you. Yeah, this part, right. So let's focus on the training and the performance. So what we will do is we'll cover some lecture, we will go through some machine learning, some training, some evaluation techniques and, and then I will go through some code that, that should really help you to understand what are the expectations here. Right. Okay, so let me. Yeah, so see we were talking about earlier when we met last time about data collection. We discovered data processing, data annotation, feature engineering. Then we discussed, started discussing about how to break complex problems into simpler ones. Right. One was classification. We had regression similarity matching, clustering. Right. How could we break into other parts? Right. Co occurrence grouping, profile matter, Link prediction. Like LinkedIn recommendation. This is what you're going to build Recommendation engine. And sometimes you have to reduce data. So how to do that? One of the techniques is PCA Principal Component Analysis. We haven't discussed that yet. We will discuss that. And we started with. So these are again, these are techniques how to map your business problem to machine learning problems. Right. So eight most important things. Classification, regression, similarity, match and clustering, co occurrence profiling, link prediction and data reduction. Now in Assignment 3, we are only focused on this part, this guy right here, classification. And I believe that's the most important part, most of the problems, right. So and the other important is the regression. At least when you talk about machine learning you definitely need to know these two things. And as I mentioned, we're focused more on the classification part now then we started about discussing what are the terminologies for classification, right. What is data? What is classification model? What is loss function? Very important. Loss function is something like a teacher who looks at your prediction ability and then penalizes if you're doing prediction. If the model is doing prediction well then it will do less pen less penalize the model. The prediction error is more than it's going to penalize more. Right. That's the importance of loss function. And we, we clearly discussed a very important thing is what is, what is algorithm versus model. There could be multiple machine learning algorithm. At the end of the day it's set of, set of instructions, a step by step instruction to execute your code. Right. What you do in machine learning algorithms is you depend on a lot of data and your algorithm is basically learning over your data, right? And one and there is a training process that needs to be implemented. And once the training is finished, once your algorithm is learning over your data, right, A model is produced. And model is not a mathematical function. It's basically something like F of X where you give some input X and it's going to predict you some Y, right? It could be a label or it could be some other value, it could be a stock price, but it's learning a function. And algorithm is a way to learn that over your data, but you learn some parameters. Once the algorithm has learned and it's turned into a model, you learn some parameters and it turns into a function, right? For example, your ChatGPT there must be many algorithms to teach Chat GPT to generate some content from, you know, everything that it has seen in the world from Google or you know, web crawlers and a lot of data has been pushed into it, right? It maybe the algorithms kept training it, training it, training it. In the end it learned some parameters, right? Now if you look at chatgpt parameter, it could be huge. It could be just the parameters. What it learned itself could be in gbs, right? The files, it's just the metadata, it's not the model itself. Basically whenever you ask a question, it's able to send you some response because it knows the whole data and based on the data it has learned some parameters. Through that it's going to able to generate all that content, right? So very, very useful to understand what is the difference between algorithm and model. Now machine learning terminology, there are samples, features, labels, output samples are something the items you're using for either training or testing features are the, the main variables. You think that it's going to help you with your model. Prediction labels are the target variables. Output is something once you, you ask a model, hey, this is, this is a cat image, what do you think? Is it a cat image? Is it a dog image? And it's going to say something, right? That's your output. And in the machine learning terminology we have like two types of data set. One is the training data set and the test data set. We don't know as of now what is validation data set. I will discuss about that. But two, you split always your take your data and split into two parts. One is the training data set and the testing data set. And you will keep your training data set more. It's going to be 7 thumb rule is 70, 30 sometimes 80, 20 as well. So you earn 70% of your data because you need more data to teach someone. Right? It's similar to your content, right? So if you're teaching somebody machine learning, you will get huge amount of content to teach someone machine learning. And you will keep some content out there to test that student. Right. If you give away everything, then how will you test? Right? So you split your, split your content into 80, 20, teach the student 80% and then you keep 20% so that you can test the student. Right? So similarly here you keep certain data with you. I told you last time that this is something like a machine learning, you know, MNIST library. And here we have, you know, handwritten images, a data set very, very popular to teach machine learning. Algorithm detect handwritten images. So here I was discussing. If you represent number eight, it's all pixels 0 to Y, 0 to 255. It's a 28 by 28 matrix, 700784 pixels, right? And if you look at this, the range is between 0 to 256. And if you look at the density of pixels is more towards the number eight. That's how it gets formed, number eight. If it was something seven, the density of pixels would be different. Right. And the machine learning algorithm should learn, right? The spatial. So, so for example, if you're teaching a machine learning algorithm that, hey, this is eight, this is seven, this is six. Yeah, we'll keep learning because every digit has some small variations and you keep making it learn, learn over the time. Maybe you're passing 50,000 digits and it's learning. Now what is it learning? That's also important. What do you think that the algorithm will learn internally here? Anybody want to get, give a guess. If you really want to identify the difference between 8 and 1 or 2, what should algorithm learn? Stroke patterns. Yeah. So basically the algorithm should learn what are the spatial features. Right? So, so, okay, what is the, you know, how, how so spatial. So here the P, all sp, all pixels are spatially collocated. So if you understand the relationship between the pixels and the spatial arrangement, then you can predict. Okay. Because the spatials are not arranged like this. It's. It's not an eight, it may be some seven. Right. That's how you have what you learn. So as, as a machine learning data scientist, we keep passing the data and the algorithm should be capable of understanding the data and understanding the patterns in the data and then learn how to identify the differences, right? Would it be able to. Yeah. Okay, let me, let me See that we go on display settings. Yeah. Yes, thank you. Here is an example of. Yeah. Unfortunately there is some audio setting here. I don't know how to disable it. For the next time I will make sure that that audio setting is disabled. Unfortunately, I cannot and it will overlap with my audio. What I can do is at least I can here it. Yeah, we were here. I was trying to tell about the numbers here. Yeah. So the algorithm should understand the spatial collocation of the features. Right. Then we talked about supervised machine learning use. Supervised machine learning is all about. You should need to have X and Y where X is a sample data point and Y is the label. As long as you have Y and you teach the machine learning model, hey, this is the data point and this is the target variable. That is supervised machine learning. The moment you don't have a Y, in many cases you don't have label data. That's a different set of algorithms called unsupervised machine learning. Basically something like Covid clusters, right? You don't know what is a variant name. You don't have a Y over there, but you throw all the data and maybe all the samples, patient samples, where they have the matching COVID virus, they form into a cluster. You can have 10 or eight clusters of different Covid, you know, COVID patients. And that shows that, there are different eight types of variants. Right. But we don't know, we don't know what is the variant name and you know any details about it, but still can cluster them. So that's all unsupervised. So there are some problems that you want to solve using unsupervised. When you don't have date labeled data and then you have some problems you want to solve when you have labeled data. That is supervised learning, you have the right point. Right. And Assignment 3 is all about supervised machine learning. Classification problem. In case you want to do anything advanced algorithm, you're welcome to do it, but just let me know. I don't want you to face some challenges at the end. Right. So if you have any problem you want to do it, then just. I'm okay to make some changes for you. It's more advanced. Yes, that's fine. But I won't reduce this current difficulty. If you want to increase the difficulty for your own reasons, you're welcome to do that, but I won't decrease the current difficulty. So the supervised machine learning algorithm where you have. It's basically, you look at this, two clusters of data to two different groups of data and basically the super classification model's objective is to create, come up with addition boundary or line to separate these data points. Points. Right. In regression it's more about fitting the data line. Right. So that you can predict the next value. So there are two different things here now. Yeah. So what we will do is that's about the supervised machine learning. I didn't put, Let me go back a little bit. Yeah. So feature engineering, you know, next thing we discuss is a model pipeline overview. So till now what we know is basically the data based pipeline to come till here, which is the data collection, data processing, data annotation, all those things, mostly the data processing. Right. And then we just discussed about what is supervised machine learning now. Now where, now feature engineering is in addition to that, if you want to look more into data pre processing, outlier detection, you know, this is the library to use that. But now we are entering into what are the algorithms out there? Because this is also very important. Right? Okay. You do all the data processing, but traditionally there are some, you know, 10, 15 machine learning algorithms. Traditional machine learning algorithm. If you know that you cover the traditional machine learning part. Now in machine learning you again have deep learning. We are not doing deep learning based in Assignment 3. If, if time permits I might go. I, you know, in the end of the class I might go, you were some deep learning model just so that you know how to deal with that in big data. But if that's only time permits, our main focus is to see how to use machine learning and spark, right? So here one of the, you know, very, very basic models is something called as linear regression, right. In the linear regression or sometimes you can have logistic regression as well. In linear regression you're basically what you're trying to do is you are trying to fit a line, something called as Y and you have some weights and you know, parameters and you are, you are basically trying to come with the values of this W1, W2, W3 and you're trying to come up with the line. That's why it calls a straight line. So for example, see if I go back a little bit, do you think just, just out of intuition, do you think always your data should be always so clean. See what, what does this, what does this line show here? What is this line trying to do? It's trying to basically differentiate between healthy data points and disease based. So this is healthy data points and this disease beta points. Now tomorrow I throw one more point here, right? It is going to fall either here or it is going to fall here. That is right I mean, if it's going to fall here, it means it's a deceased person. If it's going to fall here, it means it's a healthy person, right? That's why you build a classifier. So you already, you want to differentiate using a line or a curve or some kind of a decision boundary where you basically try to differentiate the groups, right? Now try to tell me, do all the line, do all the data in the world is so clean, I mean, so clear that you, a line can separate both the points, both the groups, probably not. Yeah, so in this case, yeah, so in this case you have a line which is separating the data. So this is linearly separable, right? So you can have a classifier that linearly separates these points or these different classes. But in many cases you might not have a line. You want to have a, there is a curve or something else, right? It's a non linear problem, right? So sometimes you have a very linear, linearly separable data, right? And you could use algorithms that are, you know, built on that, you know, intuition, right? And you could use that to separate linearly separable data. But what if your data is not linearly separable? Then you don't want to use algorithms that are linearly built on linearly separable intuition, right? You want to build, use something else. That's exactly, you know, when you look at the 7 to 15 or you know, algorithms, you need to understand, hey, why, you know, if you look at, in data analysis, you look at your data, you clearly understand whether this data is too complex, it's too complex. Then just don't go for linearly separable algorithms, right? Go for something advanced, right? And if it's linearly separable, you might use a simple algorithm to finish your job, right? And so that's why all you do is like in this class, at least in this course at least, all you look at is a couple of algorithms. What is their intuition and how do you plug in your data? You're not going behind too much math. You are not going behind, you know how these algorithms are built. That's a machine learning class, right? That's not a big data class. Here we are focused on how to build this rapidly and build with big data, right? A little bit different here. So now, now coming back here. Now, linear model is something where you're basically building a straight line. So here you have x1, x2, x3 and xn. Basically these are nothing but features, right? So features are something like it could be age, it could be salary or something. So all this, so basically you can have multiple students and you want to basically say that the student will pass the course or fail the course, right? You have different features. X1, X2, X3 different features. You plug it in and W1, W3 are parameters that model has learned over time, right? While you are training the model and you plug in the student details, x1, x2, x3, all the features, maybe his age or his previous marks, something like that, all the features. And then you plug it in. Maybe this will give you some kind of an output, right? And maybe if it's linearly separable, then it will tell you whether the student will pass or fail. So that's a linear classification problem. And similarly you could do linear regression also. See now what is difference between linear classification regression. So if you go here, right here, this is a linear classification because you can linearly separate these two items. Similarly, here you have a straight line. It's almost the same thing. But here you are trying to not separate the data points. You have only one group of data points. All you're trying to do is what would be your next data point. So you are just building a line. So tomorrow a point comes, it will be based on this line. It will be somewhere here. Tomorrow a new data point come, it will be somewhere here, right? Or somewhere here. Of course it will not. Every time a point in the future will not come and directly lie on the line. It might be a little bit here and there, but you are coming with the best fit. So when you are building this line, there could be multiple lines, but you take the best fit line possible, that is close to all these data points, right? And if it comes out to be a straight line, that means it's a linear line, right? Linearly related data. Now what could. Now why would you build a line? Right? For regression is something like stock prices. Look at that. Like if there's all Nvidia stock prices, it was 5, 400, 500, 600, 700, everything is going linearly, right? 800, 900. There is some proportion, proportion to it. So tomorrow you, you have a new data point, it will be thousand or two thousand, right? So it falls in the line, right? Suppose it's somewhere like 1 million or something. It's changing. Your line is not going to able to predict it. So based on the history, if it's coming out to be a straight line, it means that your data is, you know, spread across in a linear fashion, right? So that's the regression. Now here, if we come. So this, this equation is Basically a linear model. It could be a classification model or it could be a regression model. The major difference is if it's a classification model then you should your output. The Y should be always 0 and 1. If it's a regression model, the output should be a, you know, a range, some value between 0 to 1. But it cannot be just 0 and 1 because 0 and 1 is binary. That's the major difference. Now the output of. Suppose you want to plug in. So W1 or W1 2. These are all parameters. These are all parameters that you learned while training the model. But once you train the model, you plug in all the parameters and the X features of a data point. It will give you a value. Now this could be some value, but if you, if you're solving a classification problem, then it should be between 0 to 1. It should be 01. If it's a linear cross regression problem, it will be a zero to one between zero to one. That's the main difference. Now there are two, two important things when we talk about linear supervised model. One is something called as logistic logistic regression. And I don't think that is here. I will just open up. Okay, before I do this, just give me a minute. One thing I wanted to ask. Are you able to see my screen, the slides I'm showing? Yes, I can it. Look at this graph here. What do you, what do you see? You, you see this is a graph, right? This is a, this is a linear. You something like linear regression. So for example, what we are trying to learn is are trying to basically come up with a straight line, right? That will you know, be very close fit to all of this data. So tomorrow based. So once you. Why, why would, why do you want to come up with a line? Basically you want to understand how this data points are spread. What is the pattern, what is the distribution. And once you learn about the line then what you could do is plug in some parameters and understand, right? Like whether what will be the new point value. For example here. First thing, when you are see if I, if I, if I change my data, right? If I, if I pick put all these points somewhere else, right? You see the, the. The line is also changing, right? So based on your data, your line is going to change. It's going to completely change, right? So the math, but however the math here you are learning is this, this, this is the equation of the line. This is Y is equal to mx plus C. Basically where M is the slope of the line and X is the intercept of the line and C is a constant. But. But Y is equal to mx plus C is something where you only have one, you know, one variable. But in general, whenever you're trying to deal with machine learning data sets, you have multiple features. You don't have only one X. You know, you don't have just age. You might have income, you might have grade, you might have something else. So there are many features. Like here, the data row has multiple columns. So you want to plug in all the columns. And when you want to see whether the student is going to pass or not, and to that you need to come up with a straight line and what is the equation, right? So here the intuition is how to build this line, right? So while. So when, when you are building a linear regression model, all you. We are not discussing again, we do not have X0. X0. Yeah. Here it will start. X0 is X1. Yeah. This feature one. Yeah. So here, basically what you are trying to do is come up with a line. Now how do you come up with the line, right? What is the, what is the optimization process? Because there could be many lines. It's not a single line. It's. There are multiple lines. Now that is all machine learning class. I'm not discussing how to come up with the best. Class how to come up with the best. You know the line here because you have to take too much time. There is optimization process, you have to go really deep. Just to explain linear regression, it takes a lot of time, right? So. And we don't have that much time in the big data class to do that. But if you want to look in, I posted certain videos now what I've did is I posted certain videos. So in your announcement, you can always go back and look what does linear, linear regression, what is the intuition? Very deeply, right? Take some time and understand what is linear regression. And linear regression is always going to give you a value 0 to 1, something like stock price. But the same line you could also use if you are using different groups of data, you want to build a classification process problem, right? You can, you can use the other equivalent algorithm, which is called logistic regression. See, one thing I just told you was linear regression. You know, linear regression is a regression problem where you are basically predicting the value of the point, right? You are predicting a value. But here logistic regression is a classification. The name is regression, but it's basically a classification problem. Why classification? It basically tells whether you know it's cancer or not cancer, right? It's a anomaly or not an anomaly, right? It's a binary Classifier, right? So logistic regression can be used to build a binary classifier, right? So here the way it. Yes, yes. So here the most important thing is the sigmoid function. See, your linear regression is just doing a regression. It's coming over the value. But in logistic regression also is the same thing as the same math and the same intuition, same optimization process. But at the end what you do is see you will get a value between 0 to 1, right? You take that value and you will plug into this function. It's called a sigmoid function. To to. To have a linear logistic regression classifier. All you do is you. You add this function in the end and based on your X value it will give it give a final output whether it's 0 or 1, right? The sigmoid function is the additional stuff you add to your linear regression. It turns to be a lot. It turns out to be a logistic regression. So both linear regression and logistic regression are very, very close. Linear regression is to use to predict a value, right? And logistic regression is to predict class that is 0 or 1. And the only big change is the sigmoid function. Now if you want to do a deep look how this whole thing is built, what is optimization process, Then please look at the videos I've posted to go deep into understanding what these algorithms are. Again, all you need to. If you really want to have a good start in machine learning, all you need to do is learn 8 to 15 machine algorithms. Really solid. It's a one time investment. It really, really solid, right? Definitely you will be successful. If you want to go more deep, deep into the research area, then you take more research based classes on machine learning. But if you want to only be an engineer, be on the engineering side, then at least learn these eight or 15 algorithms and they don't change. They're always like that. You know the basics and everything is built around it. So it's a one time investment and you should be good doing this. And once you build as, as you see once you build your assignment three, building your first classifier is very difficult because you have so many doubts. But once you built it by you know your. You do the hard work and maybe you do one more small project then you will learn that this whole thing is, I won't use a term recycled, but most of the components you will be able to understand that you just plug and play. You're plug and playing it. The difficult part is how to plug and play and how to map it to your data. Whether Professor, I believe you're talking on mute. Yeah, sorry, I, I think I had some network problem. I re. Tried to reconnect and when I tried to reconnect took me automatically to mute. So I'm just going to repeat what I said I was at, I was trying to talk about that. Like Rapids. Now Rapids as is, you know, certain kind of a data science tool from Nvidia and you will also have a data science tool from Intel. If you look at it, all these tools are trying to do is like whenever you're trying to implement the code, they make it a little bit faster to run on the infrastructure because they optimize their libraries internally like that. So the tools Rapids from Nvidia will make, if you start using the libraries, it will make linear regression run faster in GPU because Nvidia is more focused on the GPU side. If you want to focus more on the CPU side. If you want to make your algorithms work faster in cpu, then you want to focus more on the libraries from Intel. So these are certain additional tools that once you learn machine learning, how to make it speed up your algorithms or infrastructure and certain companies, they might use these tools to speed up their infrastructure. We are not worried about these tools. But yeah, just for your information, if you want to explore that. Okay, now, now we just looked at our first machine learning algorithm very, very, very, very briefly, very very, you know, at a very, very high level. It's, it's a logistic regression, that's the classification algorithm and also you linear regression, which is the equivalent of it. Right. Now before we go ahead with other algorithms, what we will try to do is we are going to cover very, very important topic out here and that's called overfitting and cross validation. Now you look at it here, the assignment till here it was all about feature engineering, right? And, but here in the training I'm talking about couple of things here and what I'm trying to tell is split the data into training and validation sets. Explain the choice of cross validation technique and how hyperparameter tuning was performed. Visualization showing the convergence process should be provided. I will explain all these things but in the, in the one of the code which I have. But before that we will look at into the theory part here. So we are entering into the training part. Say you know, we are entering into the training part, all we discuss is one algorithm for supervised classification and that is logistic regression. But there are a couple others where I will quickly discuss and you will, you will have the supporting videos, YouTube videos already sent an announcement. So let's take a look at it. And ideal module should correctly estimate. Correctly estimate known or known or seen data example labels unknown data examples. For example, look at here. See when you are, when you are training the machine learning model, what you are trying to say, you know, it could, for example, it could be a algorithm called random forest, which we will see quickly, right? So you are, you're basically trying to tell the algorithm, hey, look at this data point. There's some nights it's fun and you know, it's this length and based on the history of the data, the user likes it, right? And for this data point, the user doesn't like it. For this data point, maybe the user like it. So you are giving some data points and a probability that the user will like it or not like it, right? And you're training the model based on this. And then once the model is correctly trained, if it's correctly trained, then what you do is you go ahead with another data point, a model which it has never seen during training. And now you give a data point because Chopin Swift is not available in the training data. And this is unseen data. And you show it to the model and the model is basically going to predict, hey, whether this is available or not, right, Sorry. This is whether the user is going to like it or not. We're testing the model, right? And if you have trained it correctly, then the model should say that, hey, yeah, it looks like the user is going to like it. It might do the right prediction or it might do the incorrect prediction as well, right? Now, how do you learn appropriate values A, B, C? One of the things which I was trying to tell you earlier is as you are training the algorithm, whether it's random forest, whether it's logistic regression, you are training the algorithm. What are you learning? Algorithm is turning into a model. And when we are turning it to a model, model is nothing but the parameters that it has learned over the data, right? So, so how do you learn appropriate values of abc? What are the good parameters of this day? Data, right? So, so to do that, right, one of the, so you have to, you have to basically do that. And one of the problems in, while training, and it's a classic interview question, when I, when, when I was giving interviews, I have been asked like, you know, many, many years back. This, this, this overfitting, I think multiple interviews, they ask overfitting. Why they ask you overfitting is that exactly tells them whether you really know machine learning or not. This one question is enough to Tell you know, machine learning or not, right? So if your model works really really well for training data, but poorly for test data, your model is overfitting. What does it mean? It means that like suppose, suppose you showed 90,000 data points like of students data points, whether they will pass grade or not. So this 90,000 data points would be all the data I told you from maybe the last 25 years. And we put it in the model and the model learned everything about students, whether they will, how, whether they will pass a certain course or not. Now I am showing some new data, I am testing the model and I'm giving up a student input all his details and it's, it's predicting it false and predicting false for many other students as well. What does it mean? It means overfitting. One of what does overfitting mean? Overfitting basically means that when you are training it, see, while you are training it, you can test it also. Now when you, when you're testing, you're testing with the training data. Basically you are showing some data. You are, you are saying that whether the student or pass and you're showing the same data again and say, say what do you think? And it will say, you know, it will tell you whether it thinks whether it's the student pass or five. But it's the same data. You're training with the same data. And again you are testing with the same data. But when you finish that training and you actually show the model unseen data, it might not do well. So during training, the model might look like it's doing really, really good for all the 90,000 students, right? But the moment you fix that training and you come to testing, it might start doing bad, right? And that is overfitting. So it means that model did good in training and it is doing bad in testing right now. How to fix overfitting? One of the issues, one of the solutions to fix overfitting is you need to break your data into five folds. This is not five folds. Something called as you need to apply strategy called cross validation. Now what is cross validation? Suppose you have 100 samples of data, right? So 100k data you basically take, you split your data into 20k each, right? So I'm saying 100k training data, not the testing data. You might have 120k data points. So you decided that 100k you will use it for training, 20k, you will use it for testing. So out of the hundred, don't show the machine learning algorithm all the data at once. That's cross validation. What you do is you split the 100k data points into 5 folds. It depends how many. It's generally called k fold. But let's go with five fold where you are basically trying to split the data into five folds and in each splode you are trying to tell that, okay, you again split the data into something called as test and train. Now the test data set here is called validation data set. It's also called validation data set. All you are trying to do is out of the, out of each time you keep, you have almost 100k data points, right? Out of 100k data points. Each time you keep, keep one data, one set isolated. That is like 20k, right? And you train your machine learning algorithm, whatever, it could be a random forest or it could be a logistic regulation, or it could be a decision tree, whatever algorithm. The training process is same for cross validation. You train with the 80k data points and then you test it over the 20k data points, right? And you see how it is working. Then you again go to the second fold where you, you, you, because you, you. So you shuffle the data again and, and then you have like, you know we have four, four K, right? You have four fold data. You, you train it and one fold, you keep it to test. So you do it across five folds like you each and you take an average how the algorithm or the model performed over the five folds and you will get an average. Now that average tells you that even if you, you know, when you, when you keep your data aside, you split your data and you keep your data a little bit aside and you're doing it randomly for five folds, it basically tells you if the average is good. It tells you during training itself that okay, this model, if it is going to averagely work like this, maybe good or bad, whatever it is, but the average is, is more of a robust measure. It's basically going. If you throw all the 100k data points and you in the training, it might tell you it's doing excellent. But if you apply cross validation, there's a higher probability that it will tell you that most probably your algorithm is going to fit for your algorithm going to work fine or maybe not work so good. So that's the advantage of cross validation, that you are not showing all the data at once. You're splitting the data and testing the data randomly and you're taking an average. And that average is a more robust measure to decide whether actually this method is going to work or not. Right? So, and it will lead, it will, it will not you will not run into cross overfitting, which is basically where the model works good in training and does not work in, in testing. Now I also put an very detailed video for cross validation and training. You can take a deeper look into that. Right? So again, we, we, we, we, we are. This is a very big topic by itself if you keep working on cross validation. But all you need to know for this class and this course is, and the assignment is just, I want you to split your data and just test the algorithm so, so that you, you know what is cross validation, right? And then you can, if you want to go much more deeper, that's up to you, but definitely for the assignment. And one of the most important things during your presentation and I'm going to definitely see is whether you did cross validation or not. So anybody presenting, please ensure that you speak about your cross validation. That's one thing I'm definitely going to see that this is done on. So what is cross validation? Divide your data into n parts. Hold one part as test set or hold out set. Train the classifier and remaining on the n minus one part training set, compute the test error on test set. Do the above steps and time once for each part. Compute the average test. Right? And the average is going to give you a robust measure. Right? So this is cross validation and accompanying the video should be there so in the announcement. So take a deeper look into that. Now this is, this is in during your training. This is just one of the techniques to help you do better training of your model, right? So what, what again, just to connect the dots, keep everything connected. What are we trying to do here? We started discussing about the algorithms. We started with the linear model, which is linear regression or logistic regression. Then we talked about cross validation, which is very important, how to train your model. And now we are continuing with other algorithms, right? There are other algorithms as well in machine learning and one is the decision tree, right? Let's quickly take a look at decision tree. Now see decision tree. Now I really want you to look at the intuition, the details, you look into the video or the links which I'm providing. But in the class I really want you to understand what is the difference why somebody might look at decision tree and why is it so useful? See, when you train addition tree, what is the objective of a decision tree? It can be a regression algorithm or it also can be a classification algorithm. Depends, right? What, what, what is your objective? But it supports both regression and classification. Now what, what does regression mean? You could use diction tree to Come up with, hey, what is the Nvidia stock price? You can train your models like using Decision tree like that. But at the same time you could also use decision tree to give, to give a output whether to give loan to someone or not. Like some accredit bureau, right? It's, it's, it's taking your Social Security number and it's basically trying to tell whether to give you loan or no loan. Right? Now that is a classification. Now let's take a look at how Decision tree is built. Right now if you look at it in decision tree, at the root, it's accepting. So, so when you, when you, when you take, when you want to decide, you're building a classification model, you basically want to take some input, right? And that input could be what maybe a person's details. So suppose if I am giving, if I want credit bureau to decide to give me a loan or not, what the credit bureau will take from me, what will be the features to decide for something like cancer or not, something like this transaction is fraud or not. Like we discuss all these things before to decide somebody has a Covid or not, we all need features. Similarly to give a loan we will require features. And the features here are, okay, these are different features. But if you look at it, whether I have criminal records, whether how many years of job I've done, right, whether I'm making credit card payment on time, what is my income, these are all factors or features based on which the algorithm or the model will basically decide to give me a loan or not. Now but how decision tree works, because every model works little bit different. In decision tree there is something called as root, right? Now what is root? Root is the main one. So is the main, you know where most of the decision making is made. So here for example, in the credit card, my income could be the main split factor, right? So in decision tree you will have a root feature where it's the main split factor. So here my income will go and based on here there would be certain rules, okay, if it's less than 30, then check all this thing and if it's greater than 70k, check all these things. If this is income is between 30 to 70k then check, start checking this. So based on my root value, then there will be a split, which, which neck, what is the next feature to check? And then in the next feature also there will be some conditions. So it's always feature and conditions. And then there will be split. Split, right? So each time you're doing split and, and you're Keeping doing the split based on the decision. It's like a tree. And finally the route you take might lead you to the leaf nodes. And the leaf nodes here are loan, loan, loan, loan, lone loan, loan. Right. Sometimes you might immediately reach the decision. Sometime you have to traverse based on the data point. Right. You have to do more levels of decision making here. So now tell me, just looking at this graph in this tree here, do you think this tree is biased in some way? Yeah. Joshua, please. So you can maybe want to explain. I think it's biased because it's not looking at the criminal record at the person from the 30 to 70. Yeah. So it should be taking into account everything on all the branches. Even if it does decide not two or, or two. Excellent. That's right. So. So decision trees could be, I mean in this case specifically. Right. It doesn't make sense. Okay. Less than 30k check the criminal record. Just because 30 to 70k do not check the criminal, it doesn't make any sense. So this is biased here. Right here. So your models could be biased. See. So when, when you are coming with the decision tree, the intuition is this, but the model, how is the model building this tree? How is it building? Right. Okay. This should be the root value, the root feature. This should be the conditions. This should be the other split mechanism. So if you look at the intuition of decision tree, there are some parameters called entropy and information gain internally, how the model is optimizing, how is the model built over time? Looking at the training data, we have a video on that as well. But once the model is built, the output is this. Once the model is trained, the output is there, where you have just a decision tree. You plug in some input, it will traverse the tree, you will get an output. Right? So the output of the model is nothing but a tree where you input something and you get an output. Now how is it building internally? What is optimization process? That's all machine learning. You know, the decision tree algorithm, how the model is built. But once the model is built, it's just a tree. Now, based on the data, suppose the data is so bad, for some reason the model decided that for 30k to 70k you don't have to check for criminal records. Now this is direct bias, right? So based on your data, bias could happen and you don't even know that bias is happening. Right? So trusting a machine learning model always is also going to be a problem. And that's the problem area. Right. Now a lot of research has been plugged in where they want to make sure that the model you are using is actually responsible enough. They want to ensure that explain why it's making that decision to make some trust embedded into it. So see that okay, these models are trustworthy or not, right? So right now a lot of jobs are also in the area where they just don't want to build the model. They also want to see whether these models are trustworthy or not. So if you have a decision tree which is basically telling whether somebody has a cancer or not cancer, you don't want to just believe it blindly because it has built the model based on the data manually. Engineer is not sitting and this is not like if else rules, right? So what is a decision tree in reality here a decision tree is nothing. But it's if else rules. You have block of if else rules. But who built this if else rules? The algorithm itself built this rules. Looking at the data. It looked at each data point. It kept on shifting its parameters. Each time it shifted its parameters, it shifted the decision tree. It shifted the decision tree, shifted the if else loose. So finally once it ran everything, finally the output decision tree is different. So is the if else rules are different, right? So everything is different in the end. Now but how do you trust that this if else rules are different? How do you trust that this decision tree is the correct decision tree? It could be good for a medical user where you can. It can assist a doctor. Whether somebody has Covid or not. It can. It is a good decision tree to say that the transaction are fraud or not. So you need to put some more technology in the here, right? Something called explainable AI, something called the interpretable AI where you basically expect something evidence why it made addition One of the things like okay, today I give my input and maybe addition tree is already built Based on millions of users credit card data. One of the credit bureau has built addition tree. Now as soon as I plug in my Social Security or something, it's going to look at all the features and say that hey Santosh, I rejected your loan. Now maybe I have a very super clean record. Maybe I have a very stable income. Everything is fine. Still it rejected. Now I get really mad at this credit bureau and what I do is like maybe I go to a court and I file that okay, this, this credit bureau did not approve me and I feel this is. This is wrong, right? So what quote will do is like it will issue. It might issue something order to that scan or you know to. For the credit bureau to come up with some kind of a explanation. Now credit bureau cannot come and say that, you know, hey, we are using AI and we have addition trend tree which, which said no, no, that is not acceptable. Right? So they have to look at their decision tree and they have to have their logs which tells what is the rationale why this AI took this decision. At least they should have that. They should have some kind of explanation. Especially in areas like hospitals, especially areas and maybe cyber security, especially areas and finance, especially areas in legal. You need to have explanation. There might be some areas where you don't have any explanation, but most like at least these are sensitive areas where you definitely need explanation. And in those cases, any algorithm where you can look at algorithm and identify the rationality that are really good, for example decision tree because it has a if else conditions. It is a decision tree, you can visualize it, you can see the if else conditions, there is more possibility you can interrupt the algorithm where it's working, right? So these are good for these use cases like medical cybersecurity and all those. But there are some other models, like deep learning models which are black box models. You don't know how they work. You don't want to just take them and put them because they're really good work. Good. But you don't know how they work internally, right? Because tomorrow you need explanation. You don't have explanation how they work. You just know the input and output. It's a black box. So again there is always going to be a tradeoff tomorrow when you go for the job. Which one? Which, which is the model you select? Do you, are you selecting it for accuracy? Are you selecting it for performance? Like sometimes you have to put this algorithms in very edge devices, right? You have less battery. If you use deep learning based algorithms, they are going to use lot of battery power because they are power consuming, right? So are you, are you, what are you going to trade off? Are you going to trade off accuracy? Are you going to trade off performance, system performance or you or you want other features like explainability, interability, interpretation, right? So when you're building a product based on machine learning, multiple things need to be taken into consideration. It's not always going to be accuracy. There are going to be multiple things and based on your use case, based on your business requirements, you will choose the best algorithm that suits here, right? Yeah. So again we are just discussing algorithm, but I wanted to capture multiple points here because always I do. When we talk about decision tree. Right now coming back, this is the second algorithm which we are discussing and I've been telling one thing repeatedly that the Output here is addition tree and it's if else models, right? If it's conditions. So once the algorithm is trained right, you have a model. What is a model? Right? Now a lot of students might have this confusion. Hey, we learned logistic regression or we learned dictionary. Tomorrow you learned random forest. But what is the output? You put an input, you're getting an output. But what is the output from the algorithm? You are saying model. But what is a model? Is it a file or is it something else? Of course it's a file. But what does the file consist of, right? What do you think here the file consists of? It's just if else roots, right? Yes, it's a tree. But tree consists of. Logically, it's a if else roots, right? In logistic regression, when you go to logistic regression, logistic regression, what is your output? Can anybody tell me what is the output of logistic regression? You were building a line, right? Remember you were building a line or building a line in logistic regression. So what is the output here? A binary classifier. Yes, a binary classifier, right? So that's a classifier. But what is the output? What is the model? Model? So in, in the. When I'm talking about decision tree, I'm telling that if else conditions is my model, right? What is my model here? Slope of the line. Yeah, very close. So it's the equation. The equation is nothing but your model here in decision tree you have. Do you have equation there? It's a false conditions, right? So it's also important to know what is the output. Finally, what is your model? Is it a. It is a. Is it a equation? Or it's a if else conditions. Or it's a bunch of if else conditions, random forest. So next algorithm which you are saying to see, it's. It's a, you know, set of trees. It's not just only one decision tree, but has random forest has multiple decisions, right? So always this is what I really want you to capture math and everything you can go detail. But if you really need on your own. But for me in, in this course, I really want you to know what is the model? At least at a very high level. What is the intuition why this algorithm is being used right now? Now I will quickly won't go through much of the dictionary because we have a video for that. But look at this how the decision tree is. But decision tree generates the output as a tree like structure. Build top down from a root to node, breakdown data set into smaller and smaller substrates and associate decision trees incrementally developed. So you keep decision nodes, you building leaf nodes, right? Add more branches. The topmost decision nodes is a tree which correspond to the best predictor called root node. What is your best predictor? In this case, every problem has a different. In this case, maybe somebody is going to eat. You know, in this case, age might be the best predictor. In this case, income might be the pre best predictor. In case of COVID fever might be the best root node. Who what is the root node? The topmost D node is the root node, right? So and while building, you know the tree, there might be shift of these root nodes and you know the these nodes. That's the optimization process. So how. This is a algorithm how to build the tree, right? First select the attribute for root nodes. Then split the data into repeatedly recursively do the branch. I'm not going in detail something you can take a look on your own if you, if you want to. We are not testing you how to build addition tree again, right? So but all you need to know is like there's going to be root node and every. There would be decision nodes and there would be some leaf nodes and see how one one of the most important things in decision tree is called entropy and information gain. Now to clarify one thing, see in decision tree what happens is every, every node you are going deeper, right? You are reducing the entropy. Now what is entropy? Entropy is the uncertainty, right? So suppose you are giving a data point. You want to decide on something whether to give loan or not. So more deeper you go, your uncertainty towards decision making should be reducing or it should be increasing. Entropy should be reducing, right? So the goal is always to reduce the entropy at each node, right? And so that's the intuition here in decision tree. So entropy is nothing but the measure of disorder or uncertainty. So the more deeper you go at each node, you want to reduce the uncertainty with that data point, right? So based on your feature or whatever, the decision node, you want to remove the uncertainty. So the algorithm is basically a recursive process where you keep, try, keep building the tree. And you want to ensure that each node the uncertainty is decreasing, right? And entropy is the main measure. So to. To. To build this tree right now, now again you, you have to please look into the video which I posted for decision trees. I will double check after the class if again if you have any confusion. Because you know, just covering the all the algorithms by detail itself is going to be extremely hectic. It's just not possible with the nature of our class. But that's why I'm giving you some algorithm additional videos to look into, deeper into that there is. Let me see whether we have random forest also here. Should be here. Okay, I will go into the evaluation once I finish the random forest. So just give me a minute. Yeah. So see, till now what we learned was, okay, you have a decision tree and addition tree is going to give you an output whether to give somebody a loan or no loan, right? But sometimes these are another classifier. So all we learned right now is addition tree. There is something another called as knn classifier. There is another classifier called Navy's basis. Remember I told you all you need to do in at least in traditional machine learning is learn seven to eight machine learning algorithms, how they work internally and you should be good there. So. So these are some additional machine learning or classifiers that can help with classification right now. Now sometimes what happens is suppose take a look at, you know, suppose you have a single decision tree, right? Now what it does is you can ask, hey, somebody has Covid or not. It might say yes right? Now do you want to simply go with that answer, you know, or you just want to depend on the decision tree? Or you take more eight decision trees, right? Separate decision trees, separate models. The algorithm is same, but the. The way it is trained, the data it is trained on might be a little bit different, right? And you take the decision of each decision tree and come do a majority weighting of a all the decision trees, right? And then you say that you know that go with the majority addition, right? That is something called as bagging. Now in bagging, what happens is you take the whole data set. Consider the data set s where you have Xi and Yi pick a sample S with replacement size of N. So it could be like suppose out of 100k, you have to, you know, 20k. Now. Now you train the estar. This is your sub data set, right? You take the and build a classifier F. So you maybe build with only 20k of data. You're building classifier F which is addition tree one. Now you. Until you finish all the remaining data sets, like let us say, you know, till you more four decision trees. You build totally four decision trees. And then you ask once all four decision trees are built, then you ask each decision tree whether you you give unseen data samples, a new, new patient sample and say that hey, decision tree one, what do you think this patient has Covid or not? That may say, yes, it has co, right? So you take that decision and you take the decision of all the four Decision trees and you do a majority voting, right? And that based on that majority waiting, you, you go for the final result, right? So that is bagging. So it's not just one classifier. You're depending on a bunch of classifiers, right? And if you're, if you're all, all classifiers in your bagging. Bagging is basically bringing multiple classifiers together to do majority voting, right? But these classifiers could be anything. But in, in these classifiers, if all those classifiers addition trees, they are only decision trees, they become random forest. So random forest is nothing but a bunch of decision trees. It's not a single decision tree. You have, you have many decision trees, independent decision trees, and then you do a majority voting, and that is random forest. So random forest is much reliable algorithm. So even one decision tree goes wrong, you have multiple decision trees, right? Random forest can handle noise, it can handle nonlinear data. It is not robust to missing value. It is robust to a little bit of missing values as well. So random forest is really good. And I asked many times my students is like, you know, if you're working for the first time in a machine learning project, try to work with random forest. It gives you good experience and mostly your output would be also good. So random forest is something. Again, please take a deeper look based on the video I sent. And if, and again, bagging is a technique, right? If in random forest you have multiple decision trees, but some. And if you are not using, you could have, you could plug in other, other models as well, like knn naive base. But that is not a random forest. Because random forest, that is in general bagging. But in random forest all models have to be diction trees. That's, that's very much mandatory. If you are going with random forest, then all trees should be addition trees. Okay, we'll look into this extra boost later. Okay, now one thing. See till now what we are trying to do is see you prepare, you, you have your data sets, then you do data processing, right? Then you might do little bit of feature engineering, right? Then you might select an algorithm. It could be any algorithm. It could be addition tree algorithm, it could be a random forest algorithm, it could be a logistic regression algorithm. Again, I'm not asking you to write the whole algorithm. All you are going to use is a API and we will show you how to use that, but you will use this algorithm but one, and then you will train your algorithm. One of the training process. You know, one of the important training technique when you're training it is cross validation that I really wanted you to do. But once you have the model, it's already. You pick the model you did. You plugged in your data, the transform data. You have a model, you need to test them, right? This is also important. So if you go back here in your assignment three, feature engineering, you do the training and then final one is the performance evaluation. So you need to do the final performance evaluation. And here there are multiple things you need to generally do when you are, you're evaluating your model. But for this assignment you need to only focus on, focus on one thing and that is the accuracy and the confusion matrix. You don't have to do many things. All you need to do is come up with the accuracy and confusion matrix. And again I will show you this include, you can refer to that code. So visualization, for example here, suppose somebody built a classification model and its objective is to do predict, cat, dog, rabbit, right? Maybe you give you building an image classifier. And once you build an image classifier, you give it to the classifier, the testing data you have like collected like around certain samples and you are giving it to the classifier. So it should, it should identify based on image whether it's a cat, dog or rabbit. Now can anybody tell me whether it's a binary classifier? You think this is a binary classifier problem? No, it's not a binary classifier problem. What is a binary classifier problem when you have only two classes? Anomaly, Anomaly or non anomaly. That's a binary classification. Cancer. Not cancer. It's a binary classifier. What else is a binary classifier? Covid. Not Covid. Tumor. Benign. Malign. Right. Is this a binary classifier? You have three classes, Cat dogs. Yeah, three. Yeah. So it's, it's, it's a multiclass classification problem, right? It's a multiclass classification problem. And yes, algorithms like random forest, D tree, everything can do a multiclass classification problem. Now what happens is, suppose the model is ready, you have trained it and it's working, you know, now you want to test it. Now what you did is you took what this tells us here. You, you took basically this is the actual classes and this is the predicted classes, right? So, so out of the actual classes, five classes were actually predicted as cat, right? And out of so totally you gave eight. This is 8, 5, 3, 0, right? This is the total number of cat samples. You, you gave it to the, this after training the classifier, you gave almost eight samples. Five, three, zero, right. Total eight samples to the classifier. Out of the eight samples, five samples were actually Cat. And you know, the five samples were correctly identified as cat. Right. So the predicted class was correct for five samples, but for three samples. Right, three samples. The classifier predicted as a dog. So it's called misclassification. Right. This is misclassification. There's actually cat, but it's getting predicted as dog. Now, now for now, when you show two dog samples out of order of two plus three plus one, how much is this? Six out of six dog samples, two were correctly identified as dog. So two, two were misclassified as cat, three were correctly identified as dog, and one was misclassified as rabbit. Right. And out of 13 samples you gave as input to the model, zero got predicted as cat, two got misclassified as dog, and 11 got correctly classified as rabbit. Right. Now what does this confusion matrix is basically telling us? It's basically telling us out of the total samples you gave how many samples? First thing it's telling us what are the total number of samples testing samples you gave it to the model. How many got correctly classified or not? That's the first, you know, conclusion from this confusion matrix. It's telling us how many got correctly classified. It's not only telling us that, it's also giving us another information which is very important, which is basically telling that how many of them got misclassified? Right, but how did it got misclassified? If I. Out of eight, some, some, some cats got misclassified, but how did they got misclassified? They got misclassified as rabbit or they got misclassified as dog. So what is confusion matrix again telling us? It's. It's basically telling us how many correct classifications happen. The second thing it is telling us how much misclassification happened and what the actual sample got misclassified as. Why do you think, see, main interest is to identify how many samples got correctly identified. That is our main interest. Okay, now, but why do you think the second point, which is like how many cats are getting identified as dog is really useful to us? Why do you think so? Is it really useful to us? What it is if it's a multiclass classification problem like this, if cats are getting identified as dog, or maybe, you know, some rabbits are getting classified misclassified as dog, is it going to help us to make this model better? Yes, we. This way we know about which classes look similar to each other and cause. Absolutely. Yeah. So confusion matrix is very, very useful here to help us understand which classes are very, very similar. Right. Or because now we can say that, okay, maybe certain features in cat and dog are similar, right. That is why they are getting misclassified. So we need to do a data pre processing little bit change the way we do data processing or change the way we want to add more features a little bit that will decrease this confusion. If you don't know, look at the confusion matrix and you look at the final accuracy. Like how many got misclassified out of the total number of classes given. That won't help you to do your feature engineering again to really know what is causing the confusion. Just looking at this confusion matrix you might say that, okay, that features of cat and dog get overlap, right? And then you can go back and do your feature engineering. That is the main advantage. Now you might say that, hey, you know, for this it looks what, what are the other applications like? So, so look at maybe if you were talking about cyber security, right? There are multiple malware classes out there, right. So and if you built a multiclass classification where you say that as soon as the network sample comes in, right. You might classify that into one of the malware classes. It could be ransomware or it could be something like. I'm forgetting the different malware names. Does anybody know different malware names? And not top of my head. Yeah. Anyways, one is ransomware and let's say malware two for now. Right. So yeah, sorry, not payto, paytio. Okay, so, so, so yeah, so one, let's say random where the another one was Mirai. Yeah, Mirai was a very famous botnet. So let's say Mirai. So and let's say malware three. Right. For now. So we have three different types of malware, ransomware, Mirai and malware three. Now if you put in all this, you have some network packet coming in, you want to classify whether it's ransomware, whether it's Mirai or it's something else. Now why would you do that? Because once you identify what kind of malware it is, the network administrator might take an appropriate action based on the malware it is. Now if you misclassify the malware, if you misclassify ransomware as Mirai, your network administrator will take a different action, right? And your Mirai will by then would have done the damage. Right. Now if Mirai and your ransomware has features the bytes or network packet very similar, then there will be a lot of confusion. So your network anomaly detection or malware detection software will not be able to identify the malware properly. You have to go back and look at the confusion matrix. From the confusion matrix you want to see what features are getting overlapped at least at a very high level. And you might do a little bit feature engineering, right? So confusion matrix extremely important. So look at what is overlapping, what could be the classes to focus on because your overall. So if you don't look at confusion matrix your you, you will just see that your over overall accuracy is down. But when you look at confusion matrix it tells you exactly where is the problem area so that you can focus and do better engineering, right? And if your confusion matrix is really, really good, this heat map, right, it's going to be really good. This one it's going to be much darker. All these. So this is like almost 10 classes here, right? It's a 10, it's 10 different classes. So all this is going to be high because the numbers here represent which actually so here it's like class one actually got classified as one. So it could be darker. Class two, the number of classes of class two actually getting classified as class two, this should be higher. So all if the diagonal is really, really, really high, higher in numbers, all this will be in darker shade. And it should be our ideal confusion matrix. If all darker sides are here here, it means that basically your confusion matrix is bad, right? So your accuracy is bad. Okay. And then you. We have something called as true positive, false positives, right? True negatives. Now so basically if you have here like let us say five true positives, right? Actual cats that were correct classified as cats. So these cats that were actually correctly classified getting as cats are called as true positives, right? Cats that were incorrectly classified as dogs. Right. So these cats that were incorrectly classified as dogs, these are called false negatives, right? Now dogs that were in classified, dogs that are in classified as cats, that's here that are called false positive and true true negative. But for at least you know, you should know what this definition means. This is a little bit confusing. For the first time you look, please take a look at at least Google these five. What is five true. What is true positive? What is false negative, false positive and true negative. But the most important thing is at least for this course know what is false positive, right? Focus more on what is false positive. We'll come back to evaluation. I'm not discussing the other graphs. It's a little bit complex. But I'm going to come back in another class. I'm going to focus more on the false positive also there, right? But, but for today we are not going to discuss these, these other graphs. And these are very, very important graphs. And I'm going to tell you why because always accuracy is not the best metric. The number of, you know, classes got correctly identified as the correct class. It's not always the best metric. There are multiple reasons. So there are other metrics also in machine learning which we have to see that gives a, you know, good measure of quality of the model you have built. Right? So we still need to see that. So I'll stop the evaluation section here just at the confusion matrix and I'll just do a quick recap of what did till now. And then I want to go to the code because that's the one which is really going to help with your assignment. And we'll, we'll continue these discussions in the next class as well. So the first part, as I said, are, you know, if we go back, right, in Assignment 3, you load the data, right? You do the data analysis, you do the data cleaning where you can use Feature engine also to do. If you're not using Spark, you can use Feature Engine. Or if you are not, even if you want to use Spark, you can look at Feature Engine just to see how they, how they clean it. And you can look at the keywords and from the keywords you can get the new code right to take reference codes and then try to include it. How to do it in Spark if you want to, but the process doesn't change. See how to fix null values. Whether you are taking an average, you are taking a replacement. That doesn't change whether you score Spark or this, right? But when you're using Spark, the implementation is going to change, but the concepts doesn't change. That's what I'm trying to tell. If you have outlier, you will. You have to deal outlier, whether in Spark here also. But fixing of outlier, what libraries are supported, right? You have to check in Spark data visualization, feature engineering. Then once you do the feature engineering, you have to do the training vision where you basically spit the data and plug into an algorithm, train the model, right? And then you do performance evaluation where you are basically trying to show the accuracy and the confusion matrix, right? And then you do the conclusion. Now from a lecture point, just give me a minute here. Now from a lecture point, we have discussed the concepts, but let me go to the code because that is something going to help more. Just give me a minute. I will stop my sharing my screen. Okay, so this is something you have already done in lab four, but I'm just going to give you an example of this. So before I do anything here, just again. Yeah, so here see one of the libraries when you're doing to build machine learning models and traditional machine learning models, not deep learning models. A very, very famous library is called scikit Learn here you can do classification problems, you could do regression problems, you could do clustering problems, right? And I'm going to put this again in the zoom just so that have access to this and this. But this library is not, is not useful. When you build machine learning models on Spark, you have to use mlip, right? You have to use mlib. But as I said, you're free to use Sklearn. If you want this to learn how to build machine learning model. When you become good at it, then you, you can do more on Spark because then you, you learn like how to use distribute work on distributed setting. Now when you do classification, this right there are you look at here, right? In supervised classification there are multiple linear models. Of course we, we are not worried about all the linear models. I just explained you logistic regression, right? Then you could do, then there are other, other, you know, there is K nearest neighbors. There are many, many, you know, ways to do it. There is naive B. We, we just discussed very high level, what is distant trees, right? Then there could be random forest also here somewhere in the bagging random forest here, right? So there are multiple algorithms to do there. Now of the thing is, okay, if you're using a logistic regression, suppose you're doing a logistic regression. This is a logistic regression. And you know, in logistic regression, if we. Yeah. So here, if you look at logistic regression or yeah, here if you look at, there's some kind of, you know, classification being done using logistic regression. But who is implementing the logistic regression here? The Sklearn library will have already implemented the code. All you need to do is import, import the regressor and fit it into your model and you're done with it. So you're just using it as an API, right? You're not actually calling, you are not actually implementing the algorithm, right? So Sklearn has done all the implementations for all your machine learning algorithms. All you use it is you use the training mechanisms. You use how to train, how to fit your data to this model, right? You don't have to worry about the implementation of the algorithm itself. You are worrying about the implementation of the model right? Now similarly, when you go to mlib, MLIB would have implemented similarly the random forest, the DISH entry, the logistic regression for distributed computing, right? It is not the same implementation. See, Sklearn has implemented this algorithm because it's a Standalone machine, right? This algorithms will be executed on a standalone cpu, but there the algorithms have to be distributed in the cluster, right? So it's distributed computing. So your algorithms have to be tweaked to the distributed setting, right? And that's why MLIB will have its own implementations. One thing students get confused is SK learn the, the algorithms and MLIB algorithm implementation is same. No, they are both different because this is standalone implementation and this is distributed implementation. Right? You have to be very, very clear about it. But the intuition is same again. I mean, random forest is still a collection of decision tree. Decision tree is still a collection of, you know, is a tree that is built on entropy. Logistic regression is still about linearly separable data with a sigmoid function that doesn't change. The main intuition doesn't change. But how to make it work in a cluster, right? You have to tweak the math or tweak the, the way it's collecting the data. And that's, that's something changed in the MLIB library, right? The more you want to know, more you want to go and open up these implementations and I highly recommend, after you finish this course, that go and start looking at the implementation, how they do implementation. For example, here, the random forest, the way Sklearn scikitlearn would have done it, same random forest, Amazon would have done their own implementation. And it might be proprietary, right? Intuition is same. But what works for their customers, you know, they might have changed the random forest a little bit different and that might be proprietary code, we don't know. And that might be useful for their use cases. Okay, so now coming back to the code. Now look at this code. So what we are trying to do here is very, very simple code. Yeah, we are loading the data. So there is some wine data set, right? We are. And the data set is available directly from the Sklearn data set, right? We are loading the data, we are seeing what is the X, what is the vine Y, right? And we are printing just. We are doing some initial data analysis and we are printing it, right? There is nothing fancy over here loading the data and doing the data analysis, right? So I go here, what do you have? So loading the data and data analysis, Very, very simple thing. Not much. I mean, of course, of course you want to make it fancier with many things. You have already done this part. So we are here doing just the data analysis thing, right? It's going to just show some data, univariate analysis, bivariate analysis, right? Then the cleaning process starts. Now you can take this as a reference code, but you have better code available. This is way back, right? So you have very, very best code available out there. If you look at feature engine how to handle outliers, how to you know, the very good out code out there, try to use that. This is just for reference. That what I expect you to do, right? And again I. I don't know whether you have outliers or not. I don't know whether you're. But I just want to see whether if you know your data had something and what did you do about it, right? So. So try use your best judgment and if you're giving some observations, just be realistic about it and, and you know, be honest about it. That's all I want. If you don't have outlier, okay, but what did you do? Did you at least try to find it? If you're not getting outlet, that's fine. But what was your way to find it? Right? That's. That's what I'm interested in then actually going and finding the outlier. I'm more interested in what did you do to find out the outlier and if you did find the outlier, what did you do to fix the outlet. Now whether that worked or not worked, that's a different story. And we don't have really in two weeks. You cannot learn averaging. So. But, but that I leave up to you, you know, later. But at least you should know how to connect these things. There's a box plot and then you start cleaning data cleaning. So this is all part of your data. You know, data data cleaning, data visualization. So till now nothing special. You are just using some libraries how to you do analysis, how to do do some visualization. And as you did your assignment 2 you. You are going to speak about that observation is the multivariate analysis plot, right? You. You still have to do that in your. But one thing that is important is sometimes you can have class imbalance. Your target variable Y could be class imbalance. You could have lot of fraud data. You can have very, very less fraud data, right? So suppose. Suppose you have only 2% of fraud data. 2% of credit card data is fraud. Rest of them are really good data. And you build anomaly detection system, right? And, and you are only focused on the metric accuracy. If you get 98% accuracy, do you think you have built the best anomaly detection system? No, because you have only 2% fraud. So all you want to catch is that 2% fraud data. Rest of the thing anyways you are going to non fraud so it's going to give you a high accuracy. Right. So but that's why accuracy is not the right metric. For many cases you have to go with especially when you have class imbalances. Right. And we have, you have to study. We talk about those metrics in the next lecture. But you know, accuracy is not the only metric. I want to focus again for your machine learning models. Okay. Here you're just showing the class imbalance sometimes. Now these are some activities for your feature engineering or you know, it's up to you. But what I'm trying to do here is sometimes you have class imbalance, you can do one of the. To fix class imbalance you can use Smooth SMART is one of the techniques. Again, you're not writing code. All this code is available out in a lot of places. All you need to know is what to use, when to use rest of the things. You have so many tools to. I mean we are, we are in a place where code has I mean very, very less importance. It's more important is the concept and how to make them connect because code is going to get generated and that's, that's the reality. Right. So but, but what to put, where and why to put it, right. And if you are just depending on your GPT models to do that, it's not going to work. We are GPT models is not right there yet. It could also sometimes be the content could be wrong. So you have to be very extremely careful where you are putting, why you are putting and you should have that rationality to combine things. And then here SMART is one of the techniques to suppose one of the. Your target variables is like suppose you have less fraud data, right? You want to increase the fraud data. You can based on your original data fraud cases you can increase the. You can add append synthetic data similar to the. It's not the duplicating your data but it's something like adding looking at your class data distribution and generating something similar so you can increase the underrepresented samples. You have very less number of samples for fraud data. Look at the fraud data and generate something similar so that you have more data points towards that so that you have more information for your machine learning. Right. One of the techniques is SMART in some kind. Some things you don't use smart you can just do majority over sampling or under sampling which it basically means that you're increasing randomly increasing the number of samples of your majority minority class. Right. So this is all this is how to deal with. This is the situation when. How to deal with your minority class, when you have in class imbalance, there are two things you do. Three things you do. First is either you do over sampling. Over sampling is you increase the majority class by duplicating the data you. You do under sampling, which is basically whatever your majority class is, you. You decrease it so that you. You bring the majority and minority classes at the same level, right? Either you increase the minority class or you decrease the majority class. That is two ways. The third thing is you generate synthetic data. One way to generate synthetic data is using smote, right? You use smote to increase the minority data so that you have class balance. You have fraud and non fraud data at the same level, right? One thing you could do is smart. There are other ways you could use GANs. There is no end to it. There is many, many techniques out there. So you can look the way you want to do research on this is you want to go and look at what are the different ways to do fix a class imbalance, right? And if you have class imbalance, there are traditional techniques you want to use and fix that. Apply pca. Don't worry about it. We haven't discussed it. So you don't have to apply pca. And here a Standardization. What is standardization? Basically you want to. You give all your data, right? And you build a standardizer. It's a. It's a wrapper at a scalar you use and it will scale your data. So you have all your data spread across the range is high and low across. It will bring everything in similar range, right? It will automatically do it. You can just use standard scalar and fit your data inside it and it will scale it, right? So this is all part of your feature engineering, right? And then you can, in feature engineering, as I showed you in slide, you can look at your current features, you can add new features, you can transform your new features. I put some video about that as well. Feature engineering. So you can take a look in that if you need any extra material. You are saying that, you know, if you find that, you know you need more material about it, let me know, right? But again, as long as you do some, some things in this feature engineering, I should find, I don't want the complete list because it's not possible to do the complete list. Then you have cross validation. So once your data is ready, right? You so see till now what you did, you loaded the data, you did some data analysis, you looked at the plots, right? So you know good things about data. Then you thought about, okay, your, my data is less here. So let me increase the data, right? You did the plot and you decided that you want to increase the majority or minority class, okay, that is fixed. So you're working with data. The next step you do is like, okay, the feature engineering, you think that, okay, maybe the date features you want to extract the day, date, the month, that might get more information you are extracting. You're doing more feature engineering. You think that a Fourier transform, you know, you're applying some functions on multiple features might give you more information. You do that, all that feature engine. But once you are done with all this, you've prepared your data, you transform the features, your data, you think your data is ready. That's where you go into the training phase. Now what happens in training phase is you basically take your data and you can split your training data. First you take whole complete data and take the complete data and split it into 80 to 20. One is for training and one is for testing, right? Then whatever 80% of training data you have, you take that and split. For cross validation you can do a five fold split or you can do four fold split. In this case, it has done five fold split, five split. And what you do is you take only chunks of each split and plug it to an algorithm. Basically here in cross validation you are creating subsets of data to sometimes you hold the data right and you have other data to test, right? So these are your cross validation data sets. You're creating the cross validation data sets here. And this is, this is the summary of your cross validation. You are not doing any training yet. You are just split your data. You split your training data into five fold, right? And in each fold you have training set samples and your validation. Test samples. Training set samples, validation set samples. So you have create your data is now organized and split to do the training. Now, now for training, actual training. What you're trying to do here is, see, I put in multiple algorithms here. Am I right? Implementing any algorithm? No, I'm not even implementing a single algorithm. All these algorithms from here to here is API calls, simple API calls. Sklearn has implemented all these models. I'm just calling, importing these models, right? I am putting all these models here, right? And then what we try to do here is we perform cross validation and cross quality. The most important thing here is here in the cross validation you are basically, you know, you are sending the training data, the testing data, right? And you are printing the accuracy of each model in training. So this code will actually plug in all your data, which you have Created and broke, broken down into sets. And each time it will plug into all these models and then it will test what is the accuracy. So you can see that logistic regression has 97% accuracy. Dictionary has like 91% accuracy. So some algorithms will have more accuracy, some algorithms will have less accuracy, right? Right. This is, this is the point up to where you take the data, you choose certain algorithms which you want to train. You are not implementing the algorithms, you're just using the API. You fit your data, all this very simple line of code. You fit your data into the model, right? Once you are done. Now this is something, it's an option, you know, in the assignment. I've said in training how hyper parameter tuning was performed. So you don't have to worry too much about it. If you can take a look more into it. Now what is hyper parameter tuning is. See, when you have decision trees, right? When. Suppose you want to train your data on decision trees. Now decision trees has its own configuration, right? The number of trees you want to use. So the number of nodes you want to go, the depth of the node. This is some plug and play parameters or configurable parameters for every algorithm will have certain configurations, right? For your data, what is the best configuration for the parameter? How would you decide? For. Suppose you are using random forest. How many trees? Random forest is nothing but a bagging technique, right? You have collection of decision trees. But how do you decide you should use 10 decision trees or eight decision trees. And within each decision tree what should be the maximum depth, right? How do you decide all this? So everything is a configuration in the end, right? So for every algorithm, it has its own logistic regression at its own algorithm K nearest neighbor is basically you have a configuration value for K. So every algorithm has its own configuration values and tuning, tuning the values for these configurations specific to your data is called hyperparameter tuning. What is the best configuration values for your data is hyperparameter tuning. The problem we have here, if you do it manually, if you, you know, you say that, hey, for random forest, first let's try with 10 decision trees, right? Then you say, hey, I got only a 95% accuracy, let me try with five ratio entries, right? So if you keep doing that, you will, you will never end, right? So how to come up with the best parameters, right? So one way is doing the grid search. Grid search is a technique where you basically try to give some values and you know, you leave the training process and the, the training will keep on going based on the different values you have Provided it will plug in each value you provided for that algorithm and see whether it got the best results or not. And finally it will stop where it got the best results and it will print that this is the best result, this is the best configuration values for this, this algorithm or something, right? So that is, that is very important when we talk about hyperparameter tune, right? So, okay, we got performance grid. So this is hyper parameter tuning. We discussed about the cross validation techniques, we discussed about what are different algorithms. Again, we are not implementing the algorithms, but we are, you know, consuming these algorithms using Sklearn libraries, right? So once we are done with all these things, right, we plot the accuracy and we see that which one is giving us the highest accuracy. I highly recommend students, if you're using SK learn and try to take this code as reference point and try to plug in your data into multiple models. It's not that difficult. All you need to do is just use this code, just call these functions and that's it, you're done. At least you know, for your model, for your data, which is the best model. And once you are done, see, till here, till here, what is happening is, remember when you are doing cross validation, you are actually training the, taking the training data set and you're splitting into multiple folds and in each fold you are training the model and you're taking an average, right? Once you're done with that process, once you're done with that process here, what you do is you take the. Now what does cross validation tell you? Cross validation tell you that, you know, when you split across, you are basically randomly splitting your data. It's giving you a good measure that one of these algorithms is going to better work and will not overfit, right? So this is all cross validation. This is all cross validation. But once you're done with the cross validation and you decided, okay, cross validation tells me maybe logistic regression is the best model that I can work with. So what you do is like you select that model and now you plug in all the data, all the training data. Now there is no fold, right? You take in all the data, whatever you had, like suppose you have 80k data points and you spit across multiple subsets to give it to, to the machine learning model to do cross validation and you found out that decision tree, or maybe let us say logistic regression was the best model. You pick that logistic regression. Now you retrain the model from scratch, forget about cross validation because now you know that logistic regression will not overfit confidently. You will take Logistic regression and put all the 80k data points from scratch. No splitting, no cross splitting. And retrain the model from the scratch. Right. And now you take the model once it is trained with the whole atk, not the cross validation data. Right? Clean training data. You take the model and now you test it. Now when you're testing it, what do you can do? You can put print the confusion matrix. I. I don't think I've put the confusion matrix code. But it's very much available everywhere. Right? You can look at confusion matrix code and, and then you can plan something called as classification. So I've given the code here for somewhere here. It should be there already. Right? The classification matrix. So show the code. So here there is code where it basically prints the classification report. Now we haven't discussed what is precision, what is recall, what is F1. All I care for now is about accuracy. Next class will discuss about these extra metrics. But as long as you can show accuracy, whether you're printing a classification report or you're doing it in other way, it's up to to you. Please try to print the accuracy and the confusion matrix which I showed you earlier. Right. So this is it now. Now if, if I go back here and here in the end there is some deep learning code you don't have to worry about. Just, just ignore this. You don't have to worry it for now. I'll explain you what. I have the confusion matrix code also here. So you can take a look at how confusion matrix can be printed. Right. But in the end, yeah, there's some deep learning code you don't have to worry about. We haven't discussed deep learning and we don't want to do deep learning for assignment three. So just ignore this. But look at the confusion matrix. How, how the code is, right? It prints like this. Now coming back to assignment three. See, I showed you what data loading means. We know what is data analysis. We know what is data cleaning. We know what is data visualization. We, we almost know what is feature engineering. We have to transform. This is where I think you will spending most of your time and you might get some questions and that's good because that's, that's where your assignment should be forced to, you know, to do something different. Every data is different and you might have questions, you might run into problems, but that is good because in the end you're learning, right? Most of my students really like Assignment 3 and they say that they learn a lot, but it's also challenging, right? You're bringing all the points together. But. But in the end everything works out and you have, you feel that you have done something good. Now training, as I said, I explained you cross validation, I explained you some code. Right? And performance is all about printing the conversion matrix and the accuracy. Right? So with this all knowledge you can, it's. It's enough to build a complete pipeline, right. And once you have the complete pipeline, you should be really good to go. So let me, let me take a look at the assignment here. So if I go here in the assignment, so remember, you still have to do the PowerPoint presentation, you have to still submit the code. There will be no recording accepted for assignment three. Right. And I'm changing this, I'm changing this Spark using Spark thing just to. I'm removing Spark. If you want to still use it, use it. Otherwise just do scale one. Right. And okay. So. And again, this is online presentation. And as. As based on students feedback, I've decided that, you know, assignment three will be online and your final project presentations will be recorded. So because some students are saying that recording is good, some students feel live is good, we'll take both approaches. We'll split this thing, right? Yeah. And again, you have any questions about this, let me know. I'll still try to go ahead if time permits me to go and explain you the lab four. So in lab four, if I go to lab four assignments, you look at assignment four, right? Not assignment four. Yeah. If you look at machine learning using Spark. Right. This is the one which I just showed you the code. This is the one which is on data breaks. This is more on the Spark side. Now, now we have to still discuss. I might create a video. I'm still trying to see if I get the time to create the video or I will directly discuss in the class. Next class. Right. So we still have time. I've postponed it to July 21st. Right. So if somebody is planning to do on Spark, that might be useful. Again, we are on Modules and from a modules point we are right here. Right? We are. And here this one, Spark Machine learning and Spark Machine Learning. I'm just explaining this. We are almost going to be done soon. We have just XG Boost something and then here we Discuss the Spark MLib. So this is all so you know, this all the concepts and connecting the dots and then finally discussing about Sklearn, a standalone machine library. But here this is more on the MLIB side of it, how to do machine learning or Spark, which is of more interest and more required in this course. Right. So we will focus on this, finish this PDF and probably if time permits we will finish also the code or either I do it before the recording, we'll see. So that will be the wrap up of Spark Machine learning, the main thing, right. And then we will continue with the other topics. Our main focus is as soon as I finish on that I will, I'll. Next week I will also go into recommendation engine. I'll finish recommendation engine. It's going to be intensive, full complete class. So let's just be little bit prepared for it because we will finish machine learning and the Recommendation engine and then that. So that would be around 14. And after that you come on 21st. Right. And we have the next class on 28th we'll, we'll discuss some of the optimization techniques left out there. And fourth we will discuss the generative AI. Right. And eight, the class closes. So we have only around 1, 2, 3, 4 classes left. So that's the schedule. We will be finishing most of the things which we want to. One thing I will go as I, I'm, I'm. I'm trying to do is just because I got the feedback so I'm doing a little bit rearrangement. So please, you know, sorry for any inconvenience here I am, let's not focus. So if you have submitted that's fine, don't worry about it. If you have not submitted, you have issues. I'm getting some emails, you know, I'm, you know, I'm taking your feedback and I'm going to fix those small errors, whatever you know, students have sent me. But, but what I'm going to do is because generative AI is something that it will take time for me to tell you. All right, let's not do the generative AI. I don't want to go and go and explain the generative AI lab here. So I'm just. If somebody students are having problems, they want to redo it, it's up to you. I'm just extending it till you know, the end of our class. So just so that we don't have any confusion till I do in the class so that I'm actually, I'll put it till 2 second at least so that hopefully by then I will take the generative A lecture and anyways I wanted to go to the code for this specific two labs because of multiple reasons. The, the code is really good. Both. The, both the code. Right. So the Lab 7 the data centric AI tools framework, the clean lab is extremely important. I will tell you why when We. When we get here, it's. It's. One of the students really helped me to build it. He was doing some research under me. It's turned out to be really good code and we explored some things there. Even the gen. This code is coming from Nvidia. So it's also really, really good. So I really want to discuss these two codes. So that's the only reason I'm. I'm pushing this way ahead from a lab point. If you've already done it, that's fine. But I will explain this when we. We are done with the lectures. Okay. So that's it for your assignment 3. If you have any questions, just email me again. I am available on Wednesdays. That's Wednesday every Wednesday 9. That's 4:30 to Friday. Let me look at the silver ****. Ensure that I'm giving the correct time. That's 4:30 to 5:30. 5:30. I have another class, Intro to Data Science. But you can directly walk in and zoom, right? I'm not available in Draggers in summer. Right. I am available for any appointments if you want, but otherwise I should be available in Zoom. Okay, so we will. We'll see you in the next class. Okay. Sorry I didn't have any break so I'm leaving early. But next time I'll try to have some breaks. So either. Either breaks before or. Or we. We leave early. Yeah. Okay. I'll try to make sure that some. I'm there. Here. Thank you. Let me see what happens with the zoom. Yeah. Any. Anyone? Yeah. Any other questions? No. No questions. Concerns? Nothing here. Professor, that was great. Thanks for the overview. Okay, thank you. Okay. Okay then. Thank you everyone. We'll see you next week. It. 