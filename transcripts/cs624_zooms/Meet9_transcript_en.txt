Good evening, Professor. Good evening. I just wanted to let you know I just submitted the PowerPoint a second time right now. I finished it yesterday but I couldn't get my model to run. But I left it running last night and apparently it decided to finish after about three and a half hours. So I updated it. No problem. Thank you for that. Hello, Professor. I have not submitted it yet. Is that okay or should I go ahead and submit it? Whatever I have. You could. So your team had an. Yeah. So you could, you could submit whatever you have and if you want to make an exception, since you have told me now, that's fine. Yeah. And since I already have a email from your team, that's absolutely fine. Yeah. Okay, thank you. Okay, we will be getting started in a couple of minutes here. Okay. Everybody is putting their order in chat. So thanks for doing that. Yeah. Please go ahead and put your order, your names, team names in chat so that we'll follow that order. We'll get started in a couple of minutes. Okay. First things, I made a announcement and I would like to go over the announcement, you know, first and then we'll continue with the presentations. Okay. So. So based on the, you know, previous feedback I was getting from students, I was trying to, if you make some changes, I had extended my office hours from Monday to Thursday, 4:30 to 5:25pm and, and consistently. So these are announcements already made. What I wanted to go over is like Assignment 3, the total year Scikit Learn. This was supposed to be last week and I sent out a recording from my other, other class. So that was posted as an announcement. I will talk about these two things. So there is a postponement for the extended lab. I will talk later but the recommendation engine is something we will do today after the class. I will go through it. The extended lab. The reason is like I'm looking for infrastructure and the infrastructure is I'm trying to finalize the infrastructure. So that's the reason there is a. I wanted to delay it but it's, it's. I'm going to make an announcement by tomorrow evening on this but don't worry, as I said, it's a lab and it's going to be very friendly lab. You don't have to, you know, pretty much do any programming here but I will let you know more details on this and based on the student feedback again I, you know some students like more towards the canvas side a little bit or you know, a structured way of doing this. So I tried to connect few dots here and you know, redo the recommendation engine content. Of course I cannot go back and do everything, but based on the feedback I got, I tried to organize the recommendation engine in more connected way, I believe. So if you go to the overview, right, this is where we start talking about what are Recommendation engine, what we are going to learn in this module, right? This is the slide deck which we already discussed. So we had already Recommendation engine but just. Just gives an overview of what the slide deck does, right? And then also these are some additional videos which I posted for assignment for your project. But so this is where you. This a good video on how to what are the different recommendation engines by one of the professors. And then this is more on the system design about the recommendation. These are really good videos. So I really recommend that you. You go through them and. And then one of the. So in recommendation engines when I was trying to talk last time, there is two take two main types of recommendation engines. One is the content based recommendation engine. So if I'm watching movies, like comedy movies, based on what I'm watching, there are set of algorithms or methods from the literature you could use to build a model which only looks about the content. And based on the content and preferences, the model will start recommending objects to you. But then there is another category of algorithms under Recommendation engine which is called collaborative filtering. Now collaborative filtering, last time I was trying to show you Amazon, you know, Amazon Recommendation service. And so generally all these Amazon or Netflix, what they do is like they look for user preferences. So if there are similar users and you know their preferences are similar, then based on those users and their preferences, new content would be coming up recommended to you. For example, I like to watch some horror and comedy and my friend also likes to watch horror and comedy based on what we together are watching or you know, the new recommendations will take into consideration the users and their preferences, not just the content, right? So that is collaborative filtering. In collaborative filtering, you know there is two important algorithms. One is matrix factorization, right? And another one is al. So matrix factorization is a concept. So if you have a table where you have users and you have preferences and how to make use of that for recommendation. So it's a concept and it could be implemented using multiple algorithms such as ALS and svd. So in this video it's a very good video on how metric factorization works. Now I highly recommend you to just. It's a very, very short video, but I really would like you to go ahead and see this matter fact characterization because it breaks down because sometimes the whole concept here is like sometimes, for example, you have let's say 10 users, right? And we take preferences for like let us say five movies for all, from all these 10 users. So sometimes you might get ratings for all the movies or some users might not rate all the movies. So some users might rate all the movies and some users might not rate all the movies. So when you take this whole data and put it in a matrix, you get sparse matrix, which basically means that some users don't fill in, don't rate movies. But now when you have a matrix where you have only half preferences from the users or only some preferences from the users, but you still want to, based on the preferences or information you got from the users, you want to still see how would the user rate for a movie which they have not submitted a rating, right? So based on the existing knowledge and existing preference, how could you fill up this matrix? Because this matrix is sparse and that is matrix factorization and excellent video. Please watch it. And based on this, matrix factorization can be implemented in multiple ways. There are multiple algorithms to do it. Now we are not building matrix factorization from the scratch. Neither we are building these algorithms from the scratch. What we are trying to do is even in your project is you can use either content based filtering or collaborative filtering directly from the APIs. You give the data, you use these methods and you present your results. That's it. It's just a simple recommendation engine I wanted you to work on. But the only catch here is you would have to use spark so you can learn the concept. Here we are learning the concepts of recommendation engine. What is content based recommendation engine and what is collaborative recommendation engine? And most important topic in collaborative is this, this matrix factorization. So this video, if you watch it becomes really easy to understand more on. And then this is again a small, very, very small video where they are building a recommendation engine, you know, from pipeline, the complete pipeline. And it's, it helps you to see how to build a recommendation engine, what to take care of. And it uses SVD or matrix factorization, just I told you, right? But it's not, it's just using those APIs, it's not building the SVD from scratch, right? We don't have to worry how these algorithms internally work. We just need to use this to build our model. So but here it's not a spark code, right? It's not a spark code. If you watch this video, what will come out of it is like, okay, how do you take the data, how do you pre process the data, how do you build A pipeline for a recommendation engine. And how do you present the results? This is what you will learn from this, right? So I try to organize this here and then now one thing, because in this project you have to use Spark. This video is basically, it's again a very short video, but it's going to tell you how the Spark code works, right? The complete pipeline. And we already had distributed you the code in talk about it. We've already done that lab. But you already have the Spark code. You can reuse the Spark code. But this basically tells you the meaning of that Spark code, right? So he's just doing hands on coding and explaining you that already you have done the pipeline for data analysis. You've done the pipeline with Assignment 3 using sklearn. This just shows you how to do the pipeline with mlib. Right? So we haven't done much on the MLIB side. Assignment three, you're focused on Sklearn. But if you had to use mlib, like machine learning library from Spark, there is a pipeline, similar pipeline to Scala, all in the SQL. And whatever you are doing, you are doing here also. There is nothing new, but under the hood everything is distributed, right? So what libraries to use, how to use it. This guy, you can take a look at this video and then these two videos. So this video doesn't explain anything on recommendation engineering, it's just to explain you. I mean it does a little bit on Recommendation engine, but the main, main thing is about SpicePark and MLA. What is the pipeline, right? And then this video is more about actually how to build a recommendation using pyspark, which is mostly related to your project, right? So because you have to use SpicePark. So I'll just put some reference videos where they're actually building the code, right? So and my intent is also to go over some hands on notebook in class, right? Whether it's record I will be doing or whether I will. We will, if you have time today, in the end we will do something here itself, right? So but these are all resources which should make you very, very comfortable with the project. So again, if you have any concerns or you know, you feel even after watching this you have some issues, just let me know. I'm ready to spend more time on this thing. But given the resources, I think it's very straightforward. But if there are any issues or problems with your project, please let me know. Okay, these are which I intend to go. This is not your lab, right? This is, this is a set of this open source GitHub which I tried to put. I Will go to the end of the class just to show you some of recommendation Indian projects, right? So how they work and how the code works, right? So this is something the links to those notebooks which are available in the Internet in The, in the GitHub of a book repository. So you can take take a look at again, I'm going to, if time permits at the end of the class. This is my intent, this is what I want to show you from Recommendation Engine. Because here we can show some content based recommendation engine, some collaborative recommendation engine. And this mod 11, you have already submitted the lab for it. If you look at this notebook again, there is a lot of Spark code already available. So some students are already using this in their Assignment 3. Because Assignment 3 I said it's optional whether you want to use Spark or not. So this, this library, this notebook already there is Recommendation engine code using Spark. It uses movies data set. So hopefully if time permits today we will go through some of these content here. And these are the data sets that are supporting these notebooks. Again, this is not a lab. This is supplement material or additional material which I want to go with to ensure that everybody is understanding this Recommendation Engine project. Okay? And these are supplement material just to let you know recommended when you are investing time in Recommendation engine. I start. I told you why are we doing this, right? Of course, it's a part of big data, right? The recommendation engines cannot be built just like that. You need large amount of data, right? So what does it mean? All these big tech companies do have large amount of data, right? And they are building recommendation engines. So if you talk about LinkedIn, you follow Netflix, you talk about Walmart, you talk about all these big companies, whether they Seattle, the whole San Francisco or the startup, they. They're focused on lot, lot of large data and, and building these recommendation engines, right? So, and you can see some of these videos just to. For motivation that if you apply for these tech companies, what is expected, right? How how does the interview process go and why is recommendation engineering? So here some of the senior engineer is explaining about, right? What is the why Recommendation engine and how to design a recommendation engine, I believe for Instagram, right? So because internally they use all these algorithms. So if you're more interested, you're prepping for something, it might be really useful. So, so that you, you try to focus more on from the academic side also how to get to the job market. The supplement material and these, these are two additional supplement material. Now you know what we are discussing in class are I would say are the basics of recommendation engine, we are just starting what is recommendation engineering. We're getting to know what is recommendation engine. You know how to use it with normal without spark. How to use it with spark, right? That is our intent. But when you go to job and you're going to, going to really go to big companies or something, if you're building recommendation system, how to deploy these recommendation system is very, very challenging, right? So the requirements are going to be totally. A lot of additional things are coming. So if you have time, if you have time not in this course, if you have time and you're really more interested to learn more about this, please watch about Merlin. Now, Merlin is a recommendation engine framework. See, all companies cannot create their own framework. So to host these models and all those. So Merlin is a good recommendation engine framework which they would use to host the recommendation system in production, right? So when you talk about production and industry, you want to scale your model to millions of users which are going to recommendations. You need to have that infrastructure, you need to have that system design, right? And if somebody has already developed it, suppose Nvidia in this case has already developed it, you, if you're in a new company, we would just use Merlin, right? And you would plug in your data, you prep your model and use the Merlin framework to host it. So, and many companies will do that, right? So it's a good thing to know what is the industry standard framework so that you know, if you're going to many other companies which might use this Merlin or similar framework, at least you know what's going on, right? So please take a watch if you still have time. I know the course is ending, so. But you still have time. This supplement material not required for your any submissions, Right. And for your final project, if you want, if you, after watching this, you feel that, you know, maybe why not use Merlin to do this, our project, right. So if you look at their GitHub code, you can use Merlin framework also to work on this. Okay, so with all that, we'll just go back and I think, yeah, that's it. So this is the little bit redesign on the canvas model for this. So we'll, we'll go and we'll get started. Let me look at the chats. So before we go, is there any questions on the Recommendation engine so far or the project? Okay, so let's go with, start with Sam. Yeah. All right. Yes. Let me give you the permissions to do that. Okay. Can you see it? Yes. Okay. All right, so hello class, My name is Sam and I'LL be presenting my predictive car prices machine learning pipeline. Okay, so the problem I'm trying to solve here is similar to what I was doing in Assignment 2. I'm just trying to get a fair price on new or used cars because it could be hard to find a good deal about without comparing prices across the market. And as a solution, I analyze a large data set of car sales across the United States. And by building a machine learning classifier, I estimate whether the given car is actually a good deal by having an algorithm label it as a good deal or not a good deal. And so my data set is named US Car Sales Data Set. It was retrieved by Kaggle. It was retrieved from Kaggle, and in observing my data set, saw that it had seven features over 144,000 instances. The format was CSV and it was tabular, and the data types were strings and integers. As far as sampling is concerned, I decided not to because I figured 144,000 instances wouldn't be too large for my analysis. However, I did clean the data by giving cars without a mileage a value of zero instead of leaving it null. And there were also no values for the dealer and the price. But I left the dealer as null because it wasn't really utilized in my model. And I deleted rows with the null price because I didn't want to incorporate free cars. That may skew my results. Okay, so as far as inconsistencies and outliers go, I handled outliers by removing the top 1% from the price and the mileage. And I did check for data inconsistencies and couldn't find any. I did find missing values and I cleaned them. The significant ones. As I mentioned in the previous slide, I did notice a class imbalance because there was significantly more new cars than used cars or certified cars. But I found no major issue in this as far as affecting my model. All right, so here are some visualizations for the data analysis. As you can see, I have two bar graphs for my univariate analysis. One is the top 10 most common brands and the other one is a bar graph showing the years of the cars listed. As you can see, Ford was the most common brand and the most common year was 2023. Also have two histograms for malice and price. Most of the cars had a low mileage value because they were new. And the price distribution shows that most of the cars were around 40 to 50 thousand dollars in price. Okay. And because the data set was so large, I just decided to focus on my Scatter plots on the Ford F150. And at the top we see year versus price. The bottom left we have mileage versus year. In the bottom right as mileage versus price. And as we can see, the years versus price and the miles versus price is more linear, but the mileage versus year I would say is more scattered. And so I call it non linear. And here are my box plots showing the outliers. Looking at this, we can see that the price was around an average of $48,000 in price, while the mileage was mostly zero. Because they were mainly new cars and outside this range, I observed that there was a good amount of values indicated by the orange dots here. And as I mentioned, I did exclude the top 10, the top 1% from this and most of my graphs and the entire data set because it included a few supercars, such as the $8 million Bugatti. Okay, so I did do feature engineering on my data set in hopes to get a better accuracy. And specifically, I use feature transformation in order to create two new features, car age and miles per year. Car age was determined by normalizing how old the car was, such as converting 2021 the value of 2021 to three years old, because at the time of the data set it was 2024 and so it would be three years old. Right. So miles per year was made, as the name says, by determining how many miles this car was driven per year. Right. And so if the car was 10 years old and had 100,000 miles, then the miles per year was 10,000. I then created a label called good deal, which returns true when the price is 15% lower than the average price for that model year. So I ran the Python script and put it in as a column, as a label column for my entire data set. And that's the criteria I decided to set as a good deal. And anything higher than this value I labeled not a good deal. Right. And I did not place these new features in their own unique data set so that I can compare the price, I mean the results, and see if the new features had any significant difference on accuracy. For training the data, I split it up into 80, 20, 80% training and 20% test. For cross validation, I use bipolar cross validation on the training data and apply grid search CV from my hyper parameter tuning. This selects the best parameters and evaluates them on the test set. And I do want to mention that the three models that I used were was logistic regression, random force and the gradient boosting. And I did attempt to visualize the convergence Process for my cross validation. Here's the original data set without feature engineering. And right here we had the transform data set using feature engineering. And the conversion process might look different, but when grid Search CV selected the best parameters, they both had the best results of 84% accuracy or the same results at 84%. As far as evaluations, I did mention that I tested it on three models. Logistic regression, random force and gradient boosting. Both of the data sets. Excuse me, for both of the data sets, the transformed and original had the same accuracy for all models at 84%. So I just decided to proceed with the transform data set. And even though the three models had the same accuracy, gradient boosting performed better when Precision recall and F1 score played a factor. And I also wanted to show here the BROC curve to further show that gradient boosting had the advantage over the other ones. Okay, so I actually built the model using gradient boosting and placed it in a function called is good deal. This function takes the model which is the gradient boosting, the year, mileage and the price. And for the scenario we see that predicted a two year old car with 50,000 miles as a good deal with the confidence of 73% percent when the price is listed for sale at $16,000. Also created as a side project, a regression informed classification model that it takes the same parameters, but instead it estimates first the price of the car and then determines the actual price. I mean, then it determines if the actual price is low enough in order to be classified as a good deal. Okay, so in conclusion, some of the strengths I had was that the models worked well with limited features and that cross validation provided very stable results. Limitations were that more detailed features such as trim level, condition, location and more were not included in the data set. And also my rule based approach and threshold of labeling the car as a good deal may not fit all contexts or markets. Right. But in future work I should be able to add more significant vehicle attributes and explore data dynamic thresholds based on the market trends, as well as use real time data to evaluate production like scenarios. And. Thank you. Any questions? All right, thank you all. Sam. Sorry, I was on mute. Excellent presentation, Pierre. So, Sam, I have two questions for you. Yes. Yes. So in feature engineering you used car age and miles per year. So I just wanted to understand what was the raw data and what was your intuition to extract this feature? Yeah, so the raw data was just as I go here. Anthony, could you stop sharing your screen? Yeah. Are you able to see? Yeah. Okay, so the raw data was just year of 2023. Right. Year 2024. And then the mileage was just the total number that the car had. Right. So did a few research to see what I can actually transform it into. And so first decided to do the year to make it normalized to make it more more so like a visually so you can see it better visually. But then it also helped out the mileage per year to so I can better calculate this because when you look at a car, one thing that could really play a factor is the durability of it. And so if a car is. Is brand new but has 150000 miles on it, it'll probably be worth less than the car that is 5 years old but has only 50000 miles, if that makes sense. So wanted to test it out and experiment if that actually made a difference. And unfortunately it didn't really make a significant difference. But yeah. Okay, great. So other question is what in your analysis the and model building was. Was most challenging step for you? I would say in model building it was actually like putting everything together and ran into a few issues where at first I went down the pathway of like doing the regression style. But then I realized that hey, like this assignment requires classification. So I switched up the code and and actually created. Had to formalize like hey, how can I actually determine whether this is a good deal? So that's why I built the criteria of making it creating the label column. Good deal. The price was 15% lower. And so once I had that then I can actually put everything together and build my model so that it classifies a good deal versus just predicting the price. And so I had this as a bonus. Right. But this is what I originally had made but then I transformed it into a classification model. Excellent. Excellent. Thank you. Yeah. Any. So, so everybody, if anyone has questions they can ask again, there's no you know, like grading for the questions. It's just open so that everybody shares their knowledge. So feel free to ask questions and if you know for some reason if you're not able to answer, that's absolutely fine. We don't know answers for everything. Right. So, so feel free to have this communication. It's just to share the knowledge. Don't take it as like, you know, it's. It's more of a Q and a tough question and going on. Okay. Yeah. Thank you again, Anthony. We will go next with you. Okay. Hi, I'm Anthony and can you hear me? Yes. Okay. Okay. So last time I did. I think I did healthcare death or no Death in the hospital all causes. So I I stayed in the medical light realm this time and did appointment no shows was the target variable here or the target feature. And so my problem statement was to develop and compare supervised machine learning models that would predict whether a patient would know or no show or would possibly it came from Kaggle. And the whole idea is that office workers could take preventative measures either by scheduling more people when it's likely that people are going to miss appointments, or preventive measures like text messages and things like that for data loading. I kind of started off in databricks. It's what I did for the last one. I downloaded the archive zip and just manually uploaded it to the file system. But I don't know what's going on with databricks lately. It started to get real slow and telling me to switch over to get off the community edition. So I switched over to Google Collab. When doing so I noticed that I needed to do this spark session builder that I didn't have to do in databricks. So that was one of the difference I saw. But after that the performance was much better. Bringing it in I just created the struct type and kind of took that opportunity to rename some of the column names just kind of for brevity. So as I'm working with it it was a little bit easier. And then of course I cached the data frame to see if it would fit in memory, which it had. No problem. I think it was skipping ahead. It was a little over 100,000 records. But we shall see. Here we go. So the initial data analysis it was tabular CSV 110,000 records, 14 features. The features which were included is kind of similar to what I did last time. And I keep think I'm messing myself up with this with not having like a nice variety of features. But we had IDs for patient and appointment ID which I assigned to the long data type. Numerical age was continuous scale 0 to what turned out to be 115. Handicap was numerical from 0 to 4 as far as counting the number of handicaps you might have. For categorical data we just had gender and neighborhood temporal. We had full date fields for scheduled date of the appointment and the appointment date itself. Then binary numericals. We had scholarship, hypertension, diabetes, alcoholism and SMS received which those are pretty self explanatory. And then we had the target variable no show where the positive yes or one would be obviously you didn't show up to the appointment. Some data observations that I had were that five patients had missing IDs, which is actually fine. It was. And when I say IDs it was the appointment ID. So not something I was going to consider in the model anyways. I had five records where appointments had dates prior to the schedule date as I was just kind of saying before the age range I had from minus 1 to 115. So that indicated to me that I probably take a look at the outliers there and see if the records were legitimate. And then obviously target imbalance was 80 to 20. So I knew I probably would have to do something there with the model. Although at this point I really didn't know what. I was just kind of going through the steps and just as like a bonus feature that I'll kind of revisit in my conclusion is that there are 48,000 records for repeat patients. And also just to add the scholarship in this context means that you're receiving government assistance in some form. Yeah. So kind of leading off that slide the data was fairly clean. There really wasn't any nulls Besides those five appointment IDs I ran the pandas describe and that that is what illuminated the one person aged minus one and somebody aged 115. Later I would find out that that would be that person had five separate appointments. There was no duplicates as I said before. And ultimately what I decided to move was just the records with people with that were minus 1 years old and 115 as well as all the the records with appointment dates prior to their scheduled date. I just threw them out. It was only what did I say, 10, like 10, 11, like 12 total records or so. And the way that I define my outliers I use the IQR filter and I did that on the age obviously and then days between the scheduled appointment date and the actual appointment date. Just doing an easy calculation between the two. The first thing that I chose to visualize kind of led from the IQR discussion or thought process is I wanted to see and I felt like it was necessary to choose between IQR and Z score. I didn't know if there's other methods, but those are the two that I had. And I think Z score was for a normally distributed feature whereas IQR worked better in a skewed distribution. So that's what I went as far as how to decide which methodology I would use for the outlier detection. As I said before I took a look at the imbalance class where obviously much more people are showing up to their appointments that are not showing up at a rate of 80 to 20. The next feature that I was curious about when I've lost my place here was the neighborhood to see if that would have any effect. I actually thought that was kind of silly, but you can kind of see that there's a slight correlation with rate based on neighborhood. And kind of as I was doing this portion, I wondered like, what is like, significant as far as like how, how strong does a correlation have to be for it to matter for the model? So I found this little chi squared calculation for P value and saying that any value less than 0.05, whatever, statistically significant. So it showed up that way. My intuition kind of looked at this graph, said, yeah, that looks like there's something there. So that kind of confirmed my suspicions. If I did it properly, which I think I did. The next most important feature, I thought I should say the most important feature I thought would be waiting days between the scheduled date and the appointment date. And you can kind of see like the sharp increase. Like if you're, you know, most people who make an appointment today or tomorrow are showing up the next day because it's likely urgent, but then it quickly moves up and then it's just kind of a slow climb over time. So I felt like there was enough there. You can see on the heat map that it's like, it's pretty weak at 0.23, but I do think that's still relevant. So I was planning to use it at this point. Although I will say at this point I didn't know what I was going to use. I was just surfing around trying to figure out what should go into the model. This was another point that I looked at was the. I pulled the day of the week out of. And this actually goes back to a discussion point like feature engineering and kind of doing your analysis at the same time, like you kind of. I was kind of doing them together. I, I'm. I guess in practice that might be how it goes. But anyway, so I pulled out the weekday from the appointment is scheduled to see if there was any kind of pattern here. But it honestly, it just looks like nothing to me. So I figured that might be something that I end up not using. Then we're looking at the sms, which was the second most important for my intuition that I thought, like, hey, if somebody's receiving a text message, they might be reminded in either reschedule or show up to their appointment. But it didn't really seem to bear out in the results. You can see the people who didn't show up or didn't get A text message still showed up, by and large. But then when you see the people that the proportion of people who received a message and then didn't show up is a lot higher. So that was a bit of a surprising data point. And then I also added hypertension on the right graph and that sort of just was symbolic of all the other binary features that we had as far as diabetes and handicap. They didn't really seem to have any correlation with not showing up to your appointment. So for feature engineering, it's going to be a lot of the stuff that I showed you. You got a sneak peek in the data visualizations. But I added waiting days, which was just the calculation between scheduled and appointed days. I extracted the weekday to see if I can deduce any kind of relationship between that and no show. I simplified handicap by instead of doing the 0 to 4 by just making it a binary, I binned the ages because I was trying to find some relationship across there. I ended up not even putting it in the model, but, you know, I had it out there. And then class weights, I. I created two. I think the recommendation for an 80 to 20 was 4 to 1, but you'll see later kind of how that affected the accuracy and it confused me for a bit on how I should be handling it. So. And then for encoding, I didn't research any one model at this point. I kind of just knew that, hey, things need to be numerical. I kind of knew the one hot coding deal. And then I knew I might need to scale. So I kind of took the ones that fell into that category, like gender and neighborhood were categorical. So I indexed those. And then the age group, I indexed the bins, but as I said before, I ended up not using it. No show was originally in a yes no format, so I swapped that over to the 01 and then scaling for age, which again was zero, to end up being a little over 100 and waiting days, I think got out to like 166 or something like that. So they were scaled okay as far as training. So the four models that I went through, I ended up choosing random forest, svm, decision tree, logistic regression. I really had no reason to choose those. I just went for it. And then at this point, with all my features created and things scaled, I created the weighted weighted column. I just started trying to figure out how to plug them into the specific models and what they needed. In the end, I think I ended up with the same features for random force, SVM and decision tree and use the scaled features for logistic regression, which included the days between and I think I had a vectorized neighborhood. Then I split the data 8020 and I kind of went through the process first doing the data split of 80:20, went back and did the cross validation. As a separate thing. I created kind of, you know, a method for all of those to use. As I did that process, I realized that it was sort of difficult to have different features for the model and pass them to this function for them all to use the same features. So I had to sort of build the building of the features through the pipeline and then pass that to the cross validation. I don't know if that makes sense to any of you guys, but I did find some difficulty there building the cross validation in like a programmatic way for all of the models to use. As I said, I went through the four, the four, those are the four models I used. I chose the baseline best model, which ended up being svm. And then I ran the grid search for only that model because quite honestly running these things over and over again, it takes a lot of time. And it was, it was kind of tough for me. You can see for the performance, I already had the, the AUC, the F1 Precision Recall on there when we decided that we were just going to use accuracy. So I left them all up there. You can see SVM performed the best. And this is the, the top ones that you're looking at are the, the baseline measurements before the, the grid search and the cross validation. You can see SVM is the best. What you also probably are noticing is that two and a half to one was like way better to. When I scaled it to 4 to 1. But if you look at the confusion matrix in a second it almost. It was hard for me to choose which one was better because it was. I'll show you the confusion matrix in a second. But also after I ran the cross validation, you can see that svm which is showing up here as linear SVC model was still the superior model in both cases. So in the confusion matrix you can see that the two and a half to one ratio on the weighting, it really didn't want to guess that somebody wasn't going to show up to their appointment at all. And just because the vast number of people who show up to their appointments, the, the accuracy ended up being much higher. But that's not really helping us because the whole idea is to target the bottom right quadrant. Those are the people that we want to figure out. So I figured we need that number the highest and we need the false guesses the lowest so that we can concentrate our efforts on the people who are most likely not to show up. Again here. This is the grid of the grid search that I did. I got the parameters, we got the accuracy up 72% and I did the grid search, I should say on the 4 to 1 ratio. So we got up from 68 or so to 72%, which it was better. I did not go back and plug this in and retrain with these. I just took the grid search for each word. And as far as conclusion, this is some stuff I just pulled off. But as I was working through this, not all of the strengths and limitations of a model were apparent. I didn't track the time that it took to train a model, even though that seems to be a limitation of random forest or svm. I did notice the overall time to build the entire pipeline was long and hard to work with. But overall, I mean it was pretty simple. I kind of looked up, I thought all of them were kind of simple, although my accuracy is not awesome. So maybe they weren't as simple as I thought. But yeah, I kind of just looked up, hey, what does this model need? Plugged it in, it spit out what I needed. So it's almost stolen valor to list these strengths and limitations because they weren't necessarily something that I noticed as I was working through. I don't think I'm skilled or sensitive enough to the subtleties between the different models. As far as my personal improvement, what I would like to do to these models, it occurred to me that after the fact that I might want to do previously a patient who previously no show as a engineer feature that I might have been able to use for the model. And that's why I said earlier when I had 48,000 repeat patient appointments, there was probably an opportunity there that I missed. So I would probably implement that on my next go around. Another thing would be pipeline modularity where the speed of the pipeline when you're running it over and over again and trying to tweak things and your code is relying on other pieces of the code. I think I would probably get better on refactoring the code for reusability and being able to go through the cycles a little bit faster. As far as the modeling insight, I ended up using all of the features because I wasn't sure which one was more important than the other. Now I tried to pull feature importances from each of the models, which is different for each of the model types. But in the end I really didn't understand how to know when I had too many features, if I had too few features, if the model. Is this the best the model can do given the data set that I've given it? I don't know how to detect that either. So those are things that I want to look into. And hopefully I didn't take so long because I was rambling. So that's all I have. Excellent, Anthony. A very nice presentation. So I have two questions for you. One regarding the class weights. So I know again we in the class also because this is a very big pipeline. So for when you have a class imbalance, there are many methods to choose from. I'm just curious to know why you went with class weights or that was some method that you understood or you thought it is more appropriate to choose because there are many methods. I'm just curious to know why this particular method. Well, to be honest, it was simplicity. I ran through the first one with random forest. That was the recommended one when I did a search on it. And then after that I tried to implement a different one and I can't remember the name of it for one of the other models. And I couldn't get it to work. And I said, okay, look, I got to get this thing working. And so I bounced back to what. What was working for. For random forest. Okay, absolutely. So in general, whenever you have class imbalance, there are a couple of methods. Like one, one way to do is like majority sampling which basically means that you know, if you have anything, sorry, you. You over sample your minority class. So whatever is minor, minor minority class, you bring it to the level of majority class. Other way to do is under sampling is reduce the number of your majority samples to bring it to minority sample count. The third thing is we could use something called S mode. S mode generates synthetic data and so that again, you can level up both the majority the minority class up to a level 2 majority class by generating synthetic data. And there are multiple ways to generate synthetic data. One is SAM S mode. You could use Generative Adverse Network GANS to generate. I'm just letting everyone know in the class I'm asking these questions because these are an opportunity point for me to discuss something I have not discussed before or maybe redo it a little bit. Right. So these are some things we can choose if you have class imbalance issues. But I really liked your presentation. Excellent work and I hope students are learning from these excellent presentations so far. Okay, thank you. I think. Thank you. Stephen Harris and John Gray. Hey everyone. So I'll be speaking first and I can't see my. Our screen, Professor. Yes, excellent. So again, our data set involved. Our question was whether defense wins championships and whether it was fact or football folklore. And next slide please. All right, our problem statement was how do defenses impact winning teams and do top ranked defenses lead to NFL championships? And next slide please. All right, now our data set and pre processing. So again, so team schedules align the team names and acronyms. Game data. We had NFL game stats from 1999 through 2024. And interestingly enough, when we did some poking around, we found a built in Python library for NFL data so we could use that for advanced statistics. All data sets were integrated into one consisting of over 15,000 rows and 150 columns. Carriers of our data include dates, passing, rushing, receiving, advanced defensive stats and results. Steps for data preparation. One, we had to group player stats together into their current NFL team. Two, we had to merge records together using the year or season week and NFL team is the key. We also had to remove data based on incorrect matchup. Also, teams are labeled as both home and away, so we had to account for that. And finally creating rows based on sums and averages of team stats. And next slide please. All right, so a couple visualizations. So this one right here is your turnover margin versus points allowed. You have a slight linear relationship here between the higher your turnover margin, the less points are generally allowing your opponent to score. And next slide please. And again here we have sacks versus points Aladniks, kind of. So again we see a slight linear relationship as you're getting more sacks. The naturally the number of points that your opponent is getting is going down. And next slide please. All right, and here's our box plot. Our interquartile range is roughly between 15 and 29. Our outliers start on the high end, anything above 50 and then on the low end, 0. And then next slide please. All right, so here's our correlated correlation matrix similar to last time and we're overall we had pretty moderate correlations, so nothing particularly strong that jumped out. But something's of note for games one, we had a slight positive correlation with turnover margin, which again, kind of makes sense. The opponent score at 0.56 and negative 0.56. And finally the point class at negative 0.53. And that's the end of our visualization session. So I will pass it on over to Steven. Okay. All right, so for the model development, we chose a few models. We'll go through each of them. So, linear regression, nearest neighbor and decision tree. So for the first one, our major goal was to use sort of the correlation matrix to kind of help determine the input features that we wanted to use. So we tried, we decided to focus on turnover, margin, defensive stats and class. So the point class was determining how many points the offense had scored. So basically the lower the points, the lower the better the class. Essentially the first target variable that we wanted to use was we wanted to determine if these individual features could lead to a team win. So we tried the linear aggression first. Our results really weren't too well using this one. I believe this is mainly due to even though the defenses may do well in one of these categories during the game, it just does not always consistently correlate to a win individually. So we wanted to use our next one, which was nearest neighbor. We decided to use PCA to group these three features into our two dimensional structure for this model. So we're still using the same input features and we still have the same target variable. We definitely saw much better results from this. Our accuracy was around 80%. So overall this was indicating that we that there are some consistencies in the turnover margin, defensive sacks and amount of points that are allowed that they do dictate whether or not the team will win or lose the game. Our neighbor count, we had to use about 15. That's where we got the best accuracy. This also makes sense because once again, going back to how we analyze the data, teams that typically do well in these categories normally win. However, because it wasn't consistent, the nearest neighbor allowed us to find patterns and find some of the clusters that would find us to find a better win or get better numbers in that situation. So I do have a visual here that kind of details this. The red side refers to the wins while the blue side refers to the losses. Pretty well balanced as well. As you can see, we have some inner clustering of both wins and losses in the middle. So for our next model we had limited data, but we wanted to go ahead and do this anyway, which was we wanted to determine if we could predict a playoff team using this data as well. So we added in a few input features for our decision tree. We chose to use sacks, yards allowed, points allowed, defensive pressure. So defensive pressure is how often a team is able to apply pressure to the quarterback and turnover margin, which is also basically how often the turnovers does the defense cause more turnovers more than the offense turning the ball over itself? These were our main features and the goal was we wanted to determine if we could predict a team making the playoffs versus not making the playoffs. In addition to using these input features, we also set up a ranking for them based on the 32 NFL team. So the top team would be ranked number one and the last team would be ranked 32. Our first decision tree gave us an accuracy of 81%. This was good. However, we were looking at sort of the purity of some of these leaves and we can see that we didn't get too well of purity until we reached the end. So the far left and far right where we could see that teams that typically ranked in the top 10 for both sacks and turnover margin would be labeled as a playoff team, whereas teams that typically aren't. So in other words, they're not ranked in the top 10 and their points allowed is definitely in the bottom portion of the league. While we're typically not playoff teams. We chose max depth for three to try to prevent some overfitting as once again the data was limited and we used a split of 250 and leave size of 1 100. And then for our last one, this one was further limited, which we'll get to in just a second. We wanted to see if we could predict based on the playoff teams, if we could predict a Super bowl team. Now, this model did not turn out well. We had an accuracy of 27%. One takeaway though that was sort of positive was that down at the bottom left we can see that our class super bowl did have a majority of the sample, so 13 out of 25, which is 50% of the total teams, at least in that particular section. So it was pretty good to see that we could at least see that in this case teams that ranked with a sack ranking of in the top one or two, so definitely very elite and a turnover margin rank of essentially top 10 had a much greater chance of making the super bowl versus other teams. And our last model we use season tree yet again, this one we wanted to focus on, our target was essentially just super bowl or not Super Bowl. So we condensed all of the other playoff teams into one class. Due to this causing a major issue in the distribution of the classes, we chose to use a class weight of balance. This would add more weight to the super bowl rows. In our analysis, the max that we chose two again to avoid overfitting and overall results were decent. We did have some purity in the top in the bottom left and bottom right. But overall it kind of indicated sort of the same thing, that teams that were very good at sacks and had pretty average third down conversion rate may have a better chance at the Super Bowl. So in Conclusion One of the major the improvements that we could have is essentially we needed more data. Our data was over the last 25 years for game data we had enough. But for playoff data, there's only been 300 teams that have made the playoffs since 1999 and there's only been 50 teams that have made the Super bowl since 1999 as well. So overall we just need more data over the years to really make a better prediction in this type of category. And that's all. Excellent presentation, John and Stephen. So I have two questions and one comment here. The first question I have here is about the pca. It's great that you chose PCA and I just wanted to ask you did have improvement in PCA and did it tell you which features were important? Yes, PCA helped significantly. So going back to our correlation matrix originally I did try a nearest neighbor using just one of these input features. The model was ranged somewhere around 60 to maybe 70% which is borderline baseline. Once we use PCA we definitely saw an immediate 10% increase. So it improved the model quickly. That's great. So almost 10% increase. That's excellent. Now regarding the data, so also in your conclusion slide and also when you were talking about the results, looks like the data is less for the teams that made to play off and also teams that made it to Super Bowl. So if, if you had more time to do this project and you want to maybe, you know, patent this idea, what what would you like to do? What is what, where would you like to invest in terms of time? Yeah, so I would say we definitely wanted to invest more time in trying to predict the super bowl team. We had a limited data set for it. I would say maybe trying to predict the teams at the based on maybe previous season rankings. So we would have to probably use data from the previous years to try to predict that up and coming season. That that would have probably been maybe our next goal or at least thing that we would have tried to do. So it would have increased the amount of data but the accuracy may be iffy just because the teams can change each year. Excellent. One comment is like I would recommend that to look for also synthetic data since you have since you have problems with data, one way to improve data is you have now GPT models so you that might be able to generate more data for you synthetic data, something similar. And there is also Gans tabular Gans that can generate synthetic data similar data. So also it would be great to see if you generate more data that is similar. Maybe I Don't know, maybe there are in the recent couple of months. Are there any tools that could generate forecasted data for the next 10 years or something? And maybe if you use that 10 years forecasted data and use it in your project, how would your model turn to be whether it'll start predicting? Much better. Just, just these are some of my comments and really interesting project and thanks for sharing that. Thank you. That's a good idea. Thank you. Thank you. Professor. Yeah, Benjamin, John, hang on one second. Can you guys see my presentation? Yes. Okay, sorry, just a minute. So I'm kindly requesting the whole class to please engage and ask questions. It's okay, I don't have any grading points here. If you don't answer or if anybody ask more questions, it's just a friendly chat. So please, I encourage to share the knowledge. Right. Okay. Please go ahead, Benjamin. All righty. So for my data set, I'm going with CDC Diabetes Health Indicators. And the whole point of this data set is it analyzes several different aspects about a patient's health and lifestyle with the goal to predict whether or not they have diabetes or are at risk with of having diabetes. So yeah, I loaded this from the UCI ML repository using a Python library specifically for that. And these are all the different features that it has. There's 20 different features, seven of them are integer features and the rest of them are binary features. And the what it's predicting is of course whether or not the patient has diabetes, which is a binary feature, which. And the integer features have been bucketed. And this data set has over 250,000 records. So I did a bit of preliminary analysis. So I checked to make sure that there weren't any missing values. It said that there weren't, but of course I wanted to double check just to be sure which there weren't any missing values. And I also checked for class imbalances, which as you can see, there is a massive imbalance favoring the fact that a lot of people don't have diabetes. Only like about 14% of the records are people that do have diabetes. And then of course did some visualizations to try and find which features were the most important. The reason I'm not using scatter plots because the integer features were bucketed and so all the points would be drawn on top of one another. And then I use some, some box and whisker plots to detect the outliers in the integer features, which as you can see, there's quite a lot of them. So for my data cleaning, what I did is I Went and got rid of the records that contained outliers. So if any of the any of the columns had an outlier value, it would be removed. And so that reduced it to about 160,000 records comparatively. So if you look at the differences in the box and whisker plot, as you can see there's far fewer outliers. And the ones that are here, these weren't outliers in this original graph. It's just getting rid of all of them causes the outlier calculation to be different. But yeah, but I did that to remove all of the outliers. And then of course, obviously I want to look at the different values, see how they correlate with each other. And there were quite a few of them that had a very low correlation with each other as well as with the diabetes binary which we're trying to detect. So what I ended up doing is anything any of the feature columns that where there was a correlation coefficient less than 0.05 or well the ABS or the absolute value of the correlation coefficient was less than 0.05. I decided to drop those to get rid of any noise that might interfere with the machine learning models. And so these are the matrices before and after. And then at this point I converted it from a PANDAS data frame to a SPARK data frame for the machine learning portion of my pipeline. And then I use a standard scaler for all the features to make sure that the feature that specific features weren't throwing off the model. And both, and this was for both the testing and training data sets. So because this is a binary classification classification problem, I went with both logistic regression and the random forest model since those are for binary classifications. So I use a param grid and in order to tune the hyper parameters. And I also implemented five fold cross validation to get the best models. And these are the graphs that show the convergence across different iterations for both the logistic regression and the random forest. Both of these show a similar pattern, though the random forest does converge closer than the logistic regression does. And so for my logistic regression, these are the results that I got the accuracy recall were quite high. Got the this F1 score in the area under the curve course. Something I'm noticing is a lot of false positives likely because of the fact that there is such a big class imbalance. So a lot of thing, a lot of them which are actually supposed to be a lot of patients that have diabetes are predicted as not having diabetes. And you can see that with the ROC curve here. And then the random forest model is just ever so slightly better about this. There's a higher amount of people that have diabetes that are predicted to have it. And the area under the curve and the F1 score are just a little bit higher, but not by a significant amount. And of course, the accuracy and recall also haven't really changed that much either compared to the logistic regression model. And so overall, the biggest thing I've noticed is that the CLASS in both of these models are very sensitive to the CLASS imbalances that are present within this data set. And so because of that, the false positive rate was quite high, slightly more for the logistic regression, but both of them had this issue. So in the future, if I were to work with this more, I would try to, to implement some sort of sampling where it would equalize the class imbalances in diabetes binary to account for this and try to get the model to not have so many false positives. And one more thing I should note is that the random forest model, again, it was just ever so slightly better, I'd say, at evaluation than the logistic regression model. But it took much longer to train, especially when I was doing the cross validation. So, are there any questions? Excellent presentation, Ben. So one question is about the feature engineering part. What challenges did you face with this feature engineering for this diabetes data set? The main thing is that there were a lot of different features that, like, look like they could have been, like they could have been relevant and a big indicator, but also, but it wasn't immediately obvious. So I played around with different thresholds, with different thresholds of these correlation coefficients to see how I could, which features were the relevant ones I should drop and which ones I should keep. And overall, I found this value to be the best from what I've tested. But I do think there's a bit more that I could do here, like with feature engineering. Got it. Excellent. Thank you. Yep. We'll go with next person here, Aaron Rosenberg. I have a question. Can I ask? Yes, please. Go ahead. Not a question to challenge anyone, but to understanding myself, you know, to give some context. I, when I was a. I was about to say when I was a student, because this is my second time to be a student, so. So once upon a time, I used to work as an, as an ga, and then there were a lot of statisticians and they would collect millions of data and then do this analysis, right? Then once I asked, so what do you use? How do you use this in a research paper? And he was like, I don't know. So. So now if you have a, if you have a question in your mind, suppose if you're doing this a paper or an essay, right? What would be your problem statement? My problem statement would be, how can I use these different health indicators to try and predict if somebody has diabetes so that we can get them the treatment they need in a more quickly and efficient way? Okay. Okay. Thank you. Thank you, Aaron. Okay, so my name is Aaron. I did my classification on a kinematic data set. So the data set that I use for this project is a kinematics data set I obtained from Kaggle. According to the source, the person who posted the data set, he said that the data is accelerometer data collected from an iPhone 5C in 10 second intervals at about 5 samples per second. And so the data set was tabular, contained about almost 90,000 rows of data across 10 columns. And each row consisted of a time step and a date. And it had acceleration data and gyroscope data in each direction, X, Y and Z. So that's. And then there was a username column that that column only had one value and an activity column that was labeled zero for walking and one for running. And then there was also a risk column that just told you which hand the accelerometer was in. So the picture of the accelerometer that I have up here just kind of shows what, what I assume are the, the directions. So, so the forward acceleration in this case is the Y axis and then X is lateral and then Z is vertical acceleration. And then the gyroscope values are rotational acceleration around each axis. There is no sampling required for this data set. The. The 90000 rows fit into memory in Spark. And there was no class imbalance for this data set. It wasn't perfect 50 50, but it was within about a hundred samples. So very close. And so the objective for my presentation or for my model was to just classify using the accelerometer data whether the person was walking or running at the time. So for the dataset, overall was pretty clean. I didn't have to deal with null values or any inconsistent data. Each sensor reading had a lot of outliers and I wanted to get an understanding of these outliers. So I did, you know, hair plots and, and the violin plots, and I grouped them by activity. And what I found was that in many cases most of the outliers were in one, were in one class, the running class, which makes sense. And so since I didn't already have a class imbalance, I Didn't want to create one by deleting outliers or anything since the majority of the outliers belong to a single class. So for now I decided to proceed with the outliers and just kind of see what the results were and figured I would just readdress them later. The pair plots also kind of show some non linearity between some of the features. And then for the correlation plot that I've got there, which makes sense, the forward acceleration has a really high correlation with the activity. And then you've also got some high core, some decent correlations between some of the other variables like the lateral and vertical acceleration and some of the gyro variables. So for feature engineering I, I kind of broke my, my project up into like four experiments. So the first experiment was just kind of my standard pipeline. I used all the features, I didn't do anything to the outliers. You know, I used the scalar and I tested it on two models. And then in each of the, the following experiments I kind of just changed one value or one thing that I did to see what the results were. My data set was totally numeric, so I didn't have to do any like one hot encoding or anything. And I did perform scaling which I found to be unnecessary because all my variables were. They were pretty close to each other and they were pretty close to like the standard normal distribution. They're very small values and I didn't have any need to transform features either. So I did remove unnecessary features like the dates and the timestamp and the username were all unnecessary features. They didn't need them for this. And then the wrist is one of, I handle in one of my experiments and experiment four to see if, whether, whether we need which wrist it was on at the time. So for training I split it into. I did a 7030 split between train and test. I just, it was kind of arbitrary. I used two different algorithms. I did logistic regression and random forest and I performed since I was, since I was doing so many models, I just did four fold cross validation to save myself some time. I applied parameter maps for each model and the parameter I chose were. Were these also kind of arbitrarily chosen? I just wanted to see what the model would, would give me. Here's an example of the convergence for the. The first for experiment number one, the logistic regression, it converged in about 19 iterations, so which was actually the longest. Somehow the other experiments converged a little faster but not, not a huge difference in time or anything. So for the results, these are the results for, for the, the first experiment. And then so overall logistic regression, I got 86% accuracy for random forest, I got 99% accuracy. Something I noticed looking at the, the results for the logistic regression is that I think the model kind of overestimates walking a bit, which makes sense to me based on how clustered all the points were for walking. So I think it's pretty easy to imagine that if some of the running data points happen to be close to that, it would be pretty hard to draw a decision boundary. And then, yeah, the logistic regression with 99%, there's not much to say there that was really good. And then I compared between models and you see all of them are relatively close. Probably the largest change was Experiment three, which is where how I handled the outliers in experiment three was that I decided to kind of impute all the values to the median. So for each column I found the outliers. And then if it was an outlier, I just changed the value to the median. And that probably had the worst results at 83% accuracy for logistic regression and 97% accuracy for random forest. Removing the wrist variable had a small impact to accuracy, but nothing huge. And then removing the scalar from the, from the pipeline had no change, no effect at all, which, like I said, made sense because all my values were already kind of there. It probably didn't change the values very much. So in conclusion, overall, the random forest models outperform the logistic regression in each experiment. But overall, the, the, the logistic regression models still weren't, were still pretty good. I mean, 86% is good. There were minimal changes between the experiments. And so I think probably the best case in this case would just be to choose the simplest model. So just remove the scalar. No need to do outlier adjustment. And I think despite the small accuracy hit, I think it's worth it to drop the wrist column just so that there's less data to collect, less features to deal with. And I imagine in practice, like if you were using this to build something you might not have, and you were collecting the data from people you know live, you might not have an indication of what hand the accelerometer is in. So it might make a simpler model with essentially the same accuracy if you just remove the RISC column. And there's my references. Any questions? Excellent presentation, Aaron. I have a question regarding the outlier removal. So in your slide, you, you presented, you talk about removing one of the outliers with median So I just wanted to understand. There are multiple ways to remove outliers. So. As well. What was your intuition to select the median? It doesn't have to be the right way. But just out of curiosity. Yeah. I. I had looked into a couple different ways to remove the outliers. Like I said, I didn't really want to create a class imbalance. So I figured just if I just identified them and just changed them to the median, I just wanted to see what would happen. I just chose the median because I figured it would have less of a. It would. It would change less as a result of removing the allies. Perfect. Okay. Thank you. Yeah, no problem. Siddhan. Sure. Just a minute. Is my screen visible? Not yet. Still. Is it? Can you move something? It's just a gray screen. Just a minute. Is it. Can you. It doesn't show. It's just a gray screen. Yeah, give me a second. Okay. Is it visible now? Not. Yes. All right. So. Hi everyone. So the assignment goal was to build a supervised machine learning pipeline. So I chose census income to predict whether an individual earns more than 50k per year based on various demographic and employment related attributes from the US Census data set. So this is a binary classification task. And the project focus was to build an end to end development from like the data ingestion cleaning to feature engineering training and tuning, hyper parameter tuning and the evaluation. So the first step. Yeah, so the first step involved in setting up a spark session to like handle large scale data processing. I loaded the census income Data set using PySpark, since the data set was not having headers. So I explicitly defined 42 column names including our target variable which was income. So then I identified the integrity of the loaded model by confirming the column count and previewing the sample records. So next I conducted the exploratory analysis and handle the missing values, identifying the missing values which were represented as question mark placeholders and like calculated their distribution across the columns. So reviewed the distribution across key categorical variables like income and education to understand the data balance. So and transform the target variable income into a binary numeric column. So to perform. To facilitate the correlation analysis and perform the correlation analysis between the numerical features and the target to find which were the influential variables. And so the next step was to deal with the outliers and transform and a data transformation to prepare it for the modeling. So the columns which were having a high percentage of missing data, like let's say over 50%, I identified them and removed them. And missing categorical values I replaced with the mod values and the numerical values I replace with the median so and filter out the rows which had children under education so as they were like anomalies or outliers. So I also managed the outlier by like capping the numerical values like capital gains and dividend and did the for the numerical columns I did the scaling to make the data set ready for the machine learning part. Next up is the feature engineering. So to help convert the raw data into the features which were suitable for the machine learning model. So categorical variables were encoded using the string indexer and the one hot encoder which helped in converting categories into numerical vectors. And numerical variables were combined using the vector assembler and which were like standardized with the standard scalar. Finally like, both the categorical and scaled numerical features were like merged into one large feature Vector of like dimension 365 forming the basis of our model's input. Then the next was like. Once the features were ready, I split the data into training and testing like 80, 20 to evaluate the model's performance. So I trained like multiple classification. Model 1 was like logistic regression random forest SVM and to identify like which was performing better. So the logistic regression area under curve came out to around like 0.92. And similarly for Random Forest and SVM they had like competitive performances like 0.9 and 0.92 respectively. So after that I did the hyper parameter tuning and the ROC visualization. So I improved the performance a little further by like by tuning the hyper parameters like for random forests classifier with the cross validator and optimize the parameters like num trees and maximum depth to enhance the performance. So I was able to increase the performance and the age under curve came out to be around 0.91. And if I can show you guys the plot of. Can I stop the screen and share my screen again? Yes. So is my screen visible? Right. So the ROC curves are plotted for the logistic regression and for the random forest so which illustrates like both the models were able to distinguish and distinguish between the income classes clearly. And finally like I evaluated the random forest model comprehensively using the confusion metrics to understand the performance matrices like the Precision Recall F1 score and yeah summarize the complete pipeline which had like data ingestion, cleaning, feature engineering, modeling and evaluation. Yeah, excellent Siddhant. I want just wanted to understand the data set again a little bit. Could you, could you go to the slide where you explain data set more? Yeah. Is my screen visible? I think it's going to take a minute. Yeah. Okay. Just a minute. Which slide you are seeing? Do you have a slide for where you talk about data set or. Okay, so. Okay, so your data set consists of 42 columns and your. Okay, and was this data set publicly available or. Yeah, it was available publicly. And what, what is this data set called? It's called US Census data set. US Census data set. Okay. Okay, thank you. Thank you. Next we have Joshua. No, we have Ethan Ether. Sorry. Okay, so me and Shahad were supposed to go next after. Okay. Sorry, I think I got it wrong. Just let me make sure that. Yeah, Shafi and sh. Yeah, sorry. Okay, thank you guys. Shop, you can get it loaded up. All right, give me a second. Gotcha. Thank you. Yep. Can I see my screen? Yeah, got it. Yeah, go ahead. All right, how's it going everybody? Today we are presenting on our supervised classification problem assignment 3. And it is using the same data set that we use in our prior assignment, which is the Chicago traffic accident. All right, next. Shop. All right, so like I mentioned, our goal is to build a supervised classification model using multiple machine learning algorithms. We use logistic regression, decision tree, random forest, naive Bayes and MLP neural network. And the purpose of this correlates with what I do on a day to day for the dot. Pretty much validating accidents using crash data analysis, pretty much justifying transportation engineering and the road safety that's out there to eventually down the line justify funding allocation. And this is like millions of dollars that needs to be justified. If a certain intersection is warranted. 7 3/4 million or this intersection is warranted. X, Y and Z. The data set is named traffic accidents. That's the kaggle link right there. It's about 209,000 rows, 24 columns. The objective for this assignment was to predict like the injury severity and the counts using the categorical numerical attributes that are already in the data set. And the target variable or feature that we wanted to work with was the injuries total, which we later converted to a classification. Next. All right, data loading was very similar to the prior assignment. We loaded a full data set like I said mentioned. It has 209,000 rows, 24 columns. 10 of them were numeric, 14 were categorical. On the right hand side you can see the code or the results of the code trying to identify what kind of data type it is. Objects are categorical floats and integers and numerics. No sampling was needed because the data fits in the memory. We use this, we built this in Google collabs. The spark integration was not used. There are some of the typical Tools that we used for our code. All right, next. Okay. All right. Our data set was tabular and multivariate. The target column, which is the injuries total, was continuous and just parts of good data set, like I mentioned before, is a mix of categorical and numerical features. Try to understand how they all kind of interact with each other and try and understand the logic behind it. So next, data cleaning. So this, this data set was based off of the city of Chicago and their crash data analysis, their Sea Dart database and their Department of Transportation. There wasn't really any missing or null data because the data original data owner for the gaggle set already pre processed it and removed all missing and null datas and kind of entered unknown and or other to fill the missing values. So when we ran it showed up as no missing or null data. So we went with that. All right, so the next few slides are going to be data visualization. This isolated slide is about the class imbalance for the targeted column, which is injuries total once again. And if you look on the slide, you see that a majority share of the injuries reported is actually no injuries. So zero injuries in total and fewer records show that like there's high counts of injuries. So. Yeah, yeah, sorry. Next. This is some more class imbalances. Our data set actually have a lot of class imbalances had more balances than not. So here's like one is first, crash type. What kind of types of crashes? The lighting conditions, the most severe injury and accidents by weather condition. So you can see here people assume that most accidents happen in rain, snow, which is true. But they do have accidents. But most likely it's on a regular clear sunny day that the data proves that. And working in the industry that is true. So we can go on next. This is just distribution of numerical value, numerical variables. So just kind of understanding what the data is trying to tell us. We can see like the crash hours, which is like the peak hours that crashes happen. The amount of vehicles per accident or incident. The relationship if it's linear or not, which it is the number of injuries versus number of units, which is number of vehicles involved in an incident accident. So most of the crashes are going to only involve two vehicles. So more the vehicle, more involved vehicles have a trend of less amount, lower amount of accidents or injuries. Excuse me. So we go next. All right, so we use box plots to visualize the numerical features to see the distributions and the outliers. So this is all on the different kind of columns for the injuries and types of injuries. So you can see incapacitating non incapacitating injuries not reported. Reported but not evident. So like somebody potentially, you know, being fraudulent, like hey I got hurt but they have no evidence that they got hurt or like no doctors or stuff like that which happens in insurance claims. Reported total injuries, the target column pretty much injuries that they're fatal and injuries of no indication. So they were never reported or so into the eyes of the government or the dot, nothing happened. So it could be a zero injury. So next, this is more of the relationships. So I was mentioning earlier the amount of injuries and the number of vehicles related. We can see that two vehicles produces the most amount of injuries. While we go towards the higher number of vehicles involved in an accident causes lower amounts of injuries. Now on the right hand side we have a crash density chart where it shows that between the crash hours and the days has a spikes of amount of crashes reported. So this kind of shows like the relationships of the data which reflects into a real world. Okay, so we do some more outlier detection because there's a lot of attributes in our data set use some violent inset boxes. And we did lowest trend just to kind of get more concrete on which feature or column we wanted to target, which like you said injuries total. So all right, I must pass it over to Shafi. Thank you guys. Yeah, so after we thought like which column to target or something like that, so we thought of using injuries total. So we started with engineering like we used one hot encoder from Scikit learn to do categorical encoding and for scaling the numbers of each column or each labels understand a scalar level. So that's easier for the ML algorithm to learn and train on. Also like as our target is easy is total. So we used multiclass target labels to better process the whole thing for the email because we are using multiple algorithms here like more about five. So yeah, we have to the most easiest one and most making sense one to use here. So we use all of them combined with a column transfer transformer to get our pipeline. So as you can see from here like these are the levels we can getting encoded and else out there like a lot of class imbalances. So these are the results we got. So we'll talk about it later. So one of the main influence for us choosing our like English total as our main target for training and predicting on is that the PCA by plot as you can see here, the most two things contributing to any kind of impurities or any kind of data visualization in our whole thing is that number of events and injuries total so there are the visual components. So also try it with like most severe injury or death or like any kind of pattern injuries but there are less status on that and the EBNS is too high on that too. So that didn't work out much either. So insurance total is the best bet for us to make something out of this. So yeah as there are less data we would have gone like 60, 60% test, 20%, 60% training, 20% test and 20% validation. But yeah as a total number of injuries is really low. Yeah I could create some problems. So best keep it simple and do it will display it and that models we use a logistic equation because let's say from linear baselines also use discriminatory and nano forest to find some non linearity on hand and live versus MLP for further seeing like how better it can handle things. Also we, we limited our iterations for MLP up to 300 because we don't want it to run for long run for longer time. We have to keep it simple and make it time sensitive or make it proper times. Yeah so these are after running all the models, these are confusion matrixes. So you can see from here that all of them labeling the registered are pretty fine. But as you can see from here like the other levels, not all of the models are performing real that much good. So some models are performing like others. Some models are activating a bitter or so yeah there can be much more experience than using these models or using other models. And eventually an app can be developed to like where you can input weather condition, the temperature, the approximate location of the predicted accident and the model can tell us like the probability of that accident happening. So let's have a performance evaluation as they are like really high class imbalance that Shah talked about before. All the models are like giving really good results. So yeah we had to go to like 4, 5, 6 points after decimal to have actual differences between the models. So if we include some other complex feature engines into this model or use smooth we can have better results in like again the outputs. But for now this is good enough. Yeah so these are limitations for us like severe plastic bio also knife bias and and apart from because we got some negative inputs in our model and we had to fix that using Gaussian Lear also. Yeah logic regression struggle a bit with minority classes. And also we did some cross validation and hyper parameter training before because first we are like training our models with fatal accidents or major severity accidents and for each model it took about 20 to 24 minutes to do the Cross validation and have parameter training. So we just didn't include this for our exist target column. But yeah it can be done. It is pretty impressive. And with helper parameter training the difference between the five models become pretty much like pretty much big. So that way we can make more differences between the models and decide what model to use and how to use them. So yeah. So future if we want to work on it or if anybody any of you want to work on it. Like you can use Spark for stability and you can use SVM XG boost or even hybrid models like CNN based models or DN based models to have more, you know, robust models and do a little bit more experiment on that. Yeah. So simply like with whatever information you got decision tree work best because there are some in long non linearity within our data. And yeah it worked and it gave us idea into how weather, lighting and like every other conditions are like affecting our injury count and affecting how the drivers react in a real life situation. So you guys have any questions? Excellent presentation. I have a question about. So if I understand this correctly, you are taking some features which could be like weather, lighting surface and how does it impact injury count? Now if you, if you go back to your problem statement, you know, if we have to take the build a class supervised classification model. Okay. Guide road safety policy and state federal funding. So if I, if I believe it, you know, is your problem statement more on how well the funding has been done or whether to fund a project or it's more on like looking on the, the data and trying to predict whether the accidents will happen or not? A little bit of both. It's. Well you got both sides of it. So it's pretty much a justifying if a certain intersection needs to be funded and then the like it needs to be funded and upgraded and then. Okay. And I mean we do that so we check for. I'm trying to. I was curating this, we were trying to curate this to what I do at work. It would have been much easier if I used my own data sets but those would get me fired. But so essentially what happens is you got a record accidents. So from there you pretty much warrant if a transportation engineer or any kind of transportation upgrades are warranted. Cool. Then after like two years we kind of collect a new set of data and it's like was it worth it? Okay. Because every set year, like every contract year we get several million dollars to do something with it. Essentially we have to use it or lose it. Now that has a relationship with the safety of the the road. Are we decreasing accidents? Are we, what are we doing? Or is the numbers being like stagnant? Essentially just like there's a much higher the purpose here. I think it would kind of. I brought it from work, I'll tell you that. Okay, so, so it's like, so it's, it's to. If I had to say briefly, your problem statement is about whether we should, you should upgrade it intersection Based on historical data. Yes. And historical data is accidents here. Yes, indeed. Accident. Okay, so now, so, so this upgrades how, how free, how far, how. So what are the variables here? Is it only the weather? What does this accident data contains? I mean I saw a few, few, few, few data here. Yeah, so like apart from date and time, it also contains like what kind of control devices being used, like police or traffic lights or stop sign or like yield sign, stuff like that. Also it contains weather condition, lighting condition, first crash, like sometimes like consequent crashes happen. Also it has like alignment, like how the roads is being done, like how if the road is curved or if the road is like carved with a tilt or something like that. And also like a bunch of more data like if the road is damaged or not. Yeah. Is it, is it so yeah, I, I thank you for that. So I'm understanding these, these features here. So but the intersection, now the accidents can happen in intersections or the accidents may not happen at intersections. Right. So in this data did all the accidents happen? Does this only contain accidents for intersection? Or it contains data where accidents didn't happen at intersection. So traffic way type. So it, it contains data about all sites of road, like all of like Seattle city. I think it's all of Silico city downtown or brother Chicago city. Not sure of the location, but it doesn't. Only about intersection. Absolutely. So you, you have data where there could be accidents that have happened at intersection and data that the accidents were intersect. It's not intersection. So now your target variable, what is your target variable variable from this data? Our target variable is the injuries total. Like how many people are getting injured or any location or any kind of accident. Okay. Okay. So, so given all these features, how, what is the injury? Cvrt. Okay, yeah, like, yeah, like shadow explaining. Like so depending on the type of injury and the injury players and number of injuries, the government or the department of transportation can say okay, we need to focus more on this area or this junction of the road. It might be intersection, it might be a straight road or it might be a curb road. So yeah, depends on that. Okay, let me just Add some. One more line for it. So I may have made worded my words incorrectly. So when I said earlier about like the federal and state funding they kind of do with the intersections. So municipalities and townships, they do everything else. It could be like the curve road and stuff like that. So that's where I should, we should have worded it government based. I'm just so used to it at work. It's like we do this, that's not our problem. We do this, not our problem. So I might have, like I said, I brought it. Got it. Very interesting. Thank you. Thank you for that. Yes, excellent. Thank you. Yeah, so we will go with our next presenter here, Ethan. Okay, one moment, let me share my screen and after Ethan we'll take a five minutes break and resume. Yeah, okay. Hi everybody, my name is Ethan and this is my presentation for assignment three, building a predictive model. So I carried along and used the same data set I used for assignment two in this assignment. So some things are similar, some things are different here. So again the data set I used, it was a Netflix movies and TV show data set. And this is just an overview, I'll do that first. And basically I engineered a bunch of different features, trained the model and then I got an 87% accuracy on predicted predicting whether a movie was recent so coming released after 2020. So data set summary, like I said, it's a Netflix movies and TV shows data set. It contains around 8,800 titles on it. And looking at this, this is just a visualization showing class imbalance between movies and TV shows in the data set. Some of the key data sets that we'll be utilizing are duration of a film and specifically I'm going to be looking at movies going into this, into this project. But basically key key columns are duration, the cast, what genres which is listed in and the release year. And like I said, I filtered just to be looking more so at movies. But this is what the data set originally contains. So what I wanted to do, like I said was predict whether a movie was released after 2020. So I needed to first create a few variables V and do some feature engineering. The data set I was working with doesn't have a lot of integer based values. So I had to create a lot of those. So that's where some of those key columns I discussed. So I basically feature created and I got variable for the number of actors, a variable for the number of genres and I got duration as well. And I also featured engineered the is recent variable which is our target. Basically whether movie was released after 2020. So yeah, because a lot of it, a lot of the variables I was working with was kind of just messy text, so converting it into usable numeric features. So what I have here to null handling and cleaning, there weren't hardly any nulls, at least specifically for the movies. But trying to run this model and do the predictive stuff with basically with the nulls was I had to get rid of those because it was preventing me from running the model. So I cleaned that and made sure there were no nulls. There were barely any. But here's just kind of a visualization of the class imbalance. Here's a couple visualization that'll be coming up, but this one is specifically the number of movies on Netflix, whether they were basically released after 2020. So there's a lot more movies that are released before 2020 on Netflix. A couple more visualizations here just to kind of look at the data. Just kind of a distribution of the movie release years and the, the, I guess the frequency, the. The count of the number of movies released in a given year. So a lot released before 2020, when we're looking at the grand scheme of things. But there is a cluster around 2015-2021 with the most amount of movies on there. And this is just a box of whisker plot. It's showing a similar thing just kind of looking at the whole scale of the movies released. And again, like I said, there's a big cluster around 2015-2021. And this is a quick correlation map with some of the variables that I, you know, the variables that I've discussed that I featured, comparing how they relate to one another. There's some higher correlations and lower correlations. The highest one is between duration and the number. Number of integers. Well, the number of genres, I mean, and then number of actors and duration. That's also relatively high for correlation. But there are some other variables that are unrelated here as well. I did, whenever I was doing my predictive model, determine whether there was a movie released after 2020. Looking at all the variable. Looking at all the, the instances in my data set, I did do some feature scaling. I used its standard scalar to normalize numeric variables, numeric features. Because, you know, for instance, we had duration, which, for instance, like a movie could be 100 minutes, but then we have like number of genres, which could be like 2 to 3. So we needed to normalize that so it'd be ensuring fair comparison for all variables during the training process. But I did, I used Random forest classifier as the model for my study. And I did an 8020 split. And again, some of the features, I trained it on duration of the film, the number of actors and the number of genres. And there was a label given whether it was recent. And the model did predict that it was 87% accuracy. So basically, 87 out of 100 times the model correctly predicted if a title was recent or not, meaning released after 2020. So like I said, it had an 87% accuracy. This is just some of the strengths and I would say that's relatively good using the numeric features that I was. That were given. And I did want to look more specifically at movies to give a more nuanced kind of perspective. And also too, some of the duration formats for TV shows were kind of. They weren't consistent like movies. So three seasons, 20 episodes. So decided to do that, but it did relatively well. And then some of the limitations here as well is that I believe that, you know, the is recent. I derive this variable out of the release year. So I didn't know if that was necessarily a totally independent prediction. But it still, you know, it wasn't 100% accuracy, it was 87. So I think it was still relatively separate. I only use like three numerical features. So potentially there could have been other ways to predict as well. Maybe genre, type, description, country, different things potentially there. But again, I only use three different numerical features. And again, one of the things as well that could potentially be an issue, something maybe to look into later is how there were not as many recent movies compared to movies, you know, before 2020. So that potentially have affected a performance. So that would potentially require more things to look into down the line. And I only did look at one model in my study. So that is something going forward to look at different models to compare between the two and to look at other categorical features, country of release or genre, things like that, maybe predictors, some things that were on my mind. So yeah, that is my presentation. Excellent presentation, Ethan. So I have a question about what do you think is changing from a features point, maybe after. Maybe before 2020 or after 2020. Or if to say, in other words, how does. What are the features, more future values, more looking like for recent movies and movies that are not recent. So you're talking about like the duration. Like. Yeah, what is changing? If you call. If there is. If. If you give some features, right. Abc and the model says it's a recent movie versus you give some features and model says it's not a Recent movie, what do you intuitively think is the deciding factor in your feature values that is making the model to save recent versus non recent? I actually would like your opinion because I'm not. Because it seems like a lot of things to me would carry over between like, you know, if a movie's released between, you know, 2019 and 2020, there's probably going to be movies with similar lengths and similar number of actors and similar genres, so. Yeah, I actually am not sure. Would you have an idea for that? I would. So, so when you were doing your data analysis, if you look at the features before 2020 and so is recent means. Is recent something means 2024, 2025 or it's anything that. Okay. What is your threshold for recent? I derived the is recent from the a column titled recent release year for movies. So basically, basically it was just. Basically if a movie was released after 2020, it was marked as yes, it is recent. Yeah. And your data set starts from which year? Probably like 1925, I think. 1925. Okay. Yeah. So I was just curious to. To understand that if. What, what, what is recent? What and what are the top top three features or few features you're using? I'm using. I go back to show you. I'm using duration. Duration, number of. Number of actors, number of genres. So a movie can contain number of journals. Okay. Okay. So these are three things. Yeah. So if you were doing data analysis, I don't know, I mean I feel that in the data analysis, if you had looked, maybe if you had some bins, like you know, 19 for every 20 years. Right. How, how these features are changing, maybe that may tell, like, you know, what is the trend? I was just out of curiosity trying to understand what, what is the new trend looking like Right. To me. That's nice. Yeah, that's a good idea. Something to think about going forward. Yeah. Thank you. I appreciate that. No problem. Yeah, I might think that there's a lot of collaborations going on. Right. You have multiple actors coming up in a movie. That's the trend, at least in general. Without the machine learning. I can see when I watch a movie that there are many actors. Right. So maybe the number of actors have gone up the or. Right. Right. There might be a big difference between a movie released in 1920 versus a movie released in 2020. So. Absolutely. Yes, for sure. Okay, thank you. Thank you so much. I appreciate it. Yeah. Joshua, before. Joshua, we will take a five minute break and come back. Okay. It. Okay, we'll continue. Joshua? Joshua, are you on Mute. Sorry, I just got back. Can you see my screen? Yes. Okay, so. Good evening everybody. So assignment three, predictive machine learning pipeline. So I'd use the same like most everybody else, I use the same data set from the previous one. You need to. You need to share your screen. Is that sharing? Not yet. Let me see. No. Yeah. Okay, so. So for loading the data like most everybody else, I use the same one as I did for the previous assignment. I use the institutional books. Basically Harvard and and some other universities have been scanning in old ish books and then putting them up in text files for people to use. It's advertised as a learning mod for large language models and for teaching them. I just mainly used it to since it was multiple languages to see if it could figure out and classify the languages correctly. I just cared about two data points, one being language and the other being text. I worked on Google Colab to start. Ran out of memory, ran out of ram. So I moved over to Odu Wohab. I pulled down the first hundred parquet files. There's something like 9800. It's like one terabyte. I actually ran out of room on the ODU one too. So I just scaled it back and used the first 100 files because I was running out of both RAM and time. So the initial data analysis, the data set was tabular and it was multivariate. Like I said before, I used the two call. The main two columns were language and text. I keyed on language and then tried to train it off text. The text was an interesting field. So basically every page as they scanned it in, they set it as an entry to a list. So it wasn't a. So it kind of looks like a giant string but when you parse it out it's really not. And that caused me some difficulties, you'll see later. Bit there was really no linear nonlinear relationships because it was basically a category and a text column. So data cleaning the data was pretty clean. I did find a couple languages, I think it was two that were set to null. I'm kind of curious if that's bad data or if possibly the language was like a made up language. Because I do know there's historic books out there that are written in quote unquote unknown language. Or maybe it was written in a language that doesn't have an ISO code or something like that. I just set those to na. I didn't see any of any of the text that was set to null or empty or zero. Everything had text. So data visualization, visualization Here is the distribution of languages. As expected, English is a huge chunk of them. Since I did an 8020 split, that means I needed to split my data into five chunks. So any language that has less than five wouldn't work. So I clumped those together into a language that I called other. If I could load more sets and get more texts of all the languages, I'd be curious if my numbers would go up and if I could just get rid of this. But since I was sampling the data, I had to do this. Here's for page counts by language. It's interesting per language, where they're at and their average pages. I'd be interested to be able to load more data and see if there's more in there. I'd also be curious to maybe run distribution by language by year. You know, are older manuscripts longer or shorter compared to modern ones? Here's distribution by page counts and their frequency. As you can see, most of them are between 200 and 500. After about 500, they start petering off. But some of the manuscripts were ginormous at like 18 or 1800 to 1900 pages, which would make. Which are good because it gives the model more to go off of, but it does take longer to load. And then here's average page count by language, which I thought was kind of interesting. English is actually in the middle somewhere here. English is actually over here. Some of the other languages had really large manuscripts. Be kind of curious to look at those and see what were that? What are they? You know, are they great sages, you know, biblical books? Or are they academic works? You know, that can be extremely long? So I did have to create a data point called pages to count the number of pages. Basically it was an entry. Every entry line in the list became a page. And like I was saying earlier, that caused me issues because apparently NUMPY turned all my lists into numpy arrays. So even though they're extremely similar and they have similar functions, you know, count, length, stuff like that, they are different. And my model balked. It was telling me that because I was getting pages of zero for the longest time, I finally told it to output the actual object. And that's when I got the NUMPY array instead of just object, which. Which immediately after that I was able to just everything that had list, I just replaced it with numpy array and everything worked fine. Like I said earlier, I was. I did 8020 splits five times. So everything was tested. Once I was attempting to try three data modules, logistical regression and support Vector machine both failed. Support vector I think timed out. Logistic regression threw me an error I wasn't able to resolve. The multinominal Bayes did work and I put down here again. Since I was doing the five splits, I had to create the other language and par all languages that had less than five five texts into it. Since you couldn't do a train with less than five, the performance, the tuning took an hour and five minutes. The accuracy rate came out surprising. Well, basically 97%, you know, 96.85, 0.9, 97. The K fold took two hours and 15 minutes. So altogether three hours and 20 minutes. I would be interested to see which ones failed. I do know some languages are very similar. The ones that come to mind are Portuguese and Spanish are cousins. I'd also be curious if the length had anything to do with it. You know, a shorter work that has similar language that has similar words may be misclassified. Lots of languages use English loan words. I'd also be curious about that. You know is a short technical work in German that's using lots of English technical words being classified as English. Be interesting to look at in the conclusion. Even good high quality data sets may still have some bad data points. And know your data, know your data objects, similar data is not the same. So that's all I have. Thank you. Any questions? Excellent presentation, Joshua. So one question I have is about if you would like to share. You know, you had 1 terabyte of data and some of your models took so much time because there's so much of data there. So in the start of the presentation you were trying to explain explain how you made a switch from Google Collab to Odoo Cluster. So if you could summarize or get us some insights on what were your challenges with dealing such an amount of massive data. The biggest challenge is time because it takes so long to download. That's the reason I ended up downloading the first hundred arcade files and uploading them is because to stream them it would take five or 10 minutes. So anytime I wanted to tweak something, I was taking, you know, five or 10 minutes just to stream everything again. So that's the reason I pulled it down locally. Google Colab was giving me strange errors that didn't make sense. I just happened to notice that the resource corner in the column was kind of spiking. So I like brought it up and watched it for the whole five minutes and sure enough it went to 11.9. We're allowed 12 and then it crashed. So Then I just brought everything over to the ODU like you recommended, ran it in there. I tried to grab more than 100 parquet files and I tried to grab actually the whole, the whole set of, of 9. I think it was 98,000 files actually. And I ran out of room when I was into the eight thousands, but it didn't matter because as I saw, even with, even with 100 parquet files, which gave me the 10,000 rows, it still took four hours if I had more. And I also had a problem where the first time I was running it on ODU, I couldn't get a GPU. So I was running it on 40 processors on the same CPU and I was still getting timeout errors. So I think Yesterday evening about 7 or 8 o', clock, I tried to spin up another one with GPUs this time and it succeeded. And as I was saying before, before this class started, I had to end up letting it run overnight. Got it. Excellent effort and excellent presentation. Thank you for sharing. Can I. Not the question, but I'm curious. So professor, now in this kind of giant amount of data, would Spark help or did you, did the, did the presenter use already Spark? So he's using. See, Spark is a framework. It's end of the day. It's a software, right? So it's a, it's a, it's a, it's a. It's a set of programs within a. And framework is you can call a set of tools together or a set of packages and programs together, right? So Spark is a framework to deal with big data. So it's designed to process and store compute massive amounts of data. But it's just a set of libraries. Right, but what do we need more than Spark when we deal with big data? We need infrastructure. And what is infrastructure? Infrastructure here means cluster of, of nodes. And these nodes are these nodes of cpu, are these nodes of gpu, right? What is the hardware? Where is this getting executed at audio cluster, which is a local data center or, or something in the cloud. So infrastructure, where actually this code is getting executed, executed in terms of the data getting loaded into the memory, execution in terms of the data getting computed, the results getting stored, the data getting loaded. All this is needs to happen on infrastructure. So infrastructure may the hardware maintain is a main place, a key role. Now when Joshua was working with us, definitely he was using Spark because without Spark you cannot work on these, these clusters. You need a framework to do that. But he started with Google Collab, but in Google Colabi ran into multiple Issues. One major issue was about the memory, right? So he went to a Wahab cluster and Wahab cluster I believe has, you know, both different configurations, one for gpu, one for cpu, whatever is available because it's on demand. So earlier he did, he couldn't get it through GPU so he tried CPU. But yesterday I think he got GPUs as well. So he tried couple of things. And, and you know, when you're working with massive data and if you don't, I would say even I'm trying to get some dedicated infrastructure. But you know, dedicated infrastructure in terms mean that suppose I give you two GPUs and that's your throughout the rest of the course. It's super, super costly, right? I can do that. But, but if that's something that's available, right, Then things become much easier. But when you don't have such, such kind of. Because GPUs are extremely costlier the compute. So, so you have to go with whatever is available, right? So that, and that's one of the reasons I'm not putting too much requirements on the data size because if everybody starts taking one terabyte, definitely, you know, audio cluster won't. It's, it's a little bit difficult. So these are the challenges. But if you have a dedicated infrastructure, suppose let's say that, and I have friends who do this, right? So they buy infrastructure for $5,000 because they maybe, maybe they get two GPUs dedicated, right, for themselves and that's on the local machine. If you have dedicated infrastructure, then you will not have these kind of problems. So biggest problem is infrastructure. Yeah. Okay, thank you. No problem. Actually I am curious to the presenter. I am curious, how did he, Joshua, how did you, how did you deal the outlier? Did I miss that part? I mean there was a lot of outliers, right? I didn't really look into the outliers. The two outliers that caught my interest were the two that didn't have a language. And I just set those as, as language of na. The page. Outliers were more of an interesting correlation that I was looking at, but I really didn't do anything with any outliers per se. So another curiosity, professor, in this case, like in this case cases, how do we deal with outliers actually, how do we define what is the outlier here? So outlier in general, you, I mean whatever the data is, you, you when you, when you transform it, you have standard methods, right? So earlier also when you have one of the present we were trying to tell you have IQR and the other way to do it is your, your jet score method. If the, if the data doesn't generally, you know, in a straightforward way go and fit into it, then we have to transform the data so that they can fit into these functions like IQR and JETS code. But there are set of algorithms and methods that are readily available to do your outlier detection. So I'm sorry, I'm taking a lot of time. So in this example now the ones that had so many pages would be our outlier. I'm a little unclear part Joshua. So what do you think is the right outlier for your data? For my data, the outliers would be the ones that had no language. No language? Yeah, there was, I think there was two that the language was set to null. So I just set those to na. But yeah, there the outliers would, would because I had really clean, really clean data. So the outliers would either be a document that has no text, which would be like an app, an accidental edition, or it would be something where the language is not set, which I would want to look into to see why the language would, wasn't set. Yeah. So now, now what I see here is like suppose there were thousands of documents and you know, all documents are some similar. Every document has some kind of a tag which sets what is the language used in the document. Now if one of the document doesn't have a tag now is it an outlier or is it a, is it a missing value? Right. So something, so it's, it's difficult to tell but generally you could, you could also think that if outliers could be something like if all documents are written in English and one document is in French. Now that's the outlier here, right? Yeah. Okay, thank you. No problem. Well, I, I, I'm asked because I was so interested in this study. So thank you so much for sharing. Absolutely no problem. Okay, we, we'll go with Alana next. Okay. Can you hear me? Yes. Okay, let me share my screen. Can you see my screen? Yes. Okay, so hello everyone. My name is Elena albritton and my CS624 Assignment 3 is based off the CARES data set, the same used in Assignment 2. It's also known as the CFSAN Adverse Event Reporting System. And basically what it is is each row is a report, particularly on a particular client, some of the client demographics, what they either consumed or ate or used, and then the potential symptoms that may have come from that product. And then the case outcome. So our objectives is to focus on cleaning and analyzing this data set, visualizing some of the data to find valuable features that we can use to build a supervised classification model to predict a selective feature which eventually turned out to be did a client's case result in death? So we're looking for whether or not a particular case or row had a client's result, being deaf. So our original data set, load and initial analysis. This data set is tabular. It's over 100k rows and 10 columns, so it fits in memory. I use Google Collab. As I mentioned before, each row is one reported case. Some of the primary columns, they're not too many overall, but product category, which there's 43 types of categories, client demographics, such as years, their age and years, client sex, as well as a list of symptoms that they may have had and then a list of outcomes that may have resulted from the case. Because of all of these different types of data types, we can do a variety of variant analyses in terms of initial data cleaning. Like I mentioned before, this data set was used in a prior assignment. So back then we renamed some of the columns, removed missing values, formatted some column data types specifically for this focus. For Assignment 3, we needed to make sure that symptoms and case outcomes were an array of strings, so each symptom was its own string object instead of the entire list being one string. And then visualizing, sampling for the initial visualizations that I'll show on the next few slides. They are filtered specifically on rows that contain the outcome of death. So we can kind of see if any data is skewed to death. So initial visualizations like before related to product categories. It's heavily skewed towards cosmetics. Relating to client sex is very skewed towards females. And the thing that we did new this time for assignment three is I wanted to look at how many symptoms were being mentioned multiple times within this specific set of data. And it seems that it was very skewed between a vague injury type. But as you can notice, the next two are kind of the same. So ovarian cancer and ovarian cancer stage three. So that led me to do even more analysis. So I actually ended up using a natural language toolkit and took all of the symptoms and looked specifically for keywords and did an analysis more focused on that. And now we can see that even though injury is still the highest, we can kind of see from the list you can see in the corner, as well as some of the top four here, they're kind of related to cancer. Related terms. So cancer, ovarian, like different types of cancer, or even like different. Either diseases or causes are usually a result of that end up being cancer. So that was just very interesting to see. So with that knowledge, I decided to focus on those types of words later when we're building the model, but moving on with the continuous univariate analysis. Like last time, we focused on age as well with a normal distribution. But there are clearly some spikes between the age of 60, 60 and 70. And when we split that between sex, we can see that normal distribution again for female. But because female was so skewed, it was interesting to see that males were practically flatlined. Okay. In terms of outlier detection, we focused mainly on age and females because they were the most heavily skewed. And it seems to be very young are outliers and very old. So anything 20 or younger or 90 or older, and the potential causes of this could be that the way this data is created is either by the client or a physician. So this could lead to actual, like, human error, like someone typing in a different. An extra digit or one less digit in case of the younger ages. But this could also be a possibility that some of these clients, these are their actual ages. And it kind of would explain maybe, or at least I would expect a correlation that the older you are, you're more likely to have symptoms that may lead to your death. Okay, now we move on to the main part of Assignment 3, which is features and modeling. So, as I mentioned before about those key words, as well as our main focus of predicting whether or not a particular case is deaf, we're focusing back now on the entire data set. So not just the sampling data set that's focused on deaf, but all cases that can have any type of outcome. We added a new column that's called client deaf, and it's basically a Boolean. So true or false, whether or not that case ended in deaf. And that's going to kind of be our target that we're going to train on. And then for our feature focus, I decided to use that symptom list and make two new columns. So one column is symptom count. So it's basically counting how many symptoms were in that list. And then regarding the top 10, kind of like key words that were related to those deaf outcomes, I compare those against the full list within a case and the number of times those top ten words were mentioned. I counted those in this column called deaf keyword count. And these are the two things that we're going to focus on. In order to predict whether or not a case ended in depth. So we did a typical model setup, did our training and test split. I did 7030. We did some hyperparameter tuning using bridge search. I used that mainly because it was common and effective and we had a few examples of it in class. And I chose two learning algorithms, Random forest classifier and K nearest neighbor. And the reason why I picked those two is because I wanted to see the difference in accuracy and precision compared to a eager learning system versus a lazy learning system. So when we look at random forest classifier using the the grid search and trying a few different varieties of parameters, it was best picked that using 10 trees and using the criterion of the genie coefficient would probably give the best results. And it ended up in a very high accuracy and very high precision as you can see here in the 90s, 97% ish. So the results actually ended up very well. And then in terms of K nearest neighbor, I was expecting that this wasn't going to have as high as result as random forest, but it actually did almost equally well. There are a few points, sort of maybe like 0.01 or 0.02 lower than the random forest, but otherwise it did very well. We also used the grid to kind of figure out which parameters would be best for this and it decided that neighbors we would do about 15 and then the algorithm would be the K dimensional tree. But just like forest it was high accuracy and high precision. So in summary, even though both produce positive results, I still more confident in random forest results because they're a bit more reliable. One thing I also did was scaling for the data set and I think if I hadn't done that, K nearest neighbor probably wouldn't have as good as results. So I think that depending on what type of scaling or filtering you do for skewed data will decide whether or not K nearest neighbor would be of use at all. So some potential improvements. I focused mainly on columns that could be created from client symptoms or case outcomes, mainly because a lot of the client demographics were skewed. But I do think they could be of use as features. I was just concerned that I didn't have enough time to really add them in. And it would probably reduce accuracy by a lot. Product category probably would be a bit more useful than client demographics based off of the information that I found out in Assignment 2. Another improvement that I think would be very interesting is actually adding additional data sets to use. So for example, we that keyword list that I found contained a lot of like diseases or like types of injuries or like things like a common cold or things like that, which I think there are data sets for. And sometimes you can find like a disease to death ratio on a lot of those different types of keywords and symptoms that I found. So I would have loved to pull those in and try and use those as features. But that probably would have taken way too long. And then another improvement could be trying other models. I didn't use linear regression. I just wanted to focus on one easy learning model and one lazy learning to see like the major difference between the two. But they ended up doing relatively the same. So that's all I have. Excellent presentation. A question about the data set and if you can go to this problem statement slide that you had. Mm, let me go all the way back. Sorry. Here we go. Okay. Okay, so TFSAN Adverse Event Reporting System. So could you tell me more about this data set and where is it used and you know, a little bit more background. Yeah, so it came from US Government website. I can't remember. Yes, I wrote in my notes. One second. So it comes from the United States Food and Drug Administration and they have a division that's called the center for Food Safety and Applied Nutrition. So one thing that they look at is they have other data sets which I've used in the past. But this data set is specifically like if a person was concerned that maybe a person product, whether it's a consumable product or something that they can use on their body, if they're worried that it can potentially be poisonous or causing some type of issue to their body, they can report it to the fda and then they keep a system of it, these records. If I remember, it's over 20 years worth. And in the beginning, like I cleaned a lot of it up. But most of these reports are coming either from the general public. So someone can make their own report and we would just, they just redact things like their, their name and their information, but they keep the client sex in age years or it could be an actual physician. So in the case that like an example, I was focused on death a lot, of course those clients wouldn't be able to enter in their own information. So a doctor or a hospital can report on this on their behalf in case they're worried about potential poisoning as well. That's excellent. So out of my curiosity, I'm asking this follow up question. So suppose if we have this data set and you said this is coming from fda, right? Food and Drug Administration. Yes. Okay, so. And If I start using this data set and we move model it. Now if it's, suppose you build a product out of it and it gets deployed at the hospitals. Now if the doctor and you know, a patient comes in because he's having something and he, he gives similar record, says that like hey, I got this, you know, issue at this particular date that I use this product, maybe I ate something, a breakfast for food, this product, and it started giving me this headache and something. And now, now traditionally doctors might give some recommendation that use this or maybe give some kind of a treatment. But if AI, like, like the modeling you were trying to do, do you think that if, if you, if, if it becomes a big team or something and there is some funding for this kind of projects, do you think that this, this doctors would be able to, at that particular point would be able to say that hey, it looks like this might become dangerous, so why don't you get admitted to the hospital or, or you know, because it, it can predict whether this patient is going to die or not. Right. So do you think this, if it turns into a product, the idea you have, it could start predicting patients that stay in hospital or you know, take more serious treatment? Treatment. So yes. So actually in a prior class to this, I like using this data set a lot because it's very interesting to me. But in a prior project I used this data set and they asked us to connect the two secondary data sets. And I focused more then on the product category and the product name to see if there was a correlation between some of the client symptoms and case outcomes. So and the goal of that project was that physicians or you know, medical professionals could use. I made like a visualization table where you could look up a certain category of products or a certain product. And if it was in this CARES data set, it would list like what types of symptoms are most likely related to that product in terms of it's being reported. And one of the examples I did that I noticed a lot of was that I put Cheerios up here on this page because a lot of general mill cereals were being listed a lot specifically for things like headaches or other smaller symptoms that didn't necessarily lead to death, but was causing a lot of issues. And another data set related to that that the USDA provided was the actual product in the list of its ingredients. So I tried to break it down even more. Maybe it's certain ingredients and certain products that are causing these issues. And what I found out in the example of at least cereal is that a Lot of cereals have a lot of added vitamins and minerals, and those seem to be the correlation between a lot of things like headaches and stuff like that. So my assumption at that time was that there are probably a lot of people on medication or taking vitamins, supplements of their own. And then if you're eating large amounts of these cereals that have like sometimes 80% of your daily value of those vitamins plus the supplement, again, you can actually get vitamin toxicity. So I think this is very much useful for the medical field and I hope I kind of proved that in the prior project and in the Google collab that I submitted. I actually sent a link to that original product if anybody's interested in it. Got it. Thank you so much. Thank you. You're welcome. Okay, so we will go with our next speaker, Bob. Yes. Can everybody hear me okay? Yeah. All right. So I built my machine learning classifier pipeline around the longitudinal, sorry, longitudinal secondary student data. And I'll talk about that. So this data set comes directly from department of Education source, requires a login and they only let you use the publicly available data as download. There's a lot of, of other great studies on that site, but they don't let you download the source data. There's only like statistical decompositions of that data for use. But I went with this one. It represents ninth graders in 2009 were pulled again a couple of times over the course of their high school career and then again in June 2021 as a follow up. And it contains a whole slew of data about the students demographics, their school experience, what even what kind of teachers were teaching their STEM and mathematics class. So a lot of great data. But I chose to drill down on college attendance in that second follow up study in June 2021. So while there are 23,000 rows of individual student data coming in at 800 megabytes, only 13,000 of the students, students, I think it actually might be slightly more that. But 14,000 or so students responded to the final survey. So that's what I had to work with between 2009 and 2021 to actually assess data about how many students attended even a college course, not necessarily a degree following their high school experience. And that data point there, the second to last bullet states that currently about 10 years later, you're looking at about a 40% rate of students between 1824 actually attending college after high school. So in terms of data loading is a lot like my last experiment with data analysis. So the data came in a CSV format with Extra data about encoding and what each numerical representation of the variables represented. The everything was uploaded via linking a Google Drive and the data set was able to fit entirely in the RAM there. And mild sampling was needed because of the way there. For whatever reason, there were students that answered the survey but didn't answer the questions I needed. So I had to call a lot of that data out. The data feature, the data feature I was interested in was have they ever attended college? And deriving a statistic or a feature column for that based on the data there and the responses, the data I chose to use in the classifier are from that set on the right and they're color coded by how they interact with the models, either categorical, categorical and ordinals. So there were numerical features but they did have a clear progression, whereas the categorical definitely didn't like the sex or race don't have a procedural or progression. And then continuous values there that had a scale anywhere from some of Those were like negative 8 to 5 on a scale just because it was a student rating or a student's perceived rating. So they put the scroll or the, the mark on the bar that they felt represented their data. So in terms of data visualization, a lot of these are pretty evenly distributed when the, when the values are continuous. I saw a skew to the left there with the family income. I noticed that a lot of students the perception of their school's behavior issues or sorry, their behavior and issues there were focused around the zero points. A lot of them had positive perceptions of their school's behavior issues. But then at tailsoft to the right, a lot of students had very extremely negative behavior perceptions of their school. So some distributions there to show how these are laid out. Histograms built via matplotlib and then bar data for the more categorical variables. And you'll notice there in the bottom left I had suspension reported was an original feature, one of the data points that I wanted to pull out. I figured there must be some link potentially between behavior issues and, you know, a future academic pursuit. But due to the fact that there were so few counts in the have you had a suspension in your high school career? I ended up going with a different feature that related more particular to behavior issues in the data set. So I don't recall the name of the variable, but I chose to forego that. And you can see there from the S4 ever attend college? The bottom right college attendance bar plot that doesn't read to me as a 40% relationship of students that's much higher. The one is representing students who did have some college courses after high school but that is also because I've had to call non answerers and those who chose either those who didn't answer the survey or those who chose not to answer the question about attending college which is strange to me. I removed a handful of null value from all the features that I was interested in. I ended up doing the dummy the one hot encoding for the race component. I figured that was categorical and could be its own feature due to what I figure would be a fairly important data point. I chose because students have one parent and or sometimes there's one parent with a very high level education driving the income and the student pursuits in the whole household. So I chose to derive a statistic for the highest education of either parent and for the latter two the logistic regression and yes SVM I chose I did select scaled the data but I did not need that for the random forest model that I built. I did do some primary component analysis but only use this as kind of reference point if I wanted to follow up for studies. So I checked out the loadings that were associated with each of these variables and I noticed that a lot of these, particularly like sex student sex male or female which were the only options for the survey does not have any large impact on college attendance following high school. So I thought that was interesting. But certainly the family income and parent education are large increasing factors in choosing whether a student can and does attend college after high school. So kind of intuitive there some of the techniques that I chose. I'll focus on the first point. Since there were so few relatively fewer non college attendees I chose to do a larger split than what I what I saw was a typical 7 or 8020. I chose 70 and 35 fold cross validation. It took quite some time for some of the parameter maps that I built. But you can see the hyper parameters that I built and what you can see on the right there is a plot of the training as it progressed. I believe this was 5, 10, 20 and 40 and 80 and there wasn't much model improvement and increasing accuracy through these. And this was for the random forest model and it's in its training. Okay, so I'll have a slide for each model's performance and then a breakdown slide at the end of all of them together. You'll notice that precision recall and scores for the prediction of non college attendees. Those zero values are relatively low in all three of these models but overall they have relatively similar accuracies and F1 scores. So I thought that was interesting. Even though there is a very distinct difference as you'll see from the next slide in their training times as the SVM was set up, took what, almost ten minutes, whereas the other two took a minute. And logistic regression models proved to be the fastest of all those. While we still bought similar performance in almost all of those models and predicting does a student end up attending college based on their demographic and school factors. So the data supports as the PCA and the data loading showed, there was a strong link in some of those features to college attendance, either negative or positive. With those loadings, we bought similar performance in predicting that outcome with vastly different training times across models. I think it's still relatively limited just due to the limited set of students who didn't attend college in the final set of respondees. I think as a model this could be best used to flag students in the system. Still to say if you have a college dream, your factors show that you are not likely to attend college. So they could shift course or better prepare themselves or vice versa. If the model's predicting they don't go to college and they are predicting or they would like to do that, that's, that's one use for this. I think a lot of the data, you have to retrain this entirely with post Covid and LLM data just because the school landscape has changed so substantially, so continually retraining and providing data, outcome data to the model with newer student data. And one thing that was my huge pet peeve for this whole study was because it was the publicly available data set, they wouldn't provide location or region data for the schools alongside student IDs, I guess for privacy reasons. But I think that would be a very critical feature in determining student outcomes that I don't have available. So thank you for listening. Thank you, Bob. Excellent presentation. So I have a question about if. So your data set is basically trying to predict or you're trying to basically model if students are going to attend college, is that right? Yes. Attending in any sense either just courses, associates, bachelors, whatever, within the eight or so years after graduating high school. Okay, so based on the high school data and their data from parental income or you know, other data supporting data, you're, you're trying to predict their attendance. So your conclusion said that if you had more data features like region or location, that would more help. I'm just curious how, what do you think if your modeling idea, if, if this could be transformed into a product right at universities, will it help to determine whether students will attend online classes. Now, the reason I'm trying to say is, like online classes, how relevant is the location and region in this case? I think more specifically, the location and region data would kind of correlate to how schools perform nationally. So if you could say, by and large, just throwing out a location, schools in the southeast United States have poor ratings. And on average, while I didn't have transcript data, I do have the outcome data to say they did attend college and they had this perception of their school and their own performance as a student in STEM and math courses, science and math courses. So I think that would, in conjunction with those other features, it would provide another valid and critical data point for that prediction. And a college, honestly, I see this as a product, either a college counseling program or advertising program could screen student data, say their application data, and apply these metrics to say, is this person really likely to go here or does the model fit them? Something along those lines. All right, excellent. Thank you so much. Okay, so next presenter we have Anton. Great, thanks. Let me just present my screen here. All right. All right, so good evening, everybody. My name is Anton. Hopefully you can see my screen. All right, so I'm going to be continuing the. The research that I did for the last assignment related to COPD risk. This is, this time I'm doing it actually nationally instead of, instead of regionally or instead of focusing on, for example, the Hampton Roads area. So, yeah, we'll just walk through the. The process that I did. I developed a. A machine learning pipeline in SPARK to predict COPD risk just using public health survey data. It's actually the same data I used last time. And so, yeah, I'll walk you through that process. All right, so I basically had a single problem statement and a single objective. I want to be able to meet two conditions, a technical score in getting the model. And then there's a practical idea here of having a reasonable precision and recall. So for the technical requirement, I basically established that the area under the under ROC curve would be 0.75 or higher. This, obviously, it measures how well a model can distinguish between two classes. You know, for example, spam versus not spam we've seen or fraud versus not fraud. In this case, it's COPD versus not COPD score of 0.5 here would be basically random guessing. 1.0 would be perfect. So just finding a happy medium there, 0.75 is what we're looking at. Secondly, the kind of reasonable precision recall is a really crucial check for making sure that the Model's predictions are actually helpful in the real world. And so this would be kind of like a sanity check. A good area under the curve score is great, but it's not enough to make the, to make sure that the model doesn't have certain errors. We really want to check precision. We want to, that is, we want to check for false alarms. When it predicts yes, how often is it actually? Right. And then we, we want to look at recall, for example, and say of the yes cases, how many did we find? And of course, like I said, the idea is to make sure that the model is useful. All right, so as I mentioned at the top, the data comes from the same CDC data that I used for the first project, our first presentation that I gave. There's about 366,000 records. After doing some cleaning, pretty minor cleaning actually. It's a pretty good data set. This is publicly available at the CDC. It's a 2023 survey. It's a surveillance study. So this is survey data. And for the initial model that I'm working on here, I focused on five key fields. Obviously the COPD target, that's you know, do you have COPD or not? And then the other, the, the kind of age, sex, race. And I added smoking status, which is different from what I did last time. Last time I looked at more particulate matter in the air, the environmental factors. So here we're taking a different approach and instead of involving the climate or involving particulate matter or the air, we're looking at smoking. This large scale data set is really, really provides a robust foundation for building a predictive model. All right, so a little bit about the Spark environment here. I did end up going with Databricks, the community edition. I've got the various details here on the slide of what all I actually used here were. Obviously the entire pipeline was built in databricks and it uses pyspark. This is, this kind of snippet shows the initial setup code. Very standard. I just started Spark session. I load the data from a CSV file. It was actually the getting that data. Last time I had talked about getting that data into CSV was one of the harder parts because it was this weird format formatted fixed width file. But the fact that I already had the data there was great. Loaded it and then selected the columns that I needed that I, that I talked about in the last slide. And then there's a line here where I filter the data to only include valid yes or no responses for the COPD question, ensuring that we obviously have Clean data for the target variable. So before doing the modeling I wanted to do some initial analysis and clean. As the kind of table shows, this is a fairly light process. Did a null check, dropped a few rows that were, that were like I said, null. I kept the age filter to, to be between 18 and in 95. But really what I ended up doing was there's an 80 plus field and I dropped that and I dropped the. The don't know. So, so it's really 18 to 79 is what we're looking at. And then I wanted to look at the class balance here. There is a severe class imbalance. Obviously most people don't have COPD. So this makes sense. Fewer than 8, 8% of the respondents reported having COPD. So this is actually a fairly critical challenge and it will, as you will see it heavily influenced the results. So on the right we can see distributions for various predictor variables. The age group distribution, sex, the kind of female versus male distribution, smoking status. We can see there's essentially four bins here where we have different types of smokers and then the race distribution. So these, these distributions really just give us a baseline for understanding the, the population population prior to actually starting the feature engineering. In terms of feature engineering, we're again, we're doing categorical encoding. We, we have sex, race, smoker. These are categorical variables. So we converted them into numerical format using the string indexer and one hot encoder. And I treated the age variable as like an ordinal such that I could bin it. So that was kind of convenient. And then finally the vector assembler was able to combine all the features into a single vector which was then scaled. And so this is our scaled vector approach. The entire pipeline was applied consistently across the several models that I used. So that kind of just ensures a fair comparison between each, each, each model. All right, so here we've got the three different models that I ended up comparing. Logistic regression, decision tree and then random forest. I ended up actually doing a couple different versions of decision tree, one with class weighting and one without. So it ends up being four different models. I guess if you look at it that way, I use the five fold cross validation here in the, in the regression kind of as might be expected. And then for the random forest, this is our more kind of powerful ensemble model. I've got some tuning here that seem to be, seem to provide the best results at least for this phase of analysis. All right, so in terms of results, these results I think are very telling. The ROC curve on the right shows that the model achieved this area under the curve of 0.78, so that successfully meets our goal of 0.75 or greater. However, we also see a major problem. While both the logistic regression and random forest have good AUC scores, the recall is nearly zero. And so, at least at the default, like, you know, 0.5 threshold. So this is the class imbalance that I mentioned before. The models learn to be safe by kind of predicting. No, for almost everybody. That. That makes sense. This makes the models technically accurate, but by the AUC metric, but of course, kind of useless in practice. So the decision tree struggled across the board, even. Even with class weighting. So these are interesting things to call out about. Perhaps there are other things that we need to consider when looking at COPD nationally. I'm doing this in a way to motivate the fact that there's certainly going to be regional differences. There's going to be some interesting standard determinants of health demographics that will come into play as I move forward outside of this class with more research. Research. Okay. So either way, though, to kind of solve the low recall problem, I did do some tuning on the decision thresholds of the regression model. So by lowering the threshold, obviously, we make the model more predictive or more sensitive to predicting. Yes. And then the results were fairly dramatic. Here we can see in the tables that lowering the threshold to 0.15 gives the best F1 score. That actually balances precision and recall. And then this boosted recall, you know, from nearly 0% all the way up to 49. So now we're actually looking at statistically significant findings here. The. The cool thing here, even though this is like, again, there's a lot more work to be done, at least we have a clinical framing now by doing this. And so we can say that, you know, for a threshold of, for example, 0.10, like, if we're doing broad screening, we could catch potential cases of copd. And so that's pretty neat. That's like, if the goal is screening, then we can actually catch that. But if the goal is, like, referral efficiency, for example, perhaps we could accept a lower recall. So either way, the takeaway is kind of choosing that threshold of around 0.15 actually quadruples the recall while keeping the precision comparable to the. To the baseline. All right, so key findings. Again, like I said, the data is a fairly large set of data. It's unfortunately highly imbalanced. Our best performer here overall was logistic regression with hitting the target of the AUC over 0.7. The threshold tuning ended up being our most critical step. We were able to transform the model from almost unusable to highly effective in, in like a certain clinical setting or a clinical case. And so we achieved our goal with a simple model. And we also have a kind of a flexible tool that we could offer to stakeholders that they could adjust based on their specific needs. So that's kind of fun. We can, we can actually maybe even improve it, make it more, more useful for stakeholders over time. All right. And so some limitations I've called out. The model is the starting point. It's limitations are of course the fact that it's survey data. There's a lot of bias in the data. The, the severe class imbalance that I mentioned. It also lacks the, like I said, the environment variable, environmental variables. Those are going to be really important. The fact that I wanted to start with the sensor data last time but not have it here kind of points to what does it look like when you join that together. And then of course bringing in more things like standard determinants of health, regionality, things like that I think will help us go a lot further. And then I'd like to like with next steps as these, this stuff gets more and more complicated. Start bringing in some more explanatability to the machine learning to these models. And so maybe looking at shapely values or something like that. I think this will be very helpful for the stakeholders that are within a healthcare setting. They're interested in not just what it's saying, but why it's saying it and that kind of thing. And then some validation would be really good if we could get, have an ability to compare it to like hospitalization records, that kind of thing. So here are the references and that's pretty much it. If there's any questions. Excellent presentation. So yeah, I have one or two comments and also two questions. So if you could go to the slide where you do the clinical framing. See here. Yes, the Clint. Yeah, that's a. I like that slide a lot actually. For sure. That's probably my favorite slide to work on. Yeah, yeah, that's excellent slide. And I wanted to the class to see this again. See the way he has put like, you know, the thresholds out there and then he's trying to say how those results, what does those results, if you change the threshold, what does it mean from a business point, the business here because he's working on the healthcare data and it's, you know, clinical data, what does that mean? And that's very Important. So when you're explaining your results, we, we go into this machine learning metrics. But what those machine learning metrics or what, what those tunable parameters mean to the business is, is extremely important. We have to translate that. And you know, whenever you're doing this analysis or modeling and you're presenting it to someone, maybe you're doing a startup pitch, it extremely important to finally, when you're going to your conclusions, you know, keep that balance between the technical and the, and the value. Right. What are we getting out of it? So excellent slide. Yeah. And if you could also go to the next slide, please. Yeah, you bet. Yeah. So this is also a great slide. You know, where he's, he's trying to again put all the key finding. I had a couple of students do that. Right. Summarize the whole, whole pipeline again, very, very briefly. Again it doesn't have to be because you already did it in the last 10 minutes. So it has to be very short. So it's an excellent slide again. So really appreciate it. Yeah, you bet. This will. Yeah. Great things. Yeah. And one question I had is about the class imbalance. Yes. What do you think you could do for class imbalance here? Yeah, it's, it's, that's a really good question. I thought about that. The. For me, I think the answer is obviously more data, but not just more data in terms of records, but like variety of data like where the data is coming from. I've fortunately had been able to have, get access to a really large data set set that's a national data set called all of Us Research. Unfortunately it's not publicly available, but it's still more data. And so this I think would help to, to get rid of some of that class imbalance. There's also other, other ways of approaching the problem that I think would help in terms of, you know, like we've talked about the environmental stuff, the regionality, I think getting different types of data set. Like to me it would be really interesting to be able to see, you know, like the last time I talked about the sensor data, like we're looking at zip codes. Right. And so if we can actually take the zip code aspect, like the FDA has a whole zip code crosswalk that they do where you could translate for certain county level areas, the various like sensors that are there and then, and then try to crosswalk that with some of this data, I think that might help. I don't know, it's tough because you know, COPD isn't super prevalent. So maybe we need to Take a level, a step up and say, okay, well, not, let's not just look at copd, let's look at other, other respiratory illnesses and make it broader than copd. Look at, you know, allergenic rhinitis or, you know, chronic asthma, things like that. I think that would help a lot is because then that means more respondents in these surveys would be able to answer yes to the questions. Also, again, smoke. Like looking at smoking I think is really helpful because there's a strong correlation between smoking and these respiratory illnesses, which can then be, you can do like, you know, you can look at other studies that have already been produced and then kind of validate your findings against that one. Yeah, excellent. So just one more question out of my curiosity. What, what if you had to translate this into a business product? Who, who would, who would buy it? And why should they buy it? Why should buy. They buy it? Yeah, I can answer that. I. So the company that I work for, it's a company called City Block Health and they're very interested in understanding risk modeling when it comes to their patients. There are, our patients are all Medicaid patients. And so these are, this is a very interesting group of patients in terms of a lot of the challenges they're facing. There are, you know, a lot of income inequality, a lot of unstable housing, a lot of substance abuse, things like that. But the idea is that by having a company like CityBlock, which is a for profit company, come in and take those patients under their wing, essentially they're able to do some type of risk modeling to say, okay, well this patient, this pool of patients is most likely to be readmitted to the hospital, for example. And so what we want to do is we want to intervene with the patients with, with kind of like proactively, with some kind of mobile care or something so that they don't just go to the And rack up Cost. So any, any opportunity, a company like that, the company that I work for, or many of others of these kind of value based models, any opportunity they have to be able to more accurately predict what's going to happen to their patients so that they can provide services to them before the patients get sick is going to be really interesting and is going to be something they want to look into. Excellent. So this is something also the state or the city could buy and they could give it to certain, outsource it to a couple of companies so that, you know, the number of patients in hospitals can be reduced at a time. Exactly. Yeah. Yeah, thank you so much. Absolutely. Professor, I. You know, you asked two of my questions, so that have been answered. But my. I'm still curious though. Now, in certain kind of data, we do not have any. So we do not have authority to changes, right? With the class imbalances, for instance, like with this kind of data. Now what do we do in that cases? So if I understand your question, you're asking that we don't have the authority to change the data. Yes. In terms of class imbalances. So. So there is a problem with class imbalance. I mean in general, like not just to this data, right? If it's a open, very open data. Also the class imbalance is a general problem. And you know, I was talking to the previous student also about this. So there are a couple of solutions. One is you. You try to do oversampling, which basically means that improve your. The. The minority class data, right? You just duplicate it. But it generally it's not a best solution. But these are some solutions I'm talking about. The other thing is under sampling where you bring your majority class to minor to the same level of your minority class. So you. You remove the records of your majority class, right? Again, this is not a great solution as well. Another solution is using synthetic data sets where you, you see, okay, you have one class, the minority class. You want to do lot of synthetic data to bring it to the same level as majority class. There are multiple methods. One method is S smote. Other methods could be using GANS or now LLMs as well. Can you generate similar data, right? You could also try, try to do this. I mean you. You have these GPT models give certain use cases, right? Hey, can you generate me synthetic data for. For this user case with this condition, this conditional distribution, can you generate the data and maybe it will be able to generate. Because a lot of synthetic data is getting generated to train LLMs itself, right? So we are at a place where to generate the huge amount of models. You need more data to do that. You are using a lot of synthetic data. So synthetic data is one of the popular ways to fix this problem. And the old school method is go back and try to collaborate or collect more data, right? So where are this data if it's not? This data is only based on us. Maybe. Is it enough? If it's not enough, then we have to open up to other countries, take data from other countries. Maybe multiple hospitals have to collaborate, right? Share their data. The biggest problem with the medical industry is like they don't share data, right? So if hospitals started collaborating and sharing data Then lot of these models will get enabled. But the problem they don't do is because of lot of HIPAA rules, privacy rules. So if hospitals start sharing data, we get a lot of data. But that's one of the machine learning research area is how can organizations like hospitals can share data but preserve privacy. This also applied to banks. If banks have limited data and they want to share their data, but they don't want to give a customer information. But how can still you collaborate with data, do not give away the privacy information and still create very good machine learning models. So these are some general solutions to these kind of problems. Right. Okay, thank you. Okay, so we will go with the next speakers here, Jim and Vera. Yes. Can you see my screen? Okay. Yes. All right, great. All right. My name is Vera Gama and I partner with Jim Start event on building a predictive machine learning pipeline. All right. So our project, project overview, what we did was we use the same data set from our Assignment 2 we analyzed data set from credit card clients in Taiwan. I think the company was called UCI. This data set contains 30,000 records and about 25 different features. And these features include demographics such as gender, education, marital status and age. The features also included credit limit as well as bill statements and payment amount. So our goal for this project or this assignment was to predict if a client will default on their payment the next month and explore whether, you know, age was a factor, whether younger or older clients default more often. So our data loading, we use the Google Collab again. The data was downloaded using Kegel Hub, loaded into memory using Pandas and no sampling was required. The data set also fit in memory, again without sampling. So our initial data analysis, the data is tabular and multivariate. No null values were found. We also detected a class imbalance with only approximately 22% default. Some categorical fields had undefined categories such as education. So with that, we did some cleaning on the data. So the categorical variables that I stated, education and marriage, contain undefined values. So we grouped those into another category to ensure consistency before we started our modeling. So now when we come to the visualization, we use the histograms and box plots to examine, you know, some of the distributions, outliers and relationships. And what we found was that we found high SKUs and payment amounts and multi, I have a hard time saying this word but multicollinearity billing fields, what multicolinearity is usually occurs when you have two or more independent variables in a regression model which are highly correlated with each other. So in Turn. This makes it difficult to determine the individual effect of each variable, say on a dependent variable in this case. So on the left hand side you would see the visualization of client default. The client default distribution of the number of clients with the limit balance, sorry, the credit limits ranging from 10,000 to about a million. But we didn't show all of that because most of it kind of concentrated between the 0 to 200,000 credit limit. So this showed that it was heavily skewed towards the lower. As I stated, most clients had limits under 200,000. We did have a few extreme outliers with very high limits. So this indicates maybe a need for scaling or log transformation. And then the next one we looked at age distribution. We looked at the range from 21 to 79 years old. We realized that most clients are in their 20s to early 40s with a strong peak around age 30. The age distribution also shows right skewed. So we had fewer older clients. No obvious outliers here. But again we wanted to explore whether younger older clients default more often. And then so same thing with the credit limit versus number of clients that you see on the right hand side. Top right hand side. Then we also looked at the box plot. We use box plot for credit limit by default. And here we showed clients who defaulted showing as one that they tend to have lower credit limits on average. And then the median credit limit is noticeably higher for non defaulters which are represented here at zero. So we realize that there are many outliers in both groups, some with credit limits over 500,000. So we believe this suggests a possible nonlinear relationship between credit limit and default. So we did the same box plot for age by default and median. Age by default. Median. Yeah. Age by default and median age shows that there's slightly lower defaulters, but not by much. Again, both groups have similar spread and outliers. So clients of all ages ages do default. It's not necessarily based on on younger or older. So we concluded that age may not be a strong predictor of default alone, but it could contribute when combined with other factors. So now I'll turn it over to Jim for the feature engineering. Thank you very much, Vera. Can everyone hear me okay? Yes. Okay, feature engineering. We used one hot encoding to transform some of the numeric categorical columns into binary, such as the level of the education and sex of the client. In addition, we also performed some normalization using a standard scalar to transform some of the numerical columns into something that would minimize. Next slide. All right, model training. We used a 5 fold 8020 split. We compared a logistic regression to a decision tree. In the logistic regression, we achieved an average F1 score of 0.48 across the five folds. The test set showed a precision of 0.37 in the decision tree. We scored a little bit less on the F1 score of 0.4 with a comparable precision of 0.38. The threshold optimization. The tuned classification threshold to improve F1 should have been about 0.55. Next, in conclusion, the logistical regression performed best overall with a higher F1 score of 0.47 on the default class. The decision tree tree achieved a higher accuracy 72% but struggled with recall for the defaulters. The data set's class imbalance required thoughtful evaluation using the F1 score of recall. Not just accuracy and feature scaling and one hot encoding were essential steps in preparing the data retrospectively. Smoke probably would have worked for preprocessor on this pretty well. That's about it. Excellent presentation. So the question I have here is regarding the feature scaling in your retrospective, why do you think was very essential here? Well, we had one or two outliers that went upwards of a million dollars on the available credit limit and by far the majority of them were well down 50, 60,000. Still well above my credit limit, but hey, so that would have skewed the data set drastically. And in all honesty, I wanted to see what it would do. Got it. So another question regarding your data set. Is it. Is it something a real world data set? I mean, can you give me a little bit more background about the data set? Where is it coming from and is it publicly available? Okay, yeah. I came from Cable Hub. I believe it's real. Came from Taiwan. Data set came from Taiwan from a company by the name of UCI. So it's publicly available and that's what we use for assignment too. So we just kind of built on it for this. Thank you. Thank you. You're welcome. We'll go with next speaker Anthony. Can you see my screen? Yes. Okay. Good evening everyone. My name is Anthony Lamini and in this presentation I'll walk you through a supervised classification pipeline using Lending Club loan data. The goal was to build a model to predict whether a loan will be fully paid or defaulted using machine learning. Okay. This is the outline of my presentation. Here is an overview of the steps I followed. Starting with the aim of the project, I explored the data set, clean and transformed the data, built four classification models, evaluated their performance, and finally drew conclusion based on the results. So the presentation will cover this for the aim for The AIM I use classification algorithm to predict loan defaults. The second one is compare performance across models, evaluate which futures contributes most to risk and identify limitations and areas for improvements. So there is the overview of my data and here is the it starts with the data ship which is 381,131 data sets and 11 variables or column names which is loan amount, term, interest rate, employment length and adex. Also when you come down here I have the loan status and for the loan status I created a binary variable to indicate fully paid when it's 0 and 1 to indicate defaulted. And to also note my data set comes from cargo. I did a distributional analysis visualization to see whether there's class imbalance and from my visualization I realized that for the fully paid almost 8,000 of the data sets were fully paid whilst about 2,000 was not fully paid or defaulted. And this creates a class imbalance issue which I found it difficult to deal with. For this site I wanted to know the relationship between the interest rates and loan status. So I did some bus plots and I realized that for loans which are fully paid, the average interest rate is lower compared to loans which are defaulted and which really makes sense under normal conditions. I also did some visualizations in here and for this side I have the loan amount and this is the binary 0 to indicate fully paid and there's a yellow to indicate defaulted. If you check this distribution you realize that for the loans which were fully paid they had high loan amount compared to loans which were not fully paid. And there's other distributional graphs in here. Let me talk a bit too about the interest rate. If you check the scattering of the point you realize that loans which were defaulted, which is the yellow points, had high interest rates compared to the loans which were fully paid which is at the bottom here. For the data cleaning I cleaned the data by transforming percentage strings into numeric formats, extracting the gate from test based field like employment length. For the employment length it was in months and number, so I had to extract the digit from the test based and remove rows with missing values in key features and these processing steps were crucial to ensure that the models performed optimally. Future engineering and training in these steps, I prepared the data set for modeling by transforming both categorical and numerical features to a format that machine learning algorithm can effectively use. First, for the categorical features like home ownership and purpose, I applied one hot encoding and this technique creates binary column for each category allowing the model to interpret them without assigning arbitrary values to them. And also the next thing I did was for the numerical features I applied standard scalar. And the reason why I applied standard scalar was I actually didn't know what to do for the numerical features. So I just said on the Internet and they were like if I'm to use KNN and SVM then that will really help to produce best models for that. After these steps I had a fully cleaned and engineered data set which I then use to train the four classification models which are the logistics regression, random forest, SVM and knn. So this was my model output for the logistic regression. For loans that were fully paid it was a bit higher which is 81% precision and a record of 91%, 97%, but it didn't really do well on loans that defaulted which was 53 and 12%. For the random forest it was 81 and 97, which is more or less like the same as the logistics regression. And in terms of loan that defaulted it didn't also really perform well in predicting that. For let me move my screen here. For the XVM in terms of precision for loans that were fully paid it did work but then not as compared to logistic regression. And for record it had a good record for SVM compared to logistic regression. But in terms of defaulting it also had 59% precision but then in record had only 5% recall percentage. So that was not really war K and then did WAR in terms of precision and recall for both defaults and repeat. But for the defaults it's didn't do as compared to logistic regression. Overall using the ROC to compare, I think the KNN did better. But then I went in for logistic regression because it really did better when it comes to the defaults as well compared to knn. I did model comparison here and for my model comparison, if you check here you realize that KNN overall ROC was higher compared to the other models followed by logistic regression and random forest. Then I also prepared the confusion metrics. So for the confusion metrics here I use the random forest and the top left cell represent the true negatives which is loans, this side which represent loans that were actually fully paid and correctly predicted. As such, the bottom right represent true positive which is loans that were charged off and correctly predicted. Predicted which was 43. And the top right cell is the false positive which is this side. This represent loans that were actually fully paid but incorrectly predicted as charged off or defaulted. And the last side here is bottom left cell shows false negative and this represents loans that were charged off but predicted as fully paid and this is really bad. This side is really bad for a financial institution to make such prediction and the value is too huge. So the precision level is very bad. When it comes to the ROC curve, this I use the random forest and from my search I realized that to see whether it has good prediction it means that you check the area under the curve. And if the area under the curve is larger then it means it's good. But then if it is smaller it's bad. But then if you check this, it's not really large and it's not really small. So I'll say it's moderately good for the prediction. For the strength of the choice of model, I think KNN performed best overall due to effective pre processing and scaling. Random forest provided strong performance and handles non linearity and features interaction as well. All models benefited from clean and world engineered data sets. For my limitation, I think KNN is sensitive to irrelevant features and SVM underperformed due to data set size and sensitivity to kernel parameters. So looking forward, I think use resampling technique like smooth as professor said to address class imbalance more robust. For the research I researched on Smooth, I realized that if you use the smooth, it's more or less like you are assigning more sample size to the minority class and you are not allowing the algorithm to replicate the smaller sample size or the same sample size for the minority class. Thank you. Excellent presentation. Anthony. I wanted to ask a question about you know, your minority class prediction to improve that as you said, in a future work you want to do resampling technique or use a smote or maybe other technique. You know what. What other ways in the pipeline do you think or maybe that you think could help the minority. The minority, sorry, the defaulter prediction better. What other changes do you think could. Could help the model a little bit more Apart from the minority sampling, but changing. I was thinking if I can increase the size of the. Of the minority class it will really help in the prediction because it seems that much the algorithm is used to knowing the same minority values compared to the majority class size. So you think that the. The minority is the biggest problem. But apart from the minority, apart from the addressing the class imbalance, is there something more you could do to do to the pipeline? Or do you think that everything was fine, everything was okay. But then I think for the model the predict predictions, the overall predictions were not really good. Okay. The value. So I was thinking maybe a more variety or taking into consideration other. So yeah, so. So I was asking basically like do you think anything that could be improved in feature engineering or you know because this is, this is something a very realistic application that banks would like to use and I would recommend that and this is something looks. I don't know whether it's toy data set or real world data set but something banks would be really interested to work with. Right. And maybe they are already definitely they would be already using it. So I would recommend that also try to search a little bit about use cases like how these banks especially like bank of America, JP Morgan Chase, you know also who else Capital One. So a lot of these banks look go to their website, look at their white papers. Right. This is another way to do this is what are they doing. Right. We can keep doing a lot of things here but the sometimes it's much also easier to do research and see is this what. What is the what has been done in the literature. Right and one, one area is you can look at research papers but other area is like you can look at practical applications where how these bank and a lot of these banks publish white papers. So I recommend that you go and search and see what they are doing and then see some of their methods. Could you you know reference refer in your modeling. Right. But excellent presentation and thank you. Thank you. Okay, Brian and Jasmine before the presentation class we are at 9:30 so feel free to drop. This is anyways recorded so you can see it later. I hope the reporting is. It's still going on. Yeah so it should be there. So we'll, we'll continue with the two more presenters if you want to stay, please stay. Otherwise your feel feel free to drop. Yeah. Ryan and Justin, you're on mute. Looks like. Can you hear me? Yeah, yeah. Okay, can you see my screen as well? Yes. Okay, good. Alrighty. Jasmine, are you there? Okay. Yes. Hi. All right. My name is Jasmine and this is our assignment 3. Building a supervised classification model. Okay, so for our problem statement we just wanted to explore the trends and relationships of different variables. Primarily popularity among a Spotify track data set. And so for loading the data we use Pandas and Pyspark. The full data set did fit into the memory so we did no sampling. Our data set consisted of about 587,000 values and it included data types such as strings, floats, integers and Booleans. This is a list of some of the variables in the data set such as like the names of the tracks, the popularity on a scale of 0 to 100 which was our target variable. The duration of the tracks and milliseconds whether a track was accepted, explicit or not, the artist listed in a track, and release dates, etc. Okay, so this is just some analysis of the data. This correlation matrix showed that most of the correlations are weak, indicating a lack of a strong linear relationship amongst the variables. The strongest correlations that could be found were between loudness and energy, energy and acousticness. And so that is what we see here. Okay, so our data was a tabular CSV file. And so initially we explored some of the summary statistics of the variables to understand some of the feature distributions and variability. And that's just showing that. Next we checked for null values in the data set. This is our code for that. Luckily, with our data set there were no null values or missing data points. And so that's showing that. Okay, and then we also check for class imbalances. As you can see in this bar graph of the popularity, there is a major class imbalance. There's a very large Spike at the zero popularity mark, exceeding 40,000 units. There's also a long tail distribution. So essentially as the popularity increase, the frequency sharply drops, forming that long tail. You do also see the moderate peak between like 20 to 40, suggesting that like, the popular items are centered in that range. Okay, so for training we did a 70, 30 split. 70% going to the training, 30% going to testing. Let's see. And so that's essentially that. And then, so then we also checked for outliers. We used the IQR method to filter out the popularity outliers. We applied a box plot on some of the numerical features to visually identify outliers before scaling. So looking at this graph, it displays the distribution and outlier behavior of several of the features, including popularity, et cetera. Most of the values are concentrated in a tight range, indicating some consistency. However, the duration one in particular, so some extreme outliers stretching far beyond like almost 5 million milliseconds, like over 83 minutes. You can also. We can't really tell. But the popularity is also heavily skewed towards the lower end, which was seen with the imbalance one. Let's see. And that's all right. So for the feature engineering, we did the vector assembly assembler, excluding popularity, and we also applied the standard scalar to normalize all the numeric features and eliminate that scale disparity that we saw for our pca. We really used it to just analyze that dimensionality there. We created also about a bar, bar graph, A. Sorry to sort of just pick that as well. But for the most part this summary really shows that There was a lot of variance components for each of those. So the red line really the individual and the blue the cumulative with that black dash marking the 95% variance threshold. So this really helped us to optimize to determine the optimal number of components there, just because we also were able to then go ahead and do a variance crossing of around seven components. So we reduced the future spaces to seven dimensions. So that way it could be binned better when we actually finished up the performance of our models. That being said, once we got into our hardware parameter tuning. All right, so once we got to our hyperparameter tuning, we tried to sort of make it so that way we could filter out the most of those variables and try and get a higher rate. One of the things that we had to do was bin it into three classes. So for low, medium and high, just so that way it can go along with the just the class, the actual popularity and a better range. This made it a lot easier for it to plug and play with the values as well. When it was a lot more bins that we had it in, it was really difficult for Google Collab to actually do the analysis. It took a really long time to get any, any real definitive answers out of it. So we ended up taking it down, binning it into three categories instead to get a better evaluation of sort of how the data was going to work. So we took from seven and reduced it to three. Then at that point, well, sorry, so at that point, then we then the random, randomized search CV and we tried to make it so that way we had did it twice. So that's why there's two up here. So the first one at the top is the one that we did first and then the second one is when we bend it into the third. As you can see, even after we bend it into the third, the three categories there still was actually a less it. It sort of decreased it. So that was something that was really interesting once we actually finished it out that the percentage actually went down for the best score. And so it still was after that point. That's where I did the second one. And I actually decided to do an understand sampling of the popularity to mitigate that imbalance with the zeros because there were so many zero values that it didn't really give fairness to the other values and how it was, how it was modeled. So ended up sort of taking out a lot of those zero values that were in there to sort of flush it out. And after that point, that's where the best score sort of Decreased. Right. So that's where we went ahead and we started the next process, which was the. The random forest model, just to try and get those features in there and finish up the model as well. So the random forest really showed us some big classifications there and how it could summarize the prediction as well. So what we see is the top futures is that loudness and acousticness were the most influential. So it really suggests that the high volume and acoustic qualities play the major role in determining popularity. Next would be for duration and explicitness also had a strong influence. So it does hint that the track and the lyrical content can change the listener's engagement on the lower side with tempo, time, he and mode, they all had the least impact on the songs. But popularity, just because it could indicate that there's less. And it helps us to predict that there's less popularity due to these particular factors there. This really is. It's really just trying to help us out with understanding the main attributes that are most predictive when it comes down to what the song will be, what songs will specifically be cast as popular in the end. All right, our final results after we did the randomized. The random tree model, which was our final model that we decided to go to, really had a hub. Like I said, it was a low accuracy at the end, really just because of the fact that we can see specifically believe 51.93% that threshold there. It does sort of talk to the fact that it's difficult to really get an accurate reading. But we did consistent cross validation for our results and we had a baseline of what we were looking for to make it a little bit more complex and predict that popularity, the low and high classes were much easier to predict compared to the medium classes, which have that lower precision there. And this is really due to that overlap and which we saw when we looked at the density of the popularity. So there was just a lot of less. A lot less distinct features that would really create a pattern there. The confusion matrix. You can see that many of the medium tracks were also misclassified as high or low. So it does suggest that median popularity is a lot harder to define or predict based on the available features there. But this also just reinforces the fact that there's a lot of variability in the data. So in conclusion, it was slightly better than random guessing among the three classes, but they're still not very strong. And in the case of the model struggles to generalize for future improvement, just a little bit better with the bidding strategies. We did struggle with that portion when we were doing it. It was a lot of back and forth and that was probably the largest timeframe because the data was so big that it couldn't really been multiple bins. So we had to sort of break it down a lot more. And if there was a little bit additional metadata that was added into it, it would have made it more complex and a little bit easier to strategize. The biggest thing was that of course music is very subjective to the person, but if there were things like artist fame or release timing that could really change, change the prediction specifically, because maybe one song does really well, but you know who exactly wrote the song, are they already famous? Are they already known? And that could attribute to how popular that song is based on if it's a maybe like a breaking artist or somebody that nobody really knows. Another thing is just sort of like the pipeline itself that we did, there was a lot of back and forth, had to go sort of and fix the workflow of the data. So that way it covered everything. When you're going from top to bottom in the lines, I'm sorry, in the workflow. But the pipeline itself shows that the. It is meaningful patterns that you can see. Even if it remains tough, a tough metric to perfect, but there's definitely some key values in there that could be sort of cited, like the loudness, the energy. And then what really didn't really help out, which was some of those other factors that we've mentioned before, such as tempo, key and mode. So it was, it was close. But of course music is very opinion based. And we got our data from Kaggle and there are our sources. Excellent presentation. So the question I had was about your data set. You said it's a very, very large data set. So how much large is like, are we talking about 1 million records or 10? How, how large the data was over. Was it over 400,000 data points? Jasmine, could you help me out with that? Yeah. If we go back to the beginning slide. Did I write it down? It was like 586,672 data points. I believe that are half a million. Yeah, half a million. Okay. Yeah. Okay. So that's a lot of data. So when you will. So when you were loading this data, data sets fit into the half a million records. So which infrastructure did you use? Is it like Google Collab or Databricks? We used Google Collab for it. Google Collab. And did you run into any issues with the speed or something like that or. Everything was working fine. Biggest issue with the speed was really finishing up the random forest just because we had to figure out the bidding process when it was in more classes. When we initially started, it was taking a really long time to finish. So that was probably the biggest roadblock there. Trying to figure out what would be the best way to categorize all the. All the data because it was so big that it took. I stopped it at about 30 minutes because it was just taking forever, I should say. Yeah. Yeah. Thank you. Great presentation again. Yeah, thank you. Okay, Tabisim, your. Your. Yeah, I think your presentation is the last one here. So. Yeah. Again folks who, who want to drop. We are past 9:30. So you can drop but you want to stay. Please stay. It is hard to be the last, especially after all these excellent presentations. Right. But I will try. But I'm very looking forward to almost. We had great presentations and I see that it's a great topic here as well. So please go there. Sure. So my partners unfortunately had to drop out from presenting tonight with me because of some unfortunate unforeseen situation. So I'll be presenting the. The last half of it and I'll be fast. So I believe everyone is looking forward to go to bed. Yeah. So this study. Why it's not coming. Okay. So this study aims to build random forest class certification model to predict students exam outcome which is pass or fail based on the study hours, extracurricular participation on campus, attendance rates, previous academic performances and parents education level. So we did some robust or comprehensive visualization to explain explore our data what was missing and what was not there. So we did histogram box plot, violin plots for detecting for normal distribution and detecting outliers. For correlation analysis we did, we did heat maps and target correlations. For relationship explorations we did scatter plots and pair plots and Also we did 3D scatter plot and PCA projections. So here you see some of the plots that I talked earlier about it. So the all correlations are very weak as you can see here and as you can see here as well. There are no patterns, there are no lines. And so we can say that it's non linear data. And also actually I should say that we are using the data from our previous assignment because. Because it was a perfect data for this kind of model and for future engineering and transformations there. For binary coding we did yes, which was yes and no. We did 1 and 0 for you know, ordinal coding coding which was parents education that started from high school associate degree and then went up. So we coded that from 1 to 5. For scaling we did standard scalar for PCA analysis and also to normalize the numeric features. And there were minimal transformations needed due to data quality. And we used and we and we did some preprop pre processing appropriate for the algorithm that we chose. And so as you see our PCA projection it doesn't do a great job. There are no patterns, there is no classifications, so it's not so great. And for the model training and cross validation so we split the data between 80 and 20. That maintained class balance in both sets. For the algorithm we choose because it handled or it handles mixed data types and also robust outliers and noisy data like the data that we had. And for for the H we also used hyper as you see it a hyper parameter. Toning our parameters was parameters were N estimators, tree numbers, max depth, the complexity of the of the trees. So our best score was at 0.50 or 50%. And as you see here here in the data our model, our model was not great when the max depth was equal to five. It didn't do great job. It went up eventually though and it and it did good when the max depth was equal to 10 and also an estimator or our tree was 100. So for the model performance which I will have some visualization in the next slide, our accuracy was 0.51 or 51% which is not great. Precision recall are equally poor and also F1 scores also 50 or 0.50 or 50% which is also poor. And that doesn't really, you know, handle the either classes. So for we do have, you know, future importance chart just for model interpretability because our data is super not interpretable and it's real world data and that was one of the challenges. So now when it comes to performing our models, the fact or the problem was not with the model per se, but in fact it was with the data quality issue and also its integrity issue. Our confusion matrix, the accuracy as I said it was for 51% and as you see it's almost equal. The true and the false are almost equal. So there are a lot of misclassification and that. And that doesn't. That does not. It's not a good model. So for our random forest feature importance none of them does a good job. But attendance rate, previous grades and study hours does a slightly contribution to our model. For so the model our the strength was robust which we use robust algorithm change choice for mixed and noisy data. And it was a proper methodology. So we used proper methodology which was cross validation hyperparameter Toning and we used comprehensive analysis pipeline. The limitations Data quality was one of the biggest challenges as I mentioned earlier, that impacted on the results, the reliability and there were weak signal detection between the features as I mentioned earlier. So in the future, if you, if, if you rework on this data, we would probably, you know, compare it with different algorithm algorithms and ensemble methods. We use also advanced feature engineering, which I have yet to learn and I look forward to that and other metrics that are there that we, that we will learn advanced, advanced methods. That is all. And I am happy to take any questions, concerns, comments to make this better or how we would make it better. Thank you. Yeah. Excellent presentation. I would like to go to your problem statement slide. Sure. Yep. Okay. Okay. So now so these are the main features. So the the study are extracurricular attendance, attendance rate, prior academic performance. And if you can go to the feature importance slide where using decision tree which you were showing. Yeah, yeah. Which is the most important feature here. Yeah. Oops. So yeah, the most feature importance is attendance rate. So previous grades matter, attendance matters, study hours matter. But okay, so now this data. What is the data set? Where is this coming from? I got it from the Hugging face website. Yes, it was strange because the numerical features, they were percentage. Already got it. One thing I want to tell all the students because I asked couple of students throughout the course, throughout the presentations today. The presentations are excellent. So is this presentation great and excellent. So one thing I want to clarify to all students is when you're choosing data set, maybe I want to put this in the requirements next time is we definitely want to get a publicly accessible data set. But the, you know, the slides you're building, the assignments you're doing, this is real investment. So you might want to show it to your employer. So one of the most important questions they will ask you is, is this data set reliable? Where is this data set coming from? How many people this, how many people are using this data set? Are there any models that are built using this data set? Because that shows the credibility of the data set and all your results, all your modeling depends highly on the data set. So I wanted to really tell to all the students that, you know, ensure that when we are working these data sets are really credible. So that sometimes what happens just because of the data set, your results might be not looking good. But when you have put a lot of effort, it could be the problem in the data set itself. Right. So that's something to watch out for. But other than that, excellent presentation. Thank you. Any other. So Any questions, comments or concerns so far? Okay, so now from a deliverables point, we have two more things to go. Right? One is your recommendation engine project. So I redesigned the module, which I showed you the slide at the start. So please take a look at the module. A lot of content is updated there and you can connect all the dots. Now, There are some GitHub repositories at the end. I. I am likely to record a video on that and upload it soon. And for the last lab, please wait. The announcement will come in a day, right? And again, there's not much effort to do in the last lab. So again, you have any questions, please reach out to me in the office hours. That's Monday to Thursday, 4. 4:30 to 5:25pm and if you want me to meet at certain time other than office hours, please send an email. Thank you. We'll. We'll see you in the next class. Thank you, Professor. Thank you. Have a Good evening. Dr. Nikovorov, I have a quick question. Yeah. As you know, Matthew Grant, my teammate, and I had requested extension. So would you like us to present it next week or record and submit? It would be great if you could present it next week. Sure. Have to do so. So, sorry about extension. No, no, no problem. Absolutely fine. No problem. Thank you very much. Yeah. And Professor, I have a question. Very quick. Yes, I submitted my code, but now I do recheck it. The cell that I did, the outlier detection, for some reason it got deleted. Do you want me resubmitted or is that okay? If you have the code resubmitted, that's. I do. Yeah. Okay. Yeah, please. Yeah. All right. Thank you. Good night. Good night. 