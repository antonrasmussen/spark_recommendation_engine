Hi everyone. We'll get started in a minute. Sam. Okay. Sorry for my, my camera. Yeah. The class will be totally audio based. I apologize for that. Let me go through the lab first. Okay. So are you guys able to hear me? >> Speaker 2: Yes, I can hear you. >> Speaker 1: Thank you. Okay, so the first thing is this is the extended lab and let's go and look at the, you know, the grades. Not the grades, but the syllables. Yeah. So the extended lab is 20% and this was 10%. So now one of the challenges in this extended lab. Let me go through what was the original plan is. So this is a lab spark. Let me open this up. Okay, so first thing, you don't have to execute this lab. You know, the reason is this lab was supposed to be supported with some of the Amazon. I was trying to get, get this thing done really on the cloud so that you all can have experience. And that's what I was trying to get some Amazon credits. But the, the program I think the which they were using, Amazon Educate is not offering. I tried but it's not offering any more credits. So. And this lab is dependent on the credit. So I was trying to see if I can get any alternate infrastructure, but it's not possible. So what I've done is I still went ahead and so the program I'm talking about is the AWS Educate account. So originally when create Educate account you get some kind of resources and you know you can run some of the PI spark and everything on the cloud. And this is what I because some. Some kind of a real time experience because the PI spark we are running here is all. Yeah, databricks is up to some point it's giving but mostly when you go to companies you would be using AWS a lot. So that was the intent. But we don't unfortunately I don't have the resources for this. I was thinking that we would get it from Amazon free. And if you start putting your credit card then if something goes wrong, it will bill you. And I don't want to get into those kind of problems. So what I've thought of is I'll just quickly go through this lab and what was the original intent? And I modified the lab a little bit. It made it super easy so that you still understand why we wanted to do this. But I just asking you to write a small half page report, that's it. You don't have to execute anything but you at least would have to go through the document and just read the tools a little bit and that's it. You will be done. With the lab. So let me go through this what the original intent of the lab was. So many modern day data sets are huge. We are dealing with big data. So facial social graph, that's like 1 petabytes of data, Twitter users, 12 terabytes of messages. So there's a lot of data. And this lab originally was intended to use in New York City Taxi and Limousine Commission, tlc. And this lab, this data set is extremely popular if you look at the interviews and all also. So you know there's lot of, a lot of interviews. Use this data set. Now for this lab you will be using PySpark on AWS, using Elastic MapReduce EMR to analyze large samples from this data set. So now we know what Pyspark is, right? We know now ads, aws, Amazon Web Services is a cloud service. So basically now whenever it's for students who are understanding cloud for the first time, the major difference between cloud and a normal data center like something ODU could have its own data center. On premise. The difference is huge. In on premise. Like suppose if you talk about OD data center, all resources, all computers maybe would be in the physical data center. But when we talk about cloud, the cloud data centers could be spread across many countries. And you know, when it's spread across many countries there could be multiple VMs and physical servers. But, but with the help of virtualization. So maybe the data centers could be somewhere in dc, the data centers could be somewhere in California. But you would be using a cloud account sitting here at Norfolk and AVM would be spin off somewhere in based on the infrastructure that is available in D.C. or somewhere in California. And through the virtualization you will be able to access all the resources. Now this means that you could have like the way we do it for databricks, right? So the databricks, everything is in cloud. So even the cluster is in the cloud. So you are able to offload your compute on somewhere and everything is happening at some data center we don't even know about. That's the advantage of cloud services. And AWS is a very, very, very popular cloud service. Very popular among the government as well. And Elastic MapReduce is a service again that is very much useful to run your big data services, right? So big data services could be something pyspark on EC2 instances. Now what is EC2 instance? EC2 instance is something like these VM machines which get spin off in the cloud so that you can add dynamic memory, a dynamic hard disk. You know you can. So the main difference is like if you had a physical server, you would, you would buy 30GB RAM, maybe 8GB RAM, maybe 100GB hard disk, and that's it, right? You have to use it. But with Amazon EC instances, what is the benefit is like you can, you can initially set up a VM and the VM could have like maybe 15GB RAM and 100GB hard disk. And then based on your demand, you can increase the resources. So it's, it's very flexible. That's one of the advantages of EC instances through Amazon. So now with emr, what you could do is you could install all the Spice Spark and run all these BIG database services. PySpark is only one of the tools. You have so many tools, right? You can run all these tools on EMR and then using Elastic MapReduce, you could run all these tools on Amazon infrastructure and then work on your big data projects. So something, if you were working on an enterprise or something, they would be doing these things, right? In academics, we are running on a cluster here, but, but in real world you would be running everything in the cloud. And that was my intent towards waiting for this lab. So now if you're really interested, I still went ahead and gave you this lab. So you know, if you can, you can focus. But be careful if you're using Amazon, right? If you, if you use your own credit cards, it might bill you. So, and I'm not asking you to execute anything, right, because I, I don't have the credits. So I'm saying this again, if, if you really want to play around, do that at your own risk. Because if you, if you put your credit card information and then you forget about you don't stop one of the machines, right? It will bill you and it bills in like hundreds of dollars, right? So that's, that's one of the reason I'm not, I, I don't want to get into that kind of problem. So yeah, otherwise I would try to do something. So, but, but still, if, if you want to practice, you get some free hundred credits or something, right? Try to try to do that in your own time. But for the course you don't have to do anything. I mean, from execution point now, the, the lab also, you know, basically gives access to the data sets, right? There are a couple of tasks basically. How, how would you. It's, it's set of calculation using Pyspark, basically, how do you calculate the long trips? These are some computation you want. We wanted to do using pyspark, but all the code was going to run on AWS and EC2 instances that that was the original intent. And different, different software, different tools from Amazon would have been used. And the Most important is S3. Now what is S3? S3 is like a bucket. It's also called a data lake. You can call it as a data lake. S3 is just a storage layer. So remember we were talking about HDFs, we were talking about Hadoop file system. Where do you store all the big data files? You need a storage layer where you have to store. So S3 is a really good storage layer from Amazon. So you could put all your parquet files, big data files in S3 and then you could do your compute, you could get your results and whatever results you get, you can again store it on S3. Because even to store your results you need a storage place. S3. And even if you look at some of the job requirements, they always ask you, do you know something? Amazon Services for Big Data. So cloud is a component also for your big data engineering nowadays. So knowing Pyspark how to do that's really good. But you also need to know a little bit cloud especially most of the times it's Amazon. So if you know a little bit about AWS and how to use Pyspark with aws, that would be adding another skill to your vision. So S3 is nothing but your storage layer. Right. And this all instructions is just how to configure your S3 and all those things. Now there is also. Just give me a minute. Okay? Yeah. So on S3 there is another tool called EMR which I was trying to discuss. Elastic Map Reduce. So here there are some set of instructions. How do you create some configuration and go to Amazon EMR and how to set up this notebooks, how to set up your pyspark and how to basically run everything that we generally learn in databricks, how to run it in something using EMR and EC2. Right on Amazon, basically. So this lab was originally designed for this. And I highly recommend that you please take a detailed look into this lab after the course. Right. That's, that's why I still kept it here. Right. But coming back to your task, what you really want to. And this is the notebook. This is a notebook which accompanies this PDF. So if you are trying it on your own, this notebook will be very helpful. Right. This basically does all the computation where the code is. Remember we were, I was trying to tell that there is some computation going on here. These are the, these are the methods that we want to run on big data. And those methods how to compute it, how to pull the data from Amazon, how to write the outputs into Amazon. All that code is available in Pyspark ipn. So if you it would be really good. It will be really a hands on experience for you. The content is really good, right? But now let me, let me make it very, very clear in the lab and you know, since we are almost end of the course, I know you have lot of things going on. You have a project at your hand recommendation system. So I don't want to add anything more since we don't have the infrastructure. I'll just make it very, very simple. And all I want you to do is just write a half page report, only half page. All I want you to know is these things, address these things in your half page report. The first one is why is working with large data sets difficult for a regular computer? You have been studying about it. Why do you need distributed computings like Spark, right? And these are the tools which I was trying to tell. So spyspark, you already know AWS emr just you know, you go to the website, it's very clear what what AWS EMR does. It helps to run your Spark jobs on Amazon. That's it. Right? And Amazon S3 is just for storing big data. So just look at these tools once, just do a small research maybe you know, it will just take you half an hour to one hour. Understand these tools and just connect these tools and you know how, how would you how they are helpful and give one more examples how you would use Pyspark and AWS right beside taxi data set. It's up to you and it doesn't have to be extremely technical even because I know that you might be reading AWS S3 for the first time. AWS EMER for the first time. But I'm just asking you to look at a very very high level. You don't have to go through the complete documentation, spend hours and hours of time, just a half hour how if you had a large data set, how Pyspark, how AWS EMR and how Amazon S3 would come together to solve your task. That's it. How could be these tools useful in your field or in future career? That's it. So at least, even if you're not executing it, at least you know these tools exist, right? And you can then plan that to learn these tools. And I highly recommend that you learn these tools in addition to the course which we have been going through and because then you will be ready for the actual job unless you have AWS experience and you have not run these tools on aws. It's going to be challenging with the interviews, big data jobs. So at least know what these tools means. Right. So this is, this is your lab and the due date is August 8th. We are not executing the lab. Right. We are, we are just. But you have the code and if you want to execute it on your own. Right. So before I go with the lecture today, does anybody have any questions with the requirements here? >> Speaker 2: I have a question. >> Speaker 1: I have a question. So. Yes. >> Speaker 2: Make sure I understand correctly. You just want us to answer these. >> Speaker 1: Questions and if we want to execute. >> Speaker 2: The lab, we can, we can take a look at it. But you're not requiring us to. >> Speaker 1: Yes, there is no requirement. And let me make it again, you know. Yeah, so just, just answering the questions pretty much. Yeah, that's it. Okay. Thank you so much. Yeah. Professor, what about Lab 7 and 8? Or are we going to go over them in class? Okay, let me go over them on the class as well. Yes, so today I will go over them on the class. Okay. Yeah, can add credits. Okay. Okay, I'll go through the labs. So let me go to the. So which labs are these? The. Yeah, one I was trying to go. So the Gen AI lab. Lab 8. I believe we did this before. This is the one which we didn't do. Let me before check with the class. How are we doing with the Lab 8? Did you guys execute Lab 8? Did you have any issues with Lab 8? Yeah, one of the notebooks doesn't work. Was it lab seven or lab eight? >> Speaker 2: Lab eight. >> Speaker 1: Lab eight. Okay. Okay. Okay. So let me do one thing. You know, let me go to the lecture. Let me have. Because I have some lectures ready. So let me finish the lecture and after the lecture we will take care of these two things. Okay. Okay. So there is something which we couldn't cover till now, and that's the performance part of, you know, the spark. So I will go through that first. So let me open that up. Okay. We'll first go through the performance tuning. So in Spark, we have been executing some of the Spark code. And in Spark, when the data is huge, one of the biggest problems is with the performance. So if you have really, really big data and you have Spark code that is running for hours, and if you don't tune it properly, you don't configure it properly, it can just run for days as well. And there is not only a single, there will be only a single, you know, database Spark project that would be running on Production, there will be multiple projects. You will be, you know, multiple teams using the same infrastructure. So maybe your project is something while in production, it could create problems which would impact the infrastructure. So it will impact the other projects as well. Because maybe your project is taking more memory. It's, you know, it's taking more hard disk, right? And that will lead to less resources and it will impact other, the efficiency of other projects as well. So dealing with the performance, what are the general problems with Spark code and how to deal with them is a very important skill set for big data engineers. Now let me. Yeah, so now one of the most important, you know, in generally concept is called caching. Now caching is something, you know, when you have, you know, in general you have data, right? You fetch it from the main memory. But if there is, there is some sometimes requirement that the memory is available, you know, there's something called cache. So whenever you have some frequent items or frequently used data, you could load that data frequently from the memory, right? So that's called, that's called caching. So you could, it makes your task extremely faster. So for example, I could give you an example like in general, without even the big data, suppose you have something called as Amazon website. And an Amazon website you have a lot of deals, right? So many times all those deals might not change. I mean the, the deal price might not change in the next 24 hours. So when, so when the Amazon page loads, it has to, you know, display that data and that data, you know, in the background, it will fetch from the database. The Amazon website will fetch the data from the website database. But by doing so it might take maybe a second or something and that's a huge amount of time. So instead of that, why not put, because millions of users are querying that data, why not put that data in some kind of a cache where the backend doesn't hit the database, but it actually takes it from the memory and something from the cache. So it's immediately. The phishing is extremely faster. So the rendering on the website is also faster. So the caching can improve the speed a lot. So that's one, one of the use case, right? Caching in another ways could be something called an echamai. If you ech my content content caching a lot of times these videos, suppose if you have Netflix or something, right? The videos also need to be buffered, right? So if the, if the video servers, Netflix servers are sitting somewhere in California and I'm trying to fetch something from maybe Asia, right? So there will be huge amount of traffic generated and then there will be a lot of time to buffer this video. So why not have some servers which have already this content ready to go very close to this, to the client, right? And that you can cache all the movies and everything and content ready to go and it will get access fast. So that's generally what caching is. So caching is not just, you know, in your local machine or something related to website or it could be. I'm also trying to tell like content caching content. Right. It's also similarly the caching also applies and when we talk about big data. Now in context of big data, there is something called as how Spark does the caching. Right. So how, how Spark handles caching and why caching is done. So that is one thing you would learn in this slide. The second thing is the five most common performance issues. See Big data. I was trying to tell that there are a lot of performance issues in Spark. One of the top performing issues is spill, skew, shuffle storage and serialization. These are general problems in production that spill, skew, shuffle storage and serialization. So let's see. So Spark supports caching intermediate results in ram. Once cached, the partition can be again without recomputing from scratch. This saves time since hard describe is not measured in milliseconds while RAM access is in nanoseconds. So you can bring down a time a lot. And but one of the problem is like here, the caching is not by default supported. You have to configure that and once cached, that partition can be used again without recomputing from the scratch. So see, whenever you have some data, especially with Spark and the code is distributed. So whenever you're doing some compute, you know, internally the data. Remember we have all the data shuffled in partitions. And whenever you do in compute, whenever you do an action, that action will lead to computing. So if you are doing this action multiple times in your code each time, the compute will happen. But if you have saved your results in cache, then the computing will not happen. To summarize this, if you have done caching, you will save a lot of recompute. And that's the advantage of doing caching. Now let's take a look. Now before we go further right, there is, there is important concept, something called a serialization. We need to discuss serialization because we will be using that in other slides as well. Serialization is the process of converting an object into sequence of bytes which can be persisted to disk or database or sent through network. The reverse process of Creating object from sequences of bytes is called deserialization. Now this might now try to visualize this because serialization is extremely important concept. Not just with Spark, but generally in programming also. Now, whenever you are, maybe you are a Java programmer, you are a Python programmer, what do we do? We create objects because all these are object oriented languages. In general, serialization is a process. Suppose you have an employee object or a bank object. It is just a class and you created object out of it. An object is nothing but a space created in memory and you will allocate certain bytes for that object and you will put some values to the variable. So object might contain some valuable variables and you will have assigned some values for it and it will have some memory space and that's an object and you use it for computation. Now for some reason you decide that whatever you have loaded in your memory, you want to take that and save it, right? Because you want to send it to some other computer or something. So in that case what you will do, you will serialize your object. So basically whatever is in memory, you will take that bytes, write it and you will save it in either disk or send through the network. But before doing so, you need to convert that object into a sequence of bytes. The process of converting the memory object into a sequence of bytes is called serialization. Once you send it through the network or you are stored in the database, that's fine, it's finally saved as a file or something. For some reason you want to again load that data back into the memory to do that. That is called deserialization. Deserialization is the reverse process of creating objects from sequence of bytes. If you have saved a file in database, you read the file, convert that bytes back into the object and load it in the memory. Serialization deserialization is very important. That's how a lot of object orientation computers who are implementing these programs communicate with each other. So if you have written a Java program and one PC and you have another Java program in another PC, right? And you want to interact, you know you will be sending some objects, Program one will send some projects objects and program two will receive it. Program two will send some objects, program one will receive it. But that process of sending and receiving is implemented through the serialization and deserialization. Extremely important if you're trying to, if you happen to give an interview and object orientation, which is Python, Java, it's a very known concept in those areas. Now we will see later in the slides why serialization important for us and big data. Because especially we need to know serialization, how it helps spark memory. That's why we are learning and we'll see that now two options when you are coming back to caching, we start with caching. When you do caching, there are two options available when caching in Spark. RAW storage and serialized storage. Here are the raw characteristics of raw and serialized storage. So the first thing is what does raw caching does? It's pretty fast to process, right? It can take up to 2x to 4x more space. It can put pressure in JVM and JVM do JVM garbage collection and then use something called. You would have to use RTT to persist and that's raw caching. So it's not serialized. So on the right side there is something called a serialized caching. And compared to the raw caching it is slower in processing. Overhead is minimal because when you are doing serialized caching it's little bit more sophisticated, it takes less space because you're doing everything optimized for memory and it's less pressure. So we start with two things. Raw caching and serialized caching. Now caching executes in executor memory. If rams fills up least LRU determine what to evict. If evicted, data will be recalculated, recached if needed. Now Executor, remember Executor is the one node in your Spark framework which is actually executing your task. If you remember initially when we were talking about Spark architecture, we have something called as job stages. And then task executor is the one which is finally executing your task. And that task you will have memory. Caching occurs in that memory right? Now if you want to, if you want to do caching, there are different types of settings that is available. And if you look at one of your labs right there should be. Where is this? Let's see. If you look at your spark performance, I'm not going to open it up. But you've already done it in performance tuning lab, right? You might have seen something called a cache. If not, just go back and look how this caching works. It's pretty much self explained. It's shown by just looking at the input output. There are various examples in this mod 10 so you can see how, how the caching works, right? So just, just take a look at this mod 10 lab notebook. Now when you do caching there are different types of setting available. One is memory only. So you know whatever you cache, it's only stored in the memory if you the another one is memory and disk. When you can store your results somewhere in the memory and also partially in the disk, right? But based on your requirements, you can use these settings and your objects, especially your Java objects that will be serialized and deserialized based on your settings. Now one of the questions you might ask is why the Java objects, why are we looking? What is JVM and why Java objects? We are talking about Spark. Always remember your Spark code is written in Java. To execute Java in the end you will need JVM Java virtual machine. So without JVM you cannot execute Java code, right? And here we are talking about the settings for caching. So if you are doing actually caching, what are we doing? We are basically suppose you are storing everything in the memory. What does it mean? It means the store rdd. Rdd. Remember, everything is rdd. Let's not forget we have been discussing machine learning, we have been discussing data analysis. Spark we started. But what was RDD to start with, right? RDD is a fundamental data structure. At your Apache Spark. Everything you know at native that it's the native code. It's a native data structure that you work with in Spark. Now remember we have data frame, SQL data frames. We had all these APIs. Your assignment one, you never worked with RDD directly, right? To work with RDD directly, it's. It's little bit challenging, it's not that programmer friendly. But always remember that these data frames is nothing but an abstraction over rdd, right? And now RDD everything, this is all Java, right? And to, to, to. To. When you're caching, what are we caching? We are basically caching these Java objects, right? So when you give the setting for memory only, you're caching it in the memory. Memory only. If you give it memory and disk, then you will deserialized Java objects in jvm. If RDD does not fit in memory, store the partitions that don't fit on disk and read from there unneeded. So in some cases you want to, maybe you have a big data frame, right? And you want to purchase the data frame and it's not fitting in the memory. So if it doesn't fit in the memory then you go to the disk and save it, right? So that's the. Yeah, yeah. So this is the trade off between RAW and serialize, right? So here the memory footprint is right. If you look at, on the left side. Let's see if you do. If you do see what kind of caching we have, we were discussing. There are two types of caching. One is raw caching. Which is deserialized and one is serialized caching. Now the big difference between raw caching and serialized caching is now what was so here. To serialize an object means to convert it straight to a byte stream. So suppose you had some kind of data, right? And that data do you want to save as it is to the memory and disk or do you want to convert into a byte stream and save it? If you're converting it to a byte stream, then it's called serialized caching. If you're not converting it to a byte stream and you're saving the data as it is, that is called raw caching. See, whenever you are applying another transformation, another operation, it's more time. That's why it's like, you know, it's going to be slower processing than RAV caching. This is pretty much faster. Caching right now can take up to 2x to 4x more space. Why? Because it's raw data, right? You are not converting into bytes. Bytes means less space, right? That's why it's going to take more space here. Here it's less specific. Everything is byte, right? Now let's take a look at. Now we understand what is raw caching and serialized caching. These are two options. Now if you had to take raw caching and see the performance. Take a look at here. Your RAW caching size is more right From a performance. If you are saving RDD as it's an original format, right? Then you know, you. If it's. It's going to take this. If it's a hundred MB data frame or something, it's going to take almost 400 MB of, you know, your, your memory space. While a hundred MB, you know, 100 MB of your serialized data frame would take very less. It's going to take maybe only 50 MB. So it's, it's saving a lot of space in MEM memory. So the memory footprint as your data increases, as your data increases, you know, raw, RAW caching is going to be very difficult. It will eat up your memory space, right? So that's the disadvantage. Raw serialization will eat up the memory space. But here it will be less, you know, it will be okay. So that's the problem. Now here on the right side, right on the right side, what we have here is it's a processing time. So once on the left side we have the memory footprint. On the right side is the processing time. So if you look at it, if you had raw data to process that raw data to cache it, it's much faster as the data keeps growing. It's not taking much time to process it. But if you had serialized data to process it, it's taking huge amount of time as the data grows. Now why is this? Remember in serialization you are basically, you know, converting into a byte stream and that that is all going to take more time. Serialization, deserialization, that is all going to take more time. So the problem here is this is the trade off. If you, you have two types of caching. Of course caching will improve your performance if you did it right. Right. You don't again, in general, not just big data, you don't want to put everything in the cache. Right. You want to only put important items in the cache because cache is limited. Right. Otherwise you won't be able to use what you really want to benefit out of. If you use it correctly, caching is going to help you in accelerating your jobs. But you have to be a little bit careful. And there are two types of settings. One is raw caching and one is a serialized caching. While the serialized caching is something that's going to take more time. Now what does all mean? So these now see sometimes when we are reading some slide we we make some analysis. We need to map this to real world scenario. Why does it matter? Okay, you, you, you take look always ask this question, right? So here we have two settings. You are. We are saying that there is raw. RAW serialization and there is, you know, there is also something called as would you know, serialized. There is a raw caching, raw raw caching and serialized caching. There are two types of caching available. But which one to use and when. Now that's more important. We won't use all the settings every time. The same setting every time. So it looks like this, right? So what does it this mean? The first one is for small data set few hundred ems we can use drop caching. Even though this will consume more memory, the small size won't put too much pressure on Java garbage collection. Raw quashing is also good for iterative workloads because the processing is very fast. For medium and large data set, tens of gigs or hundreds of gigs, serialized caching would be helpful. So let's go to the first point. For small data set that is few hundred megs use raw caching. Why? Because okay, it's small data set, you can use large amount of data. I mean it won't use that much amount of memory. So it's fine. Even though this will consume more memory, the small size won't put too Much pressure on Java garbage per collection. Now, for students who have not done Java before, what is Java garbage collection? See, whenever you create any object in Java, whenever you're creating, writing a program, you create some objects and those objects are loaded into memory. But sometimes as the program sequentially executes, the previous objects are just left like that. They are not still used, but they are still taken memory. There needs to be some kind of a tool that goes and cleans up the memory so that you are using your memory efficiently. Java garbage collector is just a process that runs in background and goes and deletes all the unwanted objects that are still not readed by the program. Right? So this keeps the memory intact. In C, if you have done C programming, there is something called as Malloc and D Malloc. Basically you create your memory using this. But in C you don't have anything that is going automatically and allocating or you know, clearing out this, you have to manually do it. That's the problem with C. Clean up your memory, manually clean up your memory. Otherwise here in Java you have a process that takes care. You don't have to worry too much about cleaning up your objects. So that's the first point straight away. If you have small data sets, a few hundreds of meg, you know better to use raw caching. Second one is raw caching is also good for iterative workloads. Now whenever iterative workloads, if you are using MLIB machine library, a lot of times you might be loading a small data set, a small, small bunch of, you know, maybe 100 records. Because in machine learning you show the data, small, small data to the algorithm over batches, right? You don't show all the data at once. You show 100 data. You let the algorithm run something, measure the error, then again show hundred data points. Again it, it predicts something. You measure the error. In training, it's, it's iterative. Machine learning algorithms are mostly iterative. So in those cases, if you're dealing with iterative workloads, raw caching is much better. So you can improve your machine learning training as well. If you're doing that now for medium to large data set that tens of gigs or hundreds of GIF serialized caching would be helpful because it will not consume too much memory. Garbage collection gigs of memory can be taxing. If you have large data set, that's tens and hundreds of gigs, then go with serialized caching. Because you have limited memory, you don't want to eat away the memory. That's one thing. Processing time is fine, but you still need to save memory because it will not consume too much memory. And garbage collecting gigs of memory can be taxing. Now also we just studied what is caching, what are different types of caching? Raw caching, serialized caching, when to use serialized caching, when to use raw caching. Now we also saw there are some configurations available. We have something guidelines, general guidelines to what is the trade off. So let's take a look at that. Spark storage levels are meant to provide different trade offs between memory usage and CPU efficiency. So if your objects. So suppose if you had a data frame, right? If your objects fits compare comfortably within the default storage memory only leave them that way. So just, you know, go with the memory only. This is the most. This is the most CPU efficient option allowing operations on the RD to run as fast as possible, right? So basically what it does is the is the most CPU efficient option allowing operations on the run as much as fast as possible, right? So use this. If not, try using memory only, right? And selecting a fast serialization. So if your objects doesn't fit comfortable, right? And then you might go with other options, right? Look at the others as well. What are the different storage levels? We are not going too depth on that, but I hope you get the idea of the different settings out there. Now. Spark SQL cache examples Caching both data frame and SQL tables. This is called the people DF cache. So basically on a data frame you can directly call the cache, right? Or if you are using a spark tables, remember there is a difference, right? You know your Spark SQL or Data Frame API. Remember there are different APIs. If you assignment going back to Assignment 1 Data Frame API SQL API one way you create data frame from data one way you are creating a table here. How to cache do you want to cache the table? You want to cache the data frame, how do you cache? The first one is basically you're caching a table. Sorry, a data frame. In the second one you are asking your there is a people table. You are caching the table here. Here data frame you are caching. So it goes on the cache memory. Here you have a flights data data frame. This is a data frame, right? And you are persisting. But where are you doing? You are. You're using memory and disk. Here it will be memory only. But here you are specifically telling that use memory. And what does this option tell? Maybe your data frame is so big. So it's basically telling that okay, persist as much as possible on the memory and the rest push it to the disk. This is something now what will happen? So whenever you will call people data frame, you're calling some results. Those results will not be recomputed, it will be directly pulled from the cache. So you will be saving a lot of computation. Now when finished with your table, best to uncache it. So when it is done, your spark program is almost done. Or you don't want to use this cache free of the cache. Why to free up the cache. Because then you can put another object or another data frame there and start using it. That's the advantage. How do you unpurses the unpersonal? You use this people DF unpersist and then spark dot uncache table. So this is to remove the table or remove the data from, from the persistent. This will free up your memory. Remember how much memory you are using. You are using this much memory. So it's better if you're not unpurcessing then you are. This is going to eat up your memory and you won't be able to use it. You might slow down the complete spark program also. So we have to be careful about that, right? So this is a world persist. Okay. Now these are, these are some things. If you, you know, just execute the lab, it will, it will. It's pretty self explanatory there. How you know, if it will basically show you have already executed the lab. So I believe that you might have seen that. How if you don't, if you don't, you know, use caching correctly, I think you might get wrong results. For example, I give you a general example. You know, in general example is for example, let us say, I give you a very simple example. Let us say the Amazon example I was going. It has nothing to do with big data, but at least you will understand the concept. So if Amazon website was pulling some data from the cache and the cache is, you know, so and it's basically pulling the data for a deal, right? Maybe there is a T shirt and it is $500, that's the deal running and it's pulling that data and displaying $500. Now for some reason the price changed for the T shirt, right? And it's somewhere in the database, right? So you updated the database but you never went and burst the cache. What is bursting a cache, updating the cache with a new value, right? So because the value deal price has changed, you have to update the cash as well, right? So if you for some reason you still hitting the cache and you're not, the cache has not been updated. These are the general production problem. Like okay, the front End whether it's Spark or something else, you're. You're basically reading from cache but cache got never updated right or you're reading the wrong result for some reason. That is the problems with cache either you don't update it, you don't delete it, it's not the right data. These are general problems with cache, not just in Spark. And so you have to be very, very careful if you're using Spark, ensure the data is current so that whatever the downstream tasks are that execute reliable. So cache going wrong. There are a lot of problems with that. These are some best practices if you use cache what to do and yeah, so just take a look into that. Suggest best practices based on your programming. But this is little bit more interesting here. That is delta caching. Now what is delta caching? See remember some of the slides courtesy databricks, right. So delta caching is a concept that's used mostly in databricks. So we don't have to know it more because it's proprietary. But as a concept we should definitely you know it. So and if you have heard in news or you know, in general in IT world databricks is growing a lot. So it's been adopted by a lot of governments. You know, there's a lot of things happening in the databricks side you can take a look at now the delta caching accelerates data reads by creating copies of remote files in nodes local storage using fast intermediate data format. The data is cached automatically when a file must be fetched from a remote location. See, whenever you are dealing with big data files generally what do you do? You maybe are fetching the remote server. In this case what delta caching does is it takes those copies and and it caches those remote files somewhere in databricks. And for that it uses delta caching. Now one question you will ask me. Hey, what is so special about this? Can't we just take the remote files, put it in local folder. We do it all the time and it will be faster. Absolutely. There is nothing wrong and it's best thing to do. But when you're talking about delta caching will it how much space it will use? Right. That's more important. So internally they will have all the compression techniques and everything to you know, caching. When you're doing caching you have to do it in efficient way. If you're just duplicating the file and saving it, you are that's not going to be helpful. So their internal algorithms or you know the way they were doing Caching, they have to save space also. That's the advantage here, right? So the remote files are creating local copies that's available and you don't have to fetch the remote files directly every time. The delta caching will help. Now just take it as a concept. We don't have to worry too much about the proprietary what databricks does it unless you are interested. But this could be applied to any other company. Remember databricks is just one of the cloud vendor always. Also I remind my students, always advise this compare cloud technologies, right? So if you're, if you're for example today's lab, I was trying to tell you EMR, right? That is basically Elastic MapReduce, it's a service from Amazon. How would you run Pyspark on Amazon using emr? That's a use case. If you know this in job, definitely you're going to use cloud, so that's going to help you. But somebody asked in the interview that you know, how would you use, how would you configure the same thing on databricks? Or somebody might ask you can you know how to run this on Google Cloud? Because now every company, especially enterprises, they will have their own cloud vendor. Somebody will be using aws, somebody will be using Google Cloud, somebody will be using Databricks, somebody will be using Snowflake. Now you cannot go and know everything out there. I mean knowing all the tools, of course it will be good. But investing so much time is going to be tough. So at least know one or two cloud services. You know, maybe I recommend how to do big data engineering on Amazon is going to help you a lot, right? Most of the companies are using aws so if you, if you at least know aws, then you can argue or not argue, at least debate that, hey, you know, I don't know how to do it in Windows Azure, but I've done the work using aws, right? And this should be sufficient. I mean I can hit the ground running soon day one, right? So something you can, you can always debate. So but at least know one of the one or two cloud services and that will help you. Okay, now let's see if there is something more because we have to discuss other things as well. Okay, so what we'll do is let's take a quick break and once for 10 minutes we'll come back around 6:40. And once we come back we'll start discussing these five problems. The spill, skew, shuffle, storage, serialization. Right. And, and we'll continue our discussions with the performance tuning. Right so let's come back around 6:40. Thank you. And again students who have arrived late or you know, they have not, please. You know as I'm I was trying to discuss about extended lab. I have, you know I told this a little bit in detail. You don't have to execute lab again if you're just for your own, you know you want to. This is a good lab though. If you practice it is going to be really, really helpful. And that was my intent. But unfortunately I don't have the credits. So if you, if you practice, please do practice. But it might ask you credit card information and it's on and you do it at your own risk because if you don't switch off the computer or hardware it will bill you. When I was doing something couple of years back, you know they got into big trouble. So that's why I'm trying to tell you if you forget it's going to bill you again and again. And you don't want to see that build. Now here what you need to do is only write a half page report about very what tools and how to connect it. Right. And then you're good. So but, but still see this lab, that's why I'm provided because it's at least you know how to do it in real time. Okay, we'll come back at 6:40 and we'll continue our. Welcome back everyone. Okay, so we are looking at the five most performance issues that we generally treat with Spark. The first one is spell writing of files to disk due to lack of memory. So for example you have a lot of data and you load a big data frame and you want to do some computation. You're loading it in partitions and you have memory available. But for some reason the executor memory is not enough and the data got spilled from your memory to the disk. So that is going to be a big problem. Because remember in the earlier classes when I was trying to discuss between Spark and Hadoop what was the most important thing? The most important thing was Hadoop is a disk based framework and Spark is a, you know, Spark is a memory based framework. So there is lot of, a lot of advantages when you do everything in memory because you don't have to do input output on the hard disk which is taking more time. Right? Because you load from the hard disk and then you put it on the memory. Then you move the data from memory to hard disk is more time. And Spark is all about in memory analytics. That's why extremely faster than Hadoop. Hadoop is all disk based reads and writes. That's, that's the reason why Spark got so much momentum. One of the main reasons, of course, you know, in Hadoop you have all softwares that are decoupled, multiple third party libraries in the Hadoop system to work with your resource manager and everything. But when we talk about Spark, it's a unified stack. So you have like libraries like MLib, SQL, everything packaged together. And the advantage of packing together is because you don't have to run into testing issues whether this library is compatible with that library, which you have to do if these libraries are third party and you know, because everybody will have their own release cycle, it would introduce bugs and all those things will get solved. When everything is unified, comes in this framework package together, Spark is responsible to test everything together. And as a developer, as part programmer, you don't get into much problem, right? The spill again going back is when you, when you want to write into memory, but data gets into, you know, disk that spill and it's going to be a little bit tricky how to deal with it. So the second one is skew. Now imbalance in size of partitions. So many times you would have data that is imbalanced. For example, let us say that you have five cities, Norfolk, you know, Pittsburgh and you know, so on other cities. And then you have New York, right? So there could be that, if you say the population, maybe you would say that, okay, the number of, you know, people living in the city, New York, my data would be more on there, right? So the data would be highly skewed towards New York. So now if that's the problem now when, when you know, you want to calculate something, number of people sitting in city, something like that. And you give the data of all these five cities, right? So, and suppose you have only five nodes in your cluster, right? So there could be a case where one of the node is getting a lot of data, right? If you, if you just partition it based on city and other nodes are getting very less data. Now remember, if that's the case, then you are underutilizing because you are over utilizing one of the nodes where it will be very busy with the competition and you're underutilizing the rest of the nodes. How do you split your data so that when you have data skewed so that you don't underutilize or over utilize certain resources, that's also very important. So skew is another thing, that general problem with big data you come into issues. Now the third one is shuffle moving data between executives. See, whenever you do an operation such as join or aggregation, it requires shuffle operation. Now what is, what is join? Basically, when you are, you know, getting, you're getting information or some aggregate calculations from multiple partitions, right? So you remember you have a big data big data file. And your partition, your data may based on maybe subsidies like New York and Norfolk. And it's sitting in different partitions and different basically nodes. Now for some reason you want to look at the data of Norfolk and New York together and pull some stats from it. So for that you have to do join on these two partitions, right? So data sitting on these two partitions. And when you do that, you have it will require shuffle. Basically you want to move the data from the partition where the New York data is sitting to the partition where Norfolk data is. So data has to be moved and then final calculations have to perform. Now, whenever there is moving of data or shuffling of data, it will create multiple problems. More input output, right? More you are reading, writing data. The second thing, network traffic. See, whenever you are doing shuffle, these are different nodes. Your nodes could be on the same cluster. Sometimes your node could be on separate infrastructure as well. If that's the case, you are generating more network traffic, right? So it's all load on the infrastructure. So how to deal with. And there are good some articles at Google, you know, how does shuffle, how does shuffle. What are the shuffle problems with Uber data? What are the shuffle problems with in, you know, in company called Uber, in company called Google, in company called Netflix. And you might get a lot of article. So how do big data engineers and all these big companies deal with shuffle problem? Because it is a normal traditional program when you deal with big data. And they might have created some additional framework processes to deal with such kind of thing. If you read this blogs, it will just give you how important it is so that at least you can start spending some time, if you're interested, towards the job. Now the fourth point is storage, how data is stored. So remember in your assignment one, we saw that when you're dealing with big data, you have special file formats, right? So if you're dealing with big data, there's special some parquet file. What is the advantage of parquet file? It has some metadata over it, right? So there is some advantages when you use parquet files or you know, you are not using CSV files, there are some advantages over it. If you don't save your data or format your data or load your data from efficient files, there is a lot of problem. It can Lead to performance issues. One of your questions in our assignment was want to look at the different file formats and how many spark jobs get spawned based on your file format. And if you had looked it carefully in one of the file formats would have been less number of jobs because it's more efficient to do that right now. The fifth one is serialization. The serialization is the distribution of code across the cluster. So how would you distribute your code? So one is about how do you distribute the data. Right. The another one is how do you distribute the code across the cluster. So these are top five performance issues now. Couple of, you know, a year back or something, I think I was attending something some kind of a. I was interested in taking additional. I think it's data bricks or admin. I don't remember. I was with a group of people training in a training and there were. There were, you know, engineers from, you know, who work. Used to work with big data. And these are engineers who work almost for more than 10 years. And they were. They were interested in similar problems. So the problems which we're seeing one to five is. Is very real world problem. And if you learn how to solve these problems, it's. It's something that you actually apply on your jobs. Okay. Okay. So the first one is spill. For every task there is corresponding partition. And if the task cannot process that partition with memory allocated stored, that part that data represents the partition spilled to disk or return to disk and read back again. This is what we learned, right? So spill memory size of data as it exists in memory before it is spilled. Spill. This size of data that gets spilled, serialized, written to disk and compressed. Now if you look at SPARK ui, right? It's a spark ui. In databricks or in generally, when you are dealing with spark, there is. There is something called as execution memory, storage memory, reserved memory, user memory, your memory, Spark memory is divided into these all categories. So used for execution and storage. That's the. That's the execution memory. Then you have the spark memory that the amount of storage memory immune to eviction that we have reserved memory. If something you run out of execution memory, you run out of storage memory, then you go to reserved memory. And so. >> Speaker 2: I think the professor dropped. >> Speaker 1: Sa. Sorry, I think we had a network issue. Sorry, we're getting back. Okay. I was talking about the remedy, right? So if you had data and that data got spilled over the disk and it goes to the memory or what do we decrease the size of each partition by increasing the number of partitions. So one thing you could do is increase the partition size which is the basic see in your data. When you're giving to the Spark framework, what you do is infrastructure is you basically partition your data. Maybe if your partition size is big, that means that it might not fit the memory. So why not decrease the partition size? Now decreasing the partition size means increasing the number of partition. So that's one thing you can do. And how do you do that? By managing SPARK SQL shuffle partitions, you increase them. You can use this function called repartition to large number of partition in effort to make smaller size partitions. Right. So these are some settings you can play around to as a. As a remedy for the spin. Next is the skew. Now look at the skew. When data is skewed, it is unevenly distributed according to partition. I was trying to tell you, right. Sometimes you can have some data that is maybe a lot of data for New York compared to other cities. That's skewed now. And you will utilize one node and you will underutilize the other other nodes. Right. So that's a big problem. So since partitions are nested in task for processing, the subsequent stage cannot start until the previous stage has finished. Right. So here you have job, you have stage task one, task two, task three. Right. So stage two, stage four. Right. So if you look at that, what do we see for task three? It's taking a lot of time. And task one, task two, it's taking less amount of time, which is good. But task three is taking so much of time. Right. So that's, that's a problem. Right. So if you have a skewed partition you the bottleneck becomes task three. Maybe task three is calculating number of people in New York and that's going to take a lot of time. So that's a general problem we want to avoid. And here right. Has skewed partition. All things being equal, will take 3x longer to process your partition. Right. Might even run spill to this. So whenever you have a partition size increase. This is a skew partition. It will take more time. So that's one of the problem we have. I was giving New York, but here it's basically Tokyo. The example of Tokyo, which is skewed partition. Now what do we do when to do when you want to remedy the skew goal is to fix the uneven or non uniform distribution of data across the partition. First is fix the uneven partitions or shuffle the data optimizer what column values are skewed. Now sometimes the framework can do a little bit more optimization Using something called a SKU hint. So for example, when you know that one of your column or something is skewed, right? Some of your data is skewed. You could tell Spark, hey, this is skewed, right? They give the column, you can mention it. And when Spark knows that it is skewed internally, it will optimize. It will have own mechanism to it. You know, fix this. So you could use certain configurations to fix your skew. And then there is something called as adaptive query engine. Now we will see what adaptive query engine means. It's just a SQL engine that helps you to optimize your queries. And this is something you won't be doing it. AQ is something that is a built in feature within in Spark. It's a feature which will analyze your query and over the time it will, you know, come up with the best plan for execution or best strategy for execution. So we will see these adaptive query engine separately. I mean in another slide deck, right? What would we do in case of Tokyo? No would they be taken care of A, B, C. So you could give A and you could give B as well. So both things could happen, right? So tell the optimizer about A and adaptive query engine is anyways going to help you with it. So. So this is something adaptive query engine is internally as long as you are using data frames you are using SQL Spark SQL. You could use adaptive query. You don't have to use adaptive query engine will be automatically it's inbuilt optimization framework. But skew it will additionally help you. If you tell that you know this is the skewed it is going to be more helpful now. Okay, let me see. These are some settings if you the adaptive adaptive. You know here you are enabling the adaptive query engine. Sometimes if you don't enable this setting adaptive query engine then your spark will not be efficiently using the perform will not be optimizing the sku. So you have to enable these settings and you look at the lab what you can do if you go back at your lab and see what happens when you use this setting and when you don't use this setting you can definitely see the results will get impact it. All right, so play if you. If you want to double check just go to lab and re execute it and see what happens when you disable that setting. Did we go into the shuffle part? Okay, the another one is the shuffle that is the movement of data between executors during wide operation. If you are doing join distinct group by order by repartition Count, right? These are some of the operations. When you do that, you know, there is a lot of shuffling of data and this. And sometimes you cannot avoid shuffle. Sometimes you can avoid shuffles, but sometimes you cannot avoid shuffles. Now, when, when you have shuffle, how do you. What is the remedy for shuffle? The goal is to minimize the data between executors. Remedies include larger, fewer worker nodes. Of course, if you have less worker nodes, right, then there is less number of shuffling of data. We will talk about, okay, narrow. You could also, you could narrow your columns. You know, basically, let's talk about denormalize. So first, denormalize is something called. See when, when your table, you know, you have. What is normalization? Let's start with that. Normalization is something. See, if you put all data in your single table. Suppose all OD University. Imagine what could be your data, right? It consists of employees, it consists of students, it consists of dorms. But can you have all the data in a single Excel file? Right? Maybe you can put all the columns in a single Excel file, right? But what is, what are you going to benefit out of it? First of all, it's not clear, it's not organized, right? When you are retrieving data, it's not going to be efficient. You want to update certain data, it's not going to be efficient. So normalization is the process where you break the, you know, big data and your big file into small, small tables so that they're much organized so that your insert, update, delete operations are more, more efficient. That's the process of normalization. Now the same normalization could also be problem in databases in general. Because if you want to retrieve some data or something, you have to join all these multiple tables. That is little bit tricky, that is a little bit complicated because more are the tables. You have to join all these tables. Now that could be problematic. Now denormalize is a process which basically saying you have multiple tables, combine them into a single table. It's the opposite of normalize. Normalize is breaking up the tables, right? It's. It's the opposite where you're combining the table so that you don't have to do a lot of joins. So here denormalization also means, is like wherever you think you might need shuffle, why don't you, you know, why don't we pre join that data so that we don't have to do shuffle? Can you organize your data in a way that, you know, even there is no necessity for shuffle. You don't have to Go to reach out to another partition. All data is available within the partition, right? So that way you don't have to do shuffle, right? Another example is something called as broadcast small table to executors. See, shuffle is something where you don't. Maybe you have partition data. Suppose for example, I have data from six cities. And for all the cities I want to calculate some aggregate. But. But I also want to calculate with for all the cities including New York. So for example, when I'm calculating Tokyo, I also want New York. When I'm calculating Norfolk, I, I want the data from New York. So for each city I want its segregate plus the data from New York, right? So if that's the case, what would I have to do? I have to always do a join to New York data, right? The partition where the New York data is sitting. So which will lead to shuffle, right? So one, one thing I could do is instead of doing the shuffle, what I could do is I could do a broadcast, which basically means I have New York data with my master. So here remember we are, when we are dealing with Spark, we have master and nodes. So architecture where master is there. Also there is a JVM that is sitting and from there you send out your task to the executors. So what you can do is from your master, you can send a copy of broadcast, which is basically you can take a small copy of New York data, whatever data is sitting on that partition, and make a small copy and send it to all these nodes. In that way you're not doing shuffling. You're not doing shuffling because every node will have its own copy of New York, right? So that way you don't have to do shuffle. But what will happen in this case? See the problem with the way you would say that, hey, why do we need shuffle? You know, we can always do broadcast. The problem with broadcast is, okay, if your data is, you know, data requirement for this join is very less, right? You can send a copy and duplicate this data here. A lot of redundancy is happening, A lot of duplication is happening. Lot of data you have to send to all the nodes, which is sitting at everywhere. You don't want to unnecessarily do that. You need to only use broadcast, whichever it's absolutely necessary. If your data is too large, would you take the data and broadcast it every to all the nodes? That's unnecessary, right? You don't want to do that. So that's one of the problems with the broadcast and you want to avoid that. So these Are some of the remedies which we discussed for the shuffle and storage. Let's take a look at storage now. So some files formats are if you have run into storage. Some file formats are columnar storage. If your query selects on some columns, Spark only reads those columns. So one technique you could do is like data skipping. So select only the columns, which is required. You don't want to sell because sometimes we can as a program we can just write select star. But we don't want all the data. We only want only some columns. See, generally in Spark you have some settings. If you select only the columns internally from the physical file system, it will read only the only those columns. Only those files related to those columns. And that will save a lot of storage space and time. That's one thing you can basically when you're doing the select statement, you can be a little bit more careful and that will help with the storage. The second one is data skipping. If you are doing parquet delta orc. If you use where clause it can. It can decide which which files to skip reading. And that will be more efficient and faster. Sometimes you could use some optimizers files on disk between 1 and 128 MB to 1GB, right? It's. It's a much internally how. How these files. Whenever you're dealing with parquet file or any file you know these files are stored on the disk and there need. There would be optimization process how this data is stored on the disk, right? And it might some methods called Z order how to organize this data or how to save these files on the disk. But if you generally if you can keep the files on a disk between 128M to 1GB maybe that better It's a more efficient ways for the. For the spark to organize this data, right? So smaller than 128 and we run into 10. But if you do you minimize the file system more. If you make it less than 128 MB it becomes a tiny files problem. And you can auto optimize or manually run optimize command, right? You can do certain things like that. And there are advanced techniques using bloom filters how to improve your storage. A bloom filter is some kind of a indexing based mechanism to improve your storage. Let me see anything important here? Okay, one. One thing important here is the Python UDF versus Pandas UDF. See you know what is user defined function, right? User See why do we write user defined functions? There are always any framework can give traditional SQL. Also you can have something called as you know Built in functions you want to do apply Max min normally SQL. Also you can do this. Similarly when you are dealing with Spark, Spark Framework, Spark, SQL, it gives you some built in functions. But many times you would like to write your own user defined functions to write some custom logic. Now when you are using Spark and you are using python based user defined functions, right? You can. And if you go back to your lab and look at this, I'm talking about the performance labs, there is a use case where you have some code written in Python user defined functions and you have some code pandas. Remember we have Pandas as well. So you have some code that is using Pandas user defined function. Now when you execute both the user defined functions, you can see that the native Python code is taking too much time time compared to Pandas user defined function. Now this is a performance bottleneck. So if you write, if you have user defined functions and you, you are using it a lot in your code, right? A spark can be very slow with that. So it's better to use Pandas user defined functions, which is faster. Now one thing always in big data, remember you might think that if you execute only one function at a time, right? So if you are executing a Python function it might take one second. But if you are executing Pandas it might take, let us say, you know, maybe, maybe Python function takes two seconds and it takes by the pandas take one second. You might say, it's just one second difference. But if you are executing repeatedly, repeatedly many times, consuming that function multiple times, you might save just 30 minutes over it, right? So that's, that's advantage. So we have to be careful these small, small tricks and that can save you time with the final execution. Right now there are, there are some too much, I mean in some slides there are like very much details of the Bloom filter works. It's up to you guys to see if you want to read those things and go more tough. But from this slide deck, what I really wanted to cover was the most important thing that look here, there are five issues in real world, right? These are spill, skew, shuffle, storage, serialization and there are some common remedies at a high level what we can do, right? And the labs, if you look at the performance labs, they talk some of these things and show you in the code what issues you might get and how to some of these issues you have even how to fix it. We also looked at cache basically and that's also part of your lab is how to fix, how to persist, how to cache the Labs, you might have already done it, but this is a supporting slide deck for them. Okay, so now let's, let's do one thing. Just give me a minute, I'll share my slide deck. Okay? Okay. So here. Okay, we will discuss something. What, what are the benefits of the new Spark engine? It's called Catalytic, at least you should know. What is Catalog, Catalyst, Tungsten, at least. These keywords are extremely important. What, what in your Spark framework. So let's try to look at that. Okay? The objects is. After completing this module, you will learn about Spark Catalog. What is Spark Catalog? And Spark Catalog is basically, remember we have Hive. Hive is like a metadata store where all your big data tables that could be stored. Now, Spark doesn't have its own data store, right? It doesn't store its own data tables and everything internally. Hive is something that is using and Hive is basically a data store where it will save all your big data tables. Tables are different from files. Tables are very, very big tables. And these are Hive tables. And whenever we are working with these tables or Hive tables, whenever you're working with Spark SQL tables, these are all Hive tables. And that to deal with it, we have Spark Catalog. The second one is Catalyst Optimizer. Now, Catalyst Optimizer, whenever you are writing Spark SQL code, all these Spark SQL queries, right, there is optimization that happens behind the scenes. And we are not doing the optimization. It's part of your engine. It's Spark or Spark Engine. As a Spark programmer, we are not doing any optimize. Yes, we might drop some hints, some configuration parameters, but this is something that is done internally. And to know that it is done, it's very much useful so that you can drop certain hits to the optimizer. Now the third one is called Tungsten Encoder. Now what is Tungsten encoder? See, the first one, Catalog, is to deal with the tables you have. Basically, what are the tables? What is the metadata of the tables? What is there in the data store? What did you save in tables? All that is Spark Catalog. The second one is to deal with your SQL part optimizing your SQL queries. The third one, Tungsten encoded, is to deal with your memory. Very, very, very, very important. Remember, Spark is memory analytics. There should be some optimization process for your memory as well. So whatever you're putting in your memory needs to get optimized as well. And then, you know, it's this all to speed up. At least the Catalyst and Tungsten are there to speed up things. If you don't speed up and then Big data is a big problem, right? Okay, now. Now let's look at the Hatch catalog overview. One of the most important aspect of structured data now is managing metadata. It may be temporary metadata like temporary table, user defined functions. All that can be managed using the Hatch catalog. So all the tables you are creating in Spark, the catalog is basically helping you suppose now you want to look at what are the database tables, what are the metadata of these tables. All that you can view from this catalog API. So not very, very interesting, but really good to know that how to look at your metadata of your tables. So nothing more than that here. The next one is the Catalyst. Interesting stuff here. If you look at this diagram, what do we see? We see data frames, we see views and we see some data sets. These are differently the way you use your Spark SQL API to create some data frame views and datasets to basically your Spark SQL objects. Now you could create these objects, load data frames. Now how does Catalyst Optimizer is going to help with it, right? So if you look at it whenever you're writing a SQL query or you're writing a data frame internally, what will happen is there is there will be unlogical plan. Unlogical plan means okay, you have submitted as a Spark programmer, you have submitted a query, right? But that query doesn't have any plan of so it needs to execute on the infrastructure, right? I mean you have submitted a query as a programmer, but who's going to execute that query on the infrastructure, right? There needs to be the framework that looks at your query, maps it to the actual files and then execute it. So it needs to be a plan of execution plan needs to be there. The first part of your execution plan is logical plan. So first there is there comes out a logical plan which is unresolved. I mean there is some. It needs to be basically be more precise. And then you have a logical plan here, right? Once you have a logical plan that gets converted to optimized logical plan. So okay, this is, this is the best plan of execution, executing your query, right? But that's not enough. There needs to be physical plan. What is physical plan? Physical plan is a plan where actually how to, you know, actually select the files. Because every data file would be there, right? Parquet file or whatever files are there, they are looking at certain location. All that physical mapping needs to be done. And then it'd be a physical plan. Once that physical plan is there, we will talk about what this cost model selected plan is. But finally the physical plan gets executed and you might get some RDDs right? The result now what happens is over the time, over the time when you're doing this, just using querying and data frames and everything, there is something called an adaptive query engine. What it does is based on your execution, it will find out that hey, this plan doesn't work better. Another plan works better. It's optimizing, it's trying its best to come up the best plan so that you save some resources at speed. That is adaptive query engine already built in future for Spark SQL. Whenever you submit a SQL, this is always looking at your query and trying to come up with the cost of executing that query. That is the cost model. It won't know for the first time. Over the time this cost model will become building up. Once you have it properly set up internally, then it will start selecting the best physical plan. See, there's thousand ways of executing something, but what is the best way that it will automatically come? That is where your adaptive query engine right now you as a programmer not dealing with it, it's internally happening, but you need to know what is happening. So based on statistics, AQE can change query plan and have it resubmitted further optimizations. You submit something, adaptive query engine looks at it, hey, this doesn't look good. It reshuffles it or further optimizes it and final execution plan is done. I don't know if you've worked with SQL, something called Microsoft SQL or MySQL. This is small databases, it's not even big data. Whenever you write a SQL query, even on those database servers, if you go and see, there is always something called as this called query optimizer or execution plan. So you can see how your SQL query is going to get executed. If you look at the plan, you might as experienced programmer sometimes can look at the plan and say, hey, this looks too complicated or it might take more time and then they might change the query. That's the advantage. Otherwise what is the advantage of looking the plan? If you cannot make out, you have to change some query, right? So this is generally also available in traditional database servers. But here it's at the next level. This is where it's not just showing the plan, it's coming with the best plan. That's the difference and that's adaptive query engine. So here is an example how Catalyst is helping, right? So if you look at. So suppose you write a query like this, right? And you put dot explain in the end. True. You look at your Spark, you know SQL, not this. If you go to data breaks or somewhere and you know, data breaks and there you wherever you're writing SQL query, if you put this explain true it will show you all these things. Now what is this is the analyze logical plan, right? So in analyze plan, you know, in the analyzed plan. So you submitted as programmer, you submitted this query in the analyzed plan. What is happening? First somebody is doing the join and then you're doing a filter. Then this is inefficient query. If you do this, it's going to take maybe more time or unnecessary data fetching, right? But the catalyst might look at this plan and say hey, you know what? If you. If you do the filter first in front of the join, it is much better. So it is reorganizing. You submitted this, the original plan based on the query you submitted is this, right? Right. But. But Catalyst what it said is hey, this if you execute is going to take more time. So that this is analyzed plan. Why not to optimize it and it's doing optimization. What it did here in the analyze plan you were doing join first and filter first afterwards. Here what it's saying, you will honestly say lose time. So why not push the filter first. And so these are simple things which you know you're not. One thing to remember. The analyzed plan or the optimized plan is not changing your results. Whatever the programmer wanted the final output that should not be altered, right? If you are altering the output by doing anything that is wrong, your then your cative carry engine or catalyst engine, whatever it is it's doing wrong, it should never mess up with your output of what programmer wants. But how to execute, how to make it faster without impacting the output. That is. That is the intent here, right? So the analyzed plan is something was good. But optimized plan is much better. So this is. This is the main. At least you know what what Catalyst Catalyst is at the top level. I'm not discussing joints yet. There's something I will discuss later. The two things what is Broadcast Join and short Merge Join. It some some kind of background information. I mean I'm not doing that right now. And these are these, these are. Yeah, let's. This. These are some techniques once I discuss Broadcast Join. So internally, whenever you are writing your SQL queries, right? Internally there is something called as Merge Join, Auto Broadcast Join. Some. These, these are some join operation. The sequel Spark framework will perform internally. Right now for some cases the broadcast join might be better, right? In some cases the merge join would be better. But if you believe that Broadcast Join is better, you could set this in the settings and Spark will internally use the Broadcast join. And that might also save some time. This is called dropping hints to Catalyst. You can tell Catalyst, hey, use Broadcast Join internally so you can help the Catalyst engine to further optimize this. Yes, we need to understand what is broadcast. We need to wait for that. But just. Just know that you can set up some configuration parameters to help your optimization for the SQL queries, right? And these are nothing but two join operations which we'll discuss and how that can help. Now, in Catalyst, you could also do something called as column pruning concept. Now what is column pruning? See, whenever you are reading data from a table, you can do lot of data reading. You can read all the columns. But are we always leading require all the columns? Maybe we don't. Our query is only needing only two columns, but we end up using all the columns. If we try to prune our data, you know, use and remove unnecessary columns, right? Then we can automatically reduce the execution time and this will result in performance gain, right? So for example, look at this. When using column non storage par k orc delta, Spark can automatically do column pruning for you. On the left side we have a CSV file which is non columnar. Remember, this is from Assignment 1. What is columnar? Non columnar. CSV is non columnar file. On the right side you have a parquet file which is columnar. Now what do we want to get from this data frame? Select. You only want to get category, you only want category and description. On the right side, also from the data frame, you only want category and description only. Your requirement is to get two columns from the data frame. Now if your original data was CSV, how much time you are taking, right? You are taking maybe zero milliseconds. But what is the number of files that is read? It's almost 1000kb, right? But on the right side, if you're reading from a delta file, right, you are only reading how much? 261kB, right? So it's almost 1/4 of your data that you are reading on the left side with a lot of lot of time you have saved. And why is this possible? Because internally you are reading only the data columns. That is required because your file format is supporting. So Catalyst is helping here. Main thing, who is helping? Catalyst is helping. But Catalyst can only help with column pruning if you are using the columnar format. If you are using CSV format, Catalyst cannot help you because it doesn't support it, right? It's a non columnar form the Catalyst even there is optimizer, it cannot do anything because it doesn't the file format itself is not compatible with this optimization process. Right? So again you need to have the correct file systems in certain cases where internal optimization process or engines like Catalyst could help you help your queries. Right? So that's something it's key takeaway. Okay, now next one thing is something called as predicate push down. So when you execute where or filter operators right after loading a data spare, SP will try to push the where filter predicate down to the data sets using a corresponding SQL where clause. So let. Let's take an example. Yeah, let's take this. So here, let's take. Let's actually read this out a little bit loud. So when you execute where or filter operators right after loading a data set, right, Sparse SQL will push down where filter predicate to the data source using corresponding SQL query with where clause. This optimization is called filter push down or predicate push down and aims at pushing down the filtering to bare metal. A data sourcing and the filtering is performed at the very low level rather than dealing with the entire data set after it has been loaded to sparse memory and perhaps causing memory issues. So what basically it's telling us is like sometimes you write a query, right? And you might have a where clause, right? But what you would have end up doing is you are reading the whole data from that, maybe the parquet file or maybe some CSV file, you read all the data and then you apply the where clause, right? And then you filter it out. Now what Catalyst could do is it can look at your query and it will say hey, you know you are anyways doing the where clause. Why not you know, load the whole data while loading itself. You only load the data that is required. Why would you do that? See, Suppose you had 1 MB of data. If you're loading the whole 1 MB of data into Sparks memory and then you filter the data, you are eating up Sparks memory and you are wasting it away. If Catalyst could be little bit careful and it rewrites your query in a way that it pushes the filter to the data source in a way that you read only the filtered columns, right? And you don't delete the whole data, then you are saving the memory. So this also speeds up the process. This is something Catalyst could do internally and something called as partition pruning on table See on the left side what do we have? On the right side what do we have? Select star. So sometimes in your this also happens in general databases and this is more also happen something on your big data files as well. See whenever you have a table, right? Or when you have a table, it can be partitioned. See you have a big table. What will it have? Internally whenever you are reading from the table, you are reading from a set of files. The table is stored as files on your physical hard disk. But anyways that table is a set of files. Now if whenever you're querying it, you're querying that set of files. Now if we can be a little bit more smart about it and we partition the files in a way that hey, you partition all the data for city Vermont in a separate category. You separate all the data files for California in separate partition the same table but you have distributed it based on the city. What is the advantage on the left side? When you are doing select Star from customer, you are reading all this thing right on a partition table. When you are reading you only want the data from Fremont and California only for those city and the state. Then you will end up reading only those physical files. You are still reading the table at the logical level. But internally everything is partitioned and the physical files, you are only hitting the physical files that is required for this, this state and this city. You're not not reading anything else. So maybe there are 10 states. Your your query is not going to hit the rest of the nine states. It's only reading from this state, right? This is going to extremely speed up. So partition is very, very helpful. If you can, you know, partition your tables, then use it wisely based on if you're partitioning now, you don't want to partition on columns which you are not going to use in your query. Then it's not going to help you. You want to partition based on what you are frequently going to query. And these are the stats that clearly prove that PAHAV partition helps. I hope you get why Catalyst is important and how internally it will speed up your process. Extremely important for interviews. If you are, they might ask you like do you know why? Quiet list. What is hints? Why are Catalyst so important? Why is this feature important? Right, right. And that's a Spark feature. Now another feature we started, remember we started with catalog. We discussed catalog. Second we discussed, we discussed this Catalyst and the third very important feature from Spark is called Tungsten. Now Tungsten project Tungsten will be the largest change to Spark's memory execution engine since the project inception focused on substantially improving Spark efficiency of memory and CPU for Spark to push performance closer to the limits of hardware. Now what it does is a couple of things here. It does binary processing, improved performance using some coarse stage code generation. What I want to really discuss here is this slide. Now first is the encoding process. The encoder is responsible for converting between JVM objects and tabler representation. See, whenever you are writing your data, right, Remember we discussed what is serialization deserialization, right? So whenever you're writing code in Spark, right? That that code you will be creating objects internal and that objects will be written to the memory, right? And when you want to take out that data from the memory, you want to put it in the file. So a lot of serialization deserialization would be going on internally, right? Now remember, whether you are writing in the code in Java or Scala, it doesn't matter. In the end of the day it's going to be Java, right? So if you're suppose writing the code directly in Java, right? How much process, what is the processing speed here, right? The millions of objects, how many serialization deserialization, millions of objects per second you could do for Java. You know, this is the rate for Cairo. This is a one of the encoding mechanisms. If you apply Cairo based encoding technique, how much faster processing you could do with a serialization deserialization, but with the encoder, which is basically the Spark Tungsten encoder, you could achieve, you know, so, so, so good performance right now this encoder is nothing but the way you serialize your data. So it's applying some binary encoding. So, so it's 8x faster than Cairo and 20x faster than Java. So what basically meaning you use just plain Java and you do not use. If we end up not using Tungsten encoding given by Spark and you just use plain Java encoding, you would end up losing almost 20k fast faster speed, which you could have achieved using Tungsten, right? Because the way you encode data is important because if you encode it in the right way, it's much more faster to process, right? So and we saw that similar thing when we were talking about caching. We were showing the raw serialization and you know, we also talked about something other serialization. So one was the raw caching and was one was serialized caching. And we saw that raw caching gets up more memory serialization, caching takes up less memory because it's the encoding technique, right? Similarly, if you have a very, very good encoding thinking which is given by Project Tungsten, it will save up to 20x faster time. And these are extremely important because you might write a really good program. Machine learning intuition and Everything. But if you do not scale it and with the speed, you are going to lose out on Spark benefits. And one thing always need to remember with big Data is how to make sure that you are not underutilizing the resources. You are not over utilizing the resources and how to improve the speed and how to work with common performance problems which we just saw skew. And this is the experience programmer would definitely know otherwise, you know, right. Nowadays we can Write code with GPTs as well. That's. That's going to be there. But this is something it's additional and very production grade skill set. You. You need to know. Okay, okay. These are, these are something based on your lab. If you apply tungsten and you can. If you persist, what is the advantage? So whenever you do something really in the lab, right. You can always go to the Spark UI and you can see how much memory it's eating here. It's taking less memory. It's taking more memory, right. So with. With Tungsten you can always go to the Spark UI and see how much memory you're saving. So now again just a quick recap what we were trying to discuss here, right. We were discuss. Trying to discuss couple of things. What is Spark catalog, what is Catalyst Optimizer and what is Tungsten encoder? Very important features from the Spark. Right now the next thing I would like to do is go and discuss what you know, some. I wanted to discuss one of the labs, but you might have already executed it. But I want to go over it again. Lab 7 I guess and which is the data centric. I want to discuss about what actually means and it's very, very something I want to speak about data centric AI. It's. It's a. It's a very big shift in AI where we are basically saying that data is data of high quality is very, very important for machine learning models. Otherwise in machine learning we always end up discussing about hey, this algorithm is better. This training technique is that is all called model centric AI where we are focused more on the algorithm and the model. But here we are saying data centric AI something coming in recent years is more like invest more time on improving the quality of data. Naturally the whole pipeline will improve. And data quality improvement just doesn't mean that improve. You know, just the annotation or fixing. Of course that that is there. But are there any more ways to improve the data? Right? So that is data centric AI. I'll discuss that after we do some other thing here. So. So the next is the lab that lab and one of the students asked about Gen. So I will discuss that quickly as well. Now before we do that, we have one presentation left from our last class. I would. I don't want to just check quickly if the speakers are available. We are here. Yes, yes, please go ahead. I will let you share screen and you can present your presentation and then we will. I will go with the rest of the lives. Thank you. >> Speaker 2: Okay, so our station was on road accident data from the United Kingdom and we were attempting to build a ML pipeline, predict the severity of accidents. So initially won't do that. What am I doing? There you go. So initially the goal was to prevent. Was to predict accident severity. The challenges that we encountered though were that the classes within the data set that we had were pretty imbalanced. And so we were trying to find ways to kind of get around that. So what we wound up doing was building three different machine learning models, both regression, random forest and then K nearest neighbor and just see which one got us closest or which one would help the most. We did perform some initial data analysis. So here's kind of the high points. This kind of goes back to that class imbalance. Most of the accidents were slight, which is kind of what you'd hope. Like we don't want a lot of fatal accidents. And then as you can see, the majority of them, 85% were slight. And then 14% or 15% made up the remaining two classes of serious or fatal. It had a little bit of numerical, a little bit of categorical features in the feature set, which fine with us. And as I mentioned, we. There was clearly a balancing issue and we need to do a little bit of pre processing as well. So the steps we took for cleaning and loading the data was that when we did the sampling for our models, we kind of reduced the amount of accidents that were classified as slight. As a way of trying to scale the results, we also removed some rows that had missing items like time or in this case carriageway hazard just means, like, was there. Were there other. Like, was there signage on the carriageway that there were. You know, there were hazards ahead. And we also corrected some typos. The only one we found was fetal and fatal. I'm pretty sure they meant fatal. I guess. I guess you could have an accident the results in the fetal. But that seems odd. And then we remove rows that had invalid lat long. Right. So if a lat long combination was zero, then we can't really place that anywhere. So there's no guarantee that that's actually a Pile of data. And then for visualizing the data that we had, as I mentioned, we've got this big class embedded. So obviously slight for the severity is significant. Unfortunately we didn't find a tremendous correlation between the other. The other variables. Obviously latitude and longitude are kind of related in a way but you know, thinking anecdotally it is but the other numerical features that we had, they didn't quite correlate with each other in a way that we would want that we found significant. And we had a lot of, you know, outliers, particularly on the within the slight severity case, which wasn't helping our cause either here. So yeah, so the next step we had was trying to figure out which, which model, if any, would give us any, you know, accurate or significant results. I'm going to hand it over to Celtic. >> Speaker 1: Thank you. >> Speaker 3: And in terms of feature engineering there were some high cardinality columns like district, so many police departments or vehicle types and that those were remote Also irrelevant columns like the accident intakes, date time were also removed and we used one hot encoder. We also used standard scalar for the Mercuries but yeah, those were very, very few that had very little effect. And for training again we used a 20 stratified sampling and we try to compare the three methods of recreation, random force and knn and use scikit learn and pipelines for modular pre processing. So all confusion matrices again due to cost imbalance. Still the prediction did not go well because there were so many slight accidents. But in the next slide I'll be able to talk more about their performance. >> Speaker 1: Evaluation. >> Speaker 3: And overall we had seen that yeah random forest performed better overall. Logistics regression was passed with some good baseline results and with the class imbalance KN had more struggles. So overall we had an opportunity and we have once more realized we actually we had started in our first presentation we started with the ev EV vehicle data if anybody remembers that we first that's one of the reasons that we had to ask for an extension because that one was. We tried that one that was almost perfect when we tried to have the EV types with their hybrid or full EV cars and that had almost a perfect fit and that didn't work well for us. And this one had this class imbalance. Overall we have realized that in many of these tasks I think it's really difficult to have a good prediction or a reasonable prediction. >> Speaker 1: Excellent presentation. I have a question, where is the data source repository? Where did you guys get the data? Data from. >> Speaker 2: This data set was pulled off of Kaggle. It was accident data set off kaggle. >> Speaker 1: Okay. Okay, thank you. Excellent presentation. Yeah, thank you. Yeah. Okay, so I would go with the labs next. Just give me a minute, I need to set up this up. Okay. Okay. This is a Lab 7. You might have already executed it. There's data centric AI tools and framework. So let first me go over something very interesting called Clean Lab. Okay. Clean Lab, this is one of the companies, right? This is called Clean Labs and they have a platform which is basically about cleaning the data at Clean Laby really human world, human data. So this is an open source library where they basically go and clean the data, you know, that you could use to, to clean the data. And you know, if you look at their website, GoClean Lab API about and take a deeper look there, they have a studio and they also have a studio means enterprise edition of this software. But they also have open source code which we're more interested in. And what this basically does is it cleans the data so well that lot of big companies actually use this API to make sure that the quality of the data is high. Now I'm not executing this code because you can. Look, it's all you do is you need to just install this Clean Lab API, a Clean Lab package. And once you install it, you execute this part, you load your data set, and once you load your data set here you have some messy data and then you plug in the Clean Lab, right? And when you plug in the Clean Lab, it will detect what is the messy, messy data you have, right? And then you then once you clean your data using cleanlab fit, right, you can, you can push it to the model pipeline and this time you will see that there is some improvement with your data. So that, that's the only big advantage is it helps you fix your bad data and you can. Does it? Yeah. Does it clean everything? Like not everything, it's towards that missing label. So if you have at least in this, so they, if you, you might have to go, go to the website and take a look at what new feature this lab, when we were building it at that time we were more focused on the labels, if any labels are wrong. So this is at least fixing that part. But since then they could have a lot of features. So you have to go in detail and take a look at their open source API and read the documentation. But this is just a sample to show you that There are some APIs available there that can fix help you with the annotation label fixing. If labels are missing or wrongly labeled, at least that part it can do and it is doing with AI. One thing important is like what actually how the Clean Lab works is in is you basically use your data and give it to a model. Model predicts something and then based on the model predictions, you see what got wrongly predicted and then come back and fix the data. So that is the interesting. But it's not a straightforward where we, you know, simply clean the data. It's. We use AI to see what is getting wrongly predicted and based on wrong predictions, we go back and fix the data. That's the, that's the important catch here with Clean Lab. Okay, so again, as. As you know, it's a very straightforward execution. If you have any issues, let me know. But it should be executed without any problem. But for some reason if you run into a issue, I. As I always said, partial execution is fine. But the code was executed multiple times by multiple classes. So there shouldn't be any problems. If there let me know that's lab seven and then let me go to Jenny Lab. So this is due on August, you know, second, so you have time. And then there is Jenny Lab. This is already executed. One of the students was asking me, so I, I believe I went. Went through this execution. Were there any specific questions here in this, in this lab? I can't get it to get past the hugging part. One hugging face. It airs out on me and will not install. Okay, let me see, let me see. So upload and browse. All right. And this, this one or the first one? Let me see. This. This is. This is. So are you, are you the first one the gen? I haven't done the second one yet. Okay, so let me browse the first one. Okay, so here one thing please do is like go to runtime and change runtime. Don't select cpu, select GPU and save it. Right? And you can start executing it. You don't, you don't have to execute. This is fine this normal python code. But let it execute taking so much time. Okay. While that executes, shouldn't be taking so much time. Yeah. Okay, then you come here, you check what is the GPU sits available. Should execute straightforward and give you the GPU names. Right? It has GPU available. True. This would have failed if it executed in cpu. There's something numpy pandas code that just shows you what is the plot? Nothing. Nothing generative AI related yet. Yeah, you can click on it, but it will ask you if you want to sign in. You can say connect to Google. You can skip this part if you don't want to Give access. It's up to you. I'm just connecting on it. Okay. And this is the hugging face part. Now install. And I hope you're spending everybody spending some time understanding the code. I mean, you know, now with the AI and everything, you know the code going through the code is not much difficult. It gives you line by line explanation. So please at least see what each line of code does right when you have more time. Otherwise labs are not going to be that helpful. So here we are just using CUDA CUDA the framework to to by Nvidia to to run your code on the GPUs. Right. So here we are basically executing the torch which is given by meta. You've gone past Professor. I never did get past that first one where it's downloading and building it errors out while it's trying to build. Okay, so. So yeah it should. So did you try with gpu? Yes. Okay, so there are two options then then in this case if it's not executing because it's. It has been executing for. I didn't receive any complaints on this particular notebook yet. So one thing you could do is since for some reason it doesn't execute, you can come to my office hours so that we can try to see what's going on tomorrow. The other thing you can do is like then go with the partial execution. That's fine. Just submit it with the error. But if you want it to get fixed, I'm available From Monday to Thursday, 4:25 to 5:30pm and if you're not available during that time then please send me an email and we can schedule some something. But you're also welcome. If for some reason you don't don't have time, then please submit the error. That's fine for this. Yeah, right. Right there sir. Okay, chat. What should we do in case of 2q gave version conflicts? Okay, this is where it came version clone Flex. Okay, looks like something is happening because I had executed multiple times. What is the issue? Okay, throwing an error. Did throw error for everyone or change rundown. Let me. Okay. Profile. Okay, I'm sorry for that. >> Speaker 2: I didn't have a. I wound up having to install like the. The libraries in certain sequence essentially to ensure that they got through and even then it was a little. >> Speaker 1: Okay, okay, got it now because I executed it multiple times before and I also demoed this somewhere else and I was able to run it and I was confident this is going to work. So I'm a little bit surprised what's happening. And yeah, so if anybody had errors, you know. Please submit partially. One thing is we. We have one more class but you're. You're welcome to please submit it partially right even with the error. But I'll try to try to see what's going on. If I find something, I'll send immediately send an announcement for this. But it. It should work because it's a very nice notebook. It shows you the features of gen. But didn't we. Didn't we run this last time? I believe that I showed you this last time Wasn't this class. Where did I run this? I was thinking that error to someone I showed you showed this gen AI based. >> Speaker 2: You ran something similar for CS620 but not quite like this. >> Speaker 1: Okay okay, okay. It started running. Let me see if it gives conflicts error. >> Speaker 2: I'm curious to see if you'll run into the same issue that I ran into about I don't know, six or seven where because of versions that we installed. >> Speaker 1: Yes. Yeah, it's. It's doing that. Yeah. And it didn't used to do that before. So I'm just curious why. Why it's where it's throwing this error. Executed it for not only these classes multiple times, but I've executed this somewhere else also and never had this issue. So it's just. Did anything change in. Yeah. Okay. For the device I guess because you have to select for that error. It's. You have to select the GPU and it will stop error. Okay so. So if you have even partial execution is fine. But let me take care of this. I'll work on this and as soon as I get some resolution I will send it as announcement. Hopefully I should be able to resolve this and yeah, you have GPU selected. Okay so let me. Let me make sure that I have this error as well. Okay. Okay. I'll take a deeper look offline and try to see how to fix this because I wasn't expecting this. Okay but. But for files, if you have issue, as I said, you know, go with partial execution and your gate won't be impacted. And my apologies for this has. I tried not to have get you into this but. Yeah but anyways the code is definitely. Once it executes. Right. And the code is very, very useful. It's. It shows you all the features. Basically here you have different functions. Generate audio, generate speech to audio. So basically what you do here is you. You download a model. You download a GPT like model and once you download the model you can give it a seed like once upon a time and it will start Generating a story based on your seed. Then you can translate. You can give something in English it will translate to French. You can give something English, it will translate to Hindi something. You can generate audio using your the data you are giving you can generate an image. This is not working. I know. So leave ignore this. But audio you could generate and. And then speech to text also you could generate and each time you have to do device you don't. You don't click on device to reset. If you click device to reset there are some problems. But if you ignore device to reset it should work again. Let me. Let me see this offline and let me know if there are any issues here. And I will try to be a little bit more careful with the direction here in my upcoming class. And my apology for if you had to run into an issue here. Okay so that was our labs and you have the clip also. If you you know the clip also please execute it if you have any other same instructions. If partial execution is fine but you shouldn't have any problem in clip. I hope so. And if you guys still have any problem let me know and as I fix this I'll try to fix this as well. If there is anything common between students and send an announcement and you can always come to me offline in my office hours if. If you want to see what's happening for your particular notebook. Right. There was no problem with this the clip one. No problem. Okay. Yeah. And. And I hope you're you know hope these resources additional apps are helpful. There's a lot of code, you know that, that actually sample code to get you started with your projects to see what's going on in the gen industry or in general. So I but I do intend to fix the small errors which are there so that students don't have to have more problems. But the original intent is always to have more and more code available so that you can get started with your assignments and projects. Okay so we will end our wrap up our class here but before doing that I want to ask one question. How is your Recommendation engine projects coming up? Does. Are you guys having any trouble and you have the modules. Let me go to the module once. Right. See See this is. This was a rebuilt module. Basically I posted multiple videos out here. Right. This is. We all. We discussed some of the content in class but I also posted some videos. If you look at the. These are really some nice videos to get. What is Recommendation Engine? What is the system design of Recommendation Engine and you know matrix factorization extremely important concept. How so There are two, you know, just a recap, we discuss what we discussed earlier. There are two important recommendation building techniques. One is content based recommendation engine and one is collaborative based recommendation engine. Content based is something like what you're maybe if Netflix right what you are watching, maybe you're watching more comedy movies, it will predict you more comedy movies, right? Recommend collaborative is based you and your friend have similar preferences. So whatever he's watching maybe would be recommended to you because since he has, that's collaborative filtering. So looking at users and their preferences and then recommending, so that's collaborative filter filtering. In collaborative filtering, matrix factorization is one of the most important concepts. And matrix vectorization basically tells like it solves the problem where see you collect the users and their preferences and you create a matrix. But sometimes all users might not give their preferences. They will give preferences for only few things and some preferences will not be given. So you have a sparse matrix problem where some users have given some ratings or some preferences and other users have not given and those you need to fill in these gaps. So how do you come up with the latent features? Latent features means based on the users and preferences. How do you find more details? What about the different features that are hidden features, what user really likes and that is latent features and how to do that? That is matrix factorization. We don't have to implement matrix factorization, but knowing it really helps. So this is an excellent video describing that. And under matrix factorization is a concept, but there are multiple algorithms that implement it. One is ALS algorithm and one is SVD algorithm. These are built on matrix factorization, but these are implementations. Implementations are different, a little bit different. So and in your recommendation system code, mlib, one of the labs in Spark Spark, you have matrix factorization solved by als. So ALS is running on distributed cluster. All you are calling the ALS algorithm and Spark internally, you know it will execute it on multiple nodes. So the code is a lot already available in your MLIB lab. If you look at your Spark code, right, the recommendation engine code is available there right now. This is a video where movielens data set, they build a recommendation engine. This is not using Spark code. Remember in your project you have to use Spark code. If anybody for some reason has real problem that they don't want to use Spark code, let me know and I want to really understand what is the problem. But I highly recommend use Spark code because there's an opportunity to still use it and it's not that difficult. And now here from the previous video, basically Understanding how to build a recommendation engine, at least you understand the concepts and how they build a small recommendation engine. Similarly you have, you've been using Sklearn to build a pipeline. Similarly you will use here some libraries like Sklearn and build a recommendation engine. At least you know how it is built, but without Spark code. And in this, this setting this guy is basically explaining what is mlib. Right, we know what is Sklearn here, same implementation, but internally how to use MLib. Some features are a little bit different because if you have to use Spark Pyspark and build recommendation engine, you have to know mlib. Right? So that is the video for this and this is the video where they are basically using a movie recommendations building using Pyspark. So the tutorial is already available how to build a small recommendation and if you build a movie recommendation system or any other recommendation system using similar thing. Right, that's enough. That's all I want is as long as you spice park as you plug in some data, show me result doesn't again have to be accurate. But as long as you're able to run it on a cluster. Right, that's enough. Cluster means data breaks or Google Collab or anything. That's the infrastructure is your choice and that's all I want. These are some additional resources, GitHub links if you want to practice. But this is the main set of videos that will help you with your project. So if you have any issues, just let me know. And again you can reach me in office hours, but the project is due on 8th and I have to grade and do the deadline so please submit it by then because it take me a couple of hours and you know, maybe also sometimes if I have other work, other classes also. So it might take me two days and I have to hit the deadline. So if you delay it, it's going to be little bit worrisome for me, right? So and if you have something like that, you have to let me know before so I can plan it and I can let you know how much extension I can really give, right? Maybe I can give a day or something if it's absolutely needed, but you have to let me know. Okay? Okay. Other than that we'll wrap it up and we'll meet in next class and we will try to, we will try to discuss something on the joins which you have left and I will try to select a topic which we couldn't discuss yet and which I feel that you know, might be more useful for your job related to big data. So that will be your last class. That is on fourth. That is next Monday. Because eighth, we end the summer semester. Right? Okay, we'll see you next week then. Thank you. >> Speaker 3: Thank you, Professor. Thank you, Dr. Chikanti. >> Speaker 1: Good. Thank you. 