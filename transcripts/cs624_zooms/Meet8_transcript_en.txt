Hey Professor. Yeah, yeah, hi. I see that you posted the first assignment due on Tuesday. I was wondering if you can give us a little bit more time than that if possible. Okay. More than one week. Okay, let me just go and let me share my screen and do it right away. July 22nd. What date do you guys think is more comfortable? Remember that we are ending at August 8th and there, there might be. Of course there is one more **** but I want to make it two assignments. One is very, very simple and one is okay. And a project we remember it's only a month class so I can extend it but just letting you know that try to finish it as soon as possible. So what date would be good? What do you think? Is it possible for next Saturday? Next Saturday. Okay, so. So that's 26. Okay. Okay, thank you, appreciate that. No, okay, so what we will try to do is let me just make sure that we have enough students before I start. Just give me a minute so that I have. Let's wait till 6, 5 and let's start. And I have a quick question for lab three. We're doing the same thing as the other labs, just executing and submitting in the PDF. Yes, that's right. Okay, so what we'll do today is last week we were trying to cover machine learning, right? We covered up to. We started with what are the general problems in machine learning. And then we went to do discuss what is supervised machine learning. Very, very, very high level. We discussed what is linear model. We discussed what is overfitting. Overfitting is something when your model works good with training data but doesn't work well with testing data. That's overfitting. Right. And very high level. We discuss what is decision tree. We still need to discuss how what is the intuition behind building addition tree. Very high level. Again we discuss what is random forest. And we said that random forest is a collection ensemble. Remember ensemble of addition trees where random forest can have multiple addition trees and the final prediction could be the majority voting. So maybe nine trees say that it's a cat, then it becomes a cat and if the last we say it's the dog. So we take the majority voting. Right. So our goal is to get little bit more deeper into this area. So we'll continue with some labs today and we will also continue next week but only half part of because we would be remember starting with NOSQL and other things and pattern matching, that's the other half. So we'll be doing a couple of things in the next class. Next week is also Very going to be little hectic. We'll be doing a lot of labs, a lot of code, but it will be still machine learning. So today and also next week part of it. So what we can do here is. Let's go before I start anything. Yeah, so let me share with you and I'm going to put this also somewhere in the resources so that you have access to it. Excellent resource. Right. Let me make sure I have a link. Right? Yes. So what this resource has, what this GitHub page does is it is basically called awesome machine learning visualizations. I don't know if you have seen this. I personally like it when you can visualize anything. It's not possible every to visualize everything but if there is opportunity to visualize, it's much faster to retain. Right. So this is probability, statistics, calculus, linear algebra. You can visualize what is happening. You can play around with this. It's important if you are looking into data science, some stats and calculus background. But now we are talking about machine learning and the machine learning. Let's try to see couple of you know, here before. Yeah. First we'll start with linear regression and I'm not going to go again because we're focused on labs today. But see last time I was trying to tell you show an example of linear regression. Now see these are all set of points, right? These are all set of points. And what do we do in linear regression? Our goal. Remember the word regression, right. So we are, we are trying to predict the point right now given all these points, right. Tomorrow somebody asked us to predict a new point. How do we do that? So first thing is we need to understand this data, right? So here there are multiple, there are many, many, many data points out here. So what is a, we are trying to see what is the best fit line here. And if you see, if I move this data here, my line is changing. Now whenever my line is changing, what is changing here? See the equation for a straight line is Y is equal to MX plus C. So whenever my data is changing and basically my line is changing, my parameters are changing. This M and C is going to change, right? The M and C are going to change. Now what does this mean? This means, this basically means that when I'm trying to fit a line, when I'm trying to fit a line on this data, I'm basically learning these parameters. See, Y is equal to MX +C is a standard formula for a straight line. It's the formula. That's it. But, but for every data your M and C is Going to the M here and the C is going to change, right? So as the data changes, my value changes, right? So now how I'm coming with best fit line, we have not discussed it. What is the optimization process? I mean here we are animating it, but you know, what is the, how is it learning? What is the intuition behind? We haven't discussed about it. But the goal here is you. We try to come up with the best line. And whenever we try to come up with the best line, we are trying to understand parameters. And this line is nothing but the model. So what is the model? Model is this Y is equal to MX plus C. It has to be crystal clear. That model is nothing but this Y is equal to MX plus C is the crystal. The, the model and the values and M and C are the parameters. Remember when I was trying to last time we were discussing about slides, we were saying that when you are training a model, you're basically learning the parameters of a data. So in this case, in this case in linear regression, what are we learning? We are learning these parameters, right? And this R is something like a error rate. See, whenever you try to come up with a line, it may be a best fit line or maybe you can come up with a better line, right? So if I had data points, something like this, right? Keep this. There is ways that your error might decrease here. It's increasing. But basically if you do some movement properly, your error might decrease. So finally, the distance from here to here, the distance from here to here, the distance from here to here. If you square all that distance, that is the error. So every model we also measure, you know, once the model is ready, what is the, what is the error? So smaller is the square of all this thing, right? That is the error, right? So, and this, this could be tomorrow used to. To, to predict a new point. You give the X value and it will, it will. Because C is constant. You. You predict the. Put the X value and what you will get the Y value, right? So you give one feature and you will get the other Y value and you will have a point, right? So this is, this is the intuition behind linear regression. And most important thing here, the equation itself is the model and the parameters is nothing but the values we are learning while we are trying to come up with the best fit line. Okay, now let's try to take a look at other thing as well. Let's try to take a look at kn. So this is another algorithm, right? This is, this is algorithm. Basically you could have many data points here. You can and every data point. See, this is, this color is different. This color is different. This color is different. All colors are different, right? So we can have number of classes, five, right? And we can select the number of data points. So this is our data. We have five categories of data, right? And basically what we want to do is we want to group all these points into their respective category, right? Now one of the machine learning algorithms to do that is k nearest neighbors, right? So each point in the plane is colored with class that would be assigned to its k nearest algorithm points for which the k nearest are going to be the colored by. Anyway, so here, if I, if I select three, right? So three ken, the groups might differ a little bit. If I do four, it might change a little bit if I do five and the number of data points things might, the groups might differ, right? So k nearest. The important thing here is the value of k k nearest. Basically how it intuitively what, what it works is you, you, you try to find some points. Suppose this, this is the point, right? This is a point. And you, you will try to find all the points that are closer to it. And this will become a group. All the points closer to this point are going to become a group. All the point you are closer to this are going to become a group. All the points closer to this are going to become a group. So, but what is this? This is basically you select centroids, something called in K means. We are not discussing the K means right now completely. I'm just very high level. I'm trying to tell that you, you select some set of points which is called K. And then the K could one, if you select four K's, 1, 2, 3, 4, right? So basically all the points that are closer to it will become the group. Now if you select 7K, your grouping might be different, right? If you select 6, your grouping might be different. But for your data, what is the correct value of K, right, that we have to come up with, right? And that will be your k nearest neighbor algorithm. So here, the most important thing to understand here is what you are learning is the value of K that is good for your data. So every algorithm, mostly all algorithms will have some optimization process and it will have a configuration value. The best configuration value if you, if you are during training, if you're able to come up with the best configuration value, then you will have a good accuracy or good model. So maybe for this data point, you know, three is, is the best, best configuration. If you go with seven, maybe it won't work, right? When you are actually testing it. So the coming with the best configuration is the very important thing and how to come up with the best. So here do you do a search, do I click on 1C and test, you know, put plug in my test data and see that how did it go? Then I do two and then see how the model is working. You know, do we. Do we manually do it or there is. There is much better process and we are going to see that as well today. So and that is something called a grid search where in every machine learning algorithm there could be some configuration value, configuration parameter and we need to find the best value. And instead of manually trying to identify it, we can focus on some tools. For example, something called as grid search, which is given by Sklearn. And if you use that, it will try to find the best values or you can plug some values and it will actually try to do all the permutations with the values that you give you set up in the code. Right? So two things we learned just now, recap. What is the model in this case in linear regression, you know, Y is = to MXP is a model and all. The second thing we learned is what is parameters and what are values and why are they important? Because the values will change, the model is going to change and the efficiency of the model will, you know, depend on the configuration parameters. Right. The third thing is random forest. Now we just not random forest. We were trying to discuss and please go through this tutorial, go to this blog later. It's really good. You can clearly understand. Of course we are going to post some more videos here. But you can understand much more when you try to interact with this here. This is basically trying to talk about how addition tree is built, right? And if we go somewhere down right here. Yeah. So if you see that here they are trying to use addition to make a prediction. But as you keep adding nodes, adding nodes, adding nodes. So here they got spitting one layer deeper. The tree's accuracy is up to 84%. I was telling last time. Now there could be a root node, right? These are some decision nodes. And then you finally find the leaf node, right? If you leave it here, maybe Your accuracy is 84%. Maybe you go little bit more, right? You go add more nodes, more additions, more conditions, you get 96% right? You add more, right? You might get more, right? So and then you keep adding, keep adding, keep adding. You see how much bigger decision tree has become. It has become 100% accurate. Now the problem here is I was trying to tell Last time. Yes. So what before, before we talk more on it. What, what is the configuration parameter, do you think here in decision tree a simple configuration parameter is depth a configuration parameter? Yes, because how, how deep do you want to go? Right. So again we. We just saw that in K nearest neighbor K was the configuration parameter. Here the depth of decision tree, how much deep do you want to go is a configuration parameter. But the more you go, keep going, you might run into something called. As we were discussing, overfitting because what is the model in addition tree? What is the model in addition tree? It's nothing. But if else rules, it's not a Y is equal to mxp, which we saw in linear regression. It's. It's if else roof. If your elevation, do this condition, check this condition and that how you keep doing that and finally you will arrive at the leaf node which is the. Your prediction final prediction. So here everything is the if else condition and the configuration parameter is the depth. Right? And in random forest you will have multiple decision trees and each decision tree could have its own configuration. Or you can set up a global configuration and then based on the majority voting, you will get a prediction or a regression value. So please take a look at this visualization. This is supplement material. Anyways, we are going to see this more in detail. But let's go to our main agenda today. That is the labs. So what we will do is if you. If you go to lab three, that is machine learning, right? What we have here is introduction. So for this lab, please execute the following notebooks and submit the PDF. Please execute as per the instruction done in class. So these are four notebooks for now. We will try to start with it. Okay, so only. Only click on the first notebook. Don't click on the rest of the notebooks, only click on the introduction to machine learning solutions. We'll do one by one because I need to. I need to clarify how to put the data right, so only focus on introduction to machine learning solutions for now. Okay? Okay, so don't worry about this. I need to take out something just the text here. So don't worry about the text here we start from some code. Basically we try to install Pandas, numpy, matplot, cborn, right? So we install that and we want to start with something called as a scalar. Now what is it's also about scikit Learn frameworks provide structure for data scientists to build code. Frameworks are more than just libraries because it in addition to callable code framework simple as how the code is written. So now Sklearn just let me go there. So this is the, this is excellent library for your machine learning model. So you know, here if you, you could use machine learning algorithms, they already implemented machine learning algorithms. With those algorithms you could build classification models, you could build regression models, right. You could build clustering models, right. So a lot of, lot of available code. Sklearn is greatly used in machine learning community by, for all traditional basic machine learning model, Scikit Learn is the way to go. And we'll be also using it here, right in the course. Okay, so that is Scikit Learn. Now what we want to do is first we want to load data using some of the data. And the data we are trying to load, we are not actually loading from the file as it says here. We are actually loading the data from Sklearn data set. Sklearn can have its own data set. If you say Sklearn data set, Google it. There are some toy data sets available so that we can just plug in the code. We don't have to load CSV and do all those things. We can just use the toy dataset to play around. Now what we do is we load Iris function is basically trying to load the data. And Iris, Iris is a flower data set where basically there are three types of flower go through somewhere here. One of them is Setosa. So there are different types of species of flowers, right? Three, two to three. One one of them is Setosa and the other one is, you know, it's, it's inside the data. But these are all features now you know, the sepal length, the sepal width, petal length, petal width. So based on these features, you, there is a label, right? So this is basically a data set. There are certain features and these, this, the target label is a species. Okay, now what we do is we first load the data, we load the data from the Sklearn data set, right? And then we use Panda data frame. Basically this is not giving as a Pandas data frame. So we create a Pandas data frame using this data and there's something will change. The data loading is always going to change based on how you are loading, whether you're loading from the API, whether you're loading from the, you know, from a SK Learn data set, whether you are loading from a Pandas data frame, whether you are converting Numpy to Pandas. There are a lot of things. And the best thing to do is like, I mean it's pretty obvious we are all using AI, so let's be transparent about it. And you Know you can put whatever the way you want to load it here and it will generate the code for you. Right? And but I really recommend see if you finish a course like you really know numpy, you really know pandas, you know really low visualization then using AI will make things simpler for you. Otherwise it's going to make more complex for you. You first need to know what the tools are you at least at a very high level you should know like what these functions, top level functions do. Play around with those top level functions a little bit. And then to accelerate your coding and not to memorize everything so that you can focus more on the modeling part. That's where you start using a lot of AI. If you just keep using AI am I throw something? You don't know what you're trying to do. AI is not a replacement of knowing the codes or the function or the libraries. It's not. It's just an accelerating tool. So for example, you could write everything in edit place, right? Why do you need a. You can write the Python for Python code here. Why do you need pycharm or Visual Studio? Same thing here. Is that you you you it. It is. The editors are going to accelerate. Similarly I is going to accelerate. So please start using this. Okay. Okay. So here we are loading the data and then we are we are trying to see the shape. So the number of rows dot shape of 0 will give the number of rows and if you remember what we were trying to work with shape when we were trying to deal with pandas and numpy in our previous classes. So I hope you know what is shape. This all is going to just show us what are the features we have. So now let's see what is the question first. So whenever we are working with machine learning or in general data analysis, the first question is how do we load the data? What do we do about it? And we need to determine these things. The number of data points. We have the column names, the data types for each column. This is what we are interested in. So we are trying to address question one. This is our focus when we are dealing with data science or machine learning projects. Right. Once we know those answers, once we know these answers, right then we start with question two, examine the species names and note while they begin RS remove this portion. So now question two is something where we are working on the target variable. What is our target variable? So target variable. Here is, this is all. If you see this is all in floating point. But then we convert that floating point into something like species so here, so whatever the target. Well, there is equivalent of the target variable. So for zero there is setosa for one, there could be something else. Right. So this, this line of. These lines of code are basically so we, when we load data, right, when we load data from here, load iris, it is actually, it will have the setosa the actual string values as well. But the target is basically giving us the numeric value. We don't want the numeric value, we want a string value. So we are putting up some, some code to get this species value out of it. Right, the string value. Because this is the label we require. We don't want to work with zero, zero, you know, the actual numeric values. Instead we want to look at the string value. So this lines of code is nothing but it's just more processing and making sure that we add another column here species. So nothing fancy out there. Okay. Now once we have our target variable, right? Once we have our target variable, what we do is here, I'm just dropping the target variable because we, instead of in place of the target variable, I have the species. So I don't need the numerical values. I'm just letting it go. And you know this 01 various color. This is how we created the species thing. For zero, we are manually saying setosa per one. We are saying various color. So here we are labeling instead of going with numeric values. Okay. And then we see what is the target. Now this is very important in your data analysis. Whenever you are trying to load data, you want to do some kind of analysis where you want to look at what, how your target variables are distributed. So your species, each category, what is the count. So here it's equally distributed. Sometimes it's not equally distributed. And it's called class imbalance problem, right? And you don't want to enter a class in balance problem because we'll see as machine learning can get biased towards the majority of data and there are ways to handle it. Something called as you. Majority sampling, minority sampling or duplicating the minority class to, to ask a question here. Yes, yes, please. So do we check the. The. The value counts only for the target value or for all. It's. It's based on your intuition. But definitely you check for the target variable because that's the, that's where you want to predict. So it basically tells. Suppose you have like you want to. You have your. You have three classes of images, right? One is dogs, one is cats and one is rabbit. And you have 90 images only for cats, right? And you have only let us 90 images for cats, 5 images for rabbit and 5 images for dog. Then your machine learning model will run learn really good on the cat data, but it doesn't know much about the dog and the rabbit. Here also the same thing, looking at the target variable will tell how many, how your data points are distributed. If one of the classes or some of the classes, you don't have data points, your algorithm is not going to learn anything. I'm sorry, I'm running out of power. Just give me a minute. Sa okay, we're back. Yeah, so, so this will just let you know how your target variables are distributed, but I will go through something called an initial data analysis PDF. I'll try to share that. And it goes in much more detail about what are all the activities you should be, what are all the steps you should be doing for your initial data analysis? Right. You have to do something called a univariate analysis, bivariate analysis. We will do that later. But for now the most important thing is the target variable. So then what I'm trying to do here is, I'm trying to look, select just rows desired from the describe method. So here we are, we are looking at, for some of the rows, some of the columns, the descriptive statistics. And it might tell you how your data is, you know, distributed. Once you do that. Now what we can do is we can also look at some more stats here just to understand what is an average of these, your different features. So, and you could do median also if you want. If it's not mean and median. This is all statistics you're applying. You're trying to understand your data till here. And more, more lines of code just to do that. Okay, so once we have all those things, we understand at least a very high level. What is the mean max median of some of the features which you think might be important for your data? Now that's, that's where your data knowledge comes in. You have 7,000, you have like 100 columns. Just looking at 100 columns might not be that much efficient. You, you have some intuition of which columns are good to do data analysis and which are just, you know, useless. So you want to just drop them earlier itself. And then, okay, so here, then what we can do is you can do some. And basically this is to find whether sepal width and sepal length, are they linearly related. So here definitely when you are plotting a scatter plot, it's not, it will show you your two variables are, you know, are linearly related or not. Right. In this case, if you had a straight line, it would be a linear relation. But here it's not a straight line, right? You cannot draw, it won't come as a straight line. This is definitely not a linear relation. Why do we want to find your data is linearly related or not? Because some of the algorithms like logistic regression, linear regression, remember it's a line. So that will work very well for that kind of data. But sometimes when your data is not linear related, you don't want to use this algorithm. Maybe you just go for decision tree or random forest, right? Which is much more suitable for your data. Because it's not, it's not going to fit in a logistic regression. This is the reason you do data analysis. Why are we doing data analysis, even machine learning? Because you need to select the proper machine learning algorithm. Otherwise you're going to waste your time and plugging. Without data analysis, you will choose some wrong algorithms, keep plugging it, you will never get an accuracy because that algorithm is never designed for that data, right? Now that's one thing. And then why does your mean and median these things help? See, sometimes if you, if you look at your mean and your mean is showing you very, you know, some, some, you know, difficult values, right? And it could be because of your outliers, right? So in those courses you want to clean your outliers or you want to depend on another metric like median. And this will help you, okay, Whether your data has lot of noise and if it has a lot of noise, right, a lot of outliers, then maybe simple algorithms like logistic regression or linear regressions won't work and you would have to go for random forest, which is more robust noise. Again, your descriptive statistics, your visualizations are going to intuitively help you select the algorithm. Now every algorithm by nature is different, right? So your, your whole initial data analysis is basically helping you to choose the algorithm, at least a set of algorithms where it will fit, right? So, so that you can use your training time more efficiently. Because whenever you plug in any algorithm, it might be 15, 20 algorithm. Would you train your model on all 20 algorithms? Then it will take for days and you will run out of GPUs. Nobody is going to wait. Both resources are getting waste time is getting waste cost. Everything is going. So you don't want to do that. And sometimes we will have some features where you want to plot and look at the distribution. Now when you look at univariate, basically univariate analysis means only looking at, only looking at a single feature, right? But sometimes you might Want to do bivariate analysis. So bivariate analysis is when you are looking at two features, right? That is bivariate analysis. When you are looking at a single feature like this. This is univariate analysis. And you could do univariate analysis for a set of models. Sorry, a set of. Suppose you select eight columns. You can do one by one, one by one. And I have a notebook in the end I will try to show that that is, that is more focused on the data analysis. But here we are just. I'm keeping it very, very, very simple so that we only focus on one column or two columns like that, right? And then you can plot histogram. Now once you could you. You could also use Seaborn and other, you know, plots to do do, do the histograms. And this is where your univariate and this is your is continuation of that. We saw this plotting the blocks plot. So there is nothing new here. This continuation of this box plots we saw sometimes like how to pair put the pair plots as well, right? So you pair plots will also help you understand side by side how this relationship between different variables exist. So here this is a series of notebooks. The first I'm quickly going it because there is overlap between the notebooks which we're already doing, right? So to my belief, you know, there is a. There is nothing special about this notebook. Most of the things we already kind of know because from our previous labs maybe couple of things are new but most of the things are same. But this is needed for the continuity, which we do. So this is the first notebook that you need to execute. And once you execute just the same thing, you know you need, you need to put a print as PDF and you submit that right? Now let's go to the second notebook. And here we start a little bit on the machine learning side, right? So what we start to do here is we will focus on something called as Orange Telecom churn data. Now what you need to do here is first thing for that one you need to come here. This is the second, this is the link, right? And I'll put that information. I mean I will let me actually put by this notebook which is yeah, this notebook is using this data. So all you need to do is go to that link, right? And you need to download that data, right? And here so let me, let me save this. We save this here and you come here to the schedule data set. Again you need to have a login for that. So you need to download data. So click on download here, right? Click on Download and download the zip file. Download the zip file. It will download. Okay? Once it downloads, extract or. And this is, this is the data Orange Telecom chart. Only Orange Telecom channel data. Okay? Very easy. All you need to go to the resist link, click on download, Download a zip file, it will have a CSV file. Okay, and we all know how to upload the CSV file here, right? We have been doing that. You come here, click on this upload, right? Select the CSV file and it will be uploaded, right? So Orange Telecom churn data, right? And if you click on it, this is data. It has some data on state, account length, all these things, right? And in the end it is basically saying churn, that is the label, that is the target variable. We have churned false or true, right? This telecom data. Okay? Now we have a prediction problem here and we are going to solve it using one of the algorithms which will be very, very high level. We just saw that the K nearest neighbors algorithm, right? So we have this data set and what we are trying to do is supervise machine learning. Now last time I was in the last session we went through what is supervised learning. The most important tools right here is what is your data, what is your data point and why? As long as you have a Y, it's a supervised machine learning algorithm. You're teaching the model that hey, this is the image and this is a label, right? As long as you have a Y, it's always going to be supervised machine learning algorithm. At the moment you don't have a label. It's an unsupervised machine learning algorithm, right? And you get into a different category of problems like you know, something called as clustering, right? And we are going to see that as well later. So here we will be using customer churn data from the telecom industry and the data file is called Orange Telecom Churn data. Load this data to do some processing and use Knarest neighbors to predict customer churn based on account characteristics, right? So here this is all account characteristics and this is basically whether that got churned or not. Okay, so now with that modeling data we start. So first thing is here we are trying to import few libraries. Now see, Sklearn is a framework, it's a machine learning framework. Here you might import from SKlearn something called as k nearest neighbors Kneeboards is a machine learning algorithm. One of the machine learning algorithms and the internal code of the machine learning algorithm is already implemented in the Sklearn framework. You don't have to write anything. You don't have to write Even a single line of code. Now this here, the min max scalar label band riser. What are these things? The Sklearn does not only provide the implementation of machine learning algorithms, it also gives you lot of tools and libraries because it's a framework through which you can build a complete machine learning pipeline. And building a complete machine learning pipeline you might need multiple things. See, you might start with Pandas to load the data, then you might put data into NumPy. These are some tools you might visualize using matplot plate. But the story is not over there. Then you need to give it to the machine learning algorithm. Now when you're making the data ready for the machine learning algorithm, that is where Sklearn gets really helpful. And then when you actually use the transforming the data for machine learning algorithm, again you are going to use Sklearn. Then you are using the actual implementation of the algorithm, also Sklearn. And then once you're training the model, you are using libraries from Sklearn. Then finally after the model is ready, you want to visualize the results, evaluate the results. Again you are using libraries from a scalar. So initially you start with Pandas Numpy exploratory data analysis, you do Seaborn matplotlib. So this is only half of the story. But next half of the story is where the Sklearn journey begins. And it starts with transforming your data to the machine learning model. And as I've always been saying, that your algorithms are not going to check because they are going to remain same. They are a sequence of steps. The sequence of steps will never change. It's a constant math, right? But what is going to change is the data. And every data set is different, the characteristics are different. The way you map these data to these algorithms, that is going to change. And that is where a lot of skill set has to be developed. Right? So we will try to see in this notebook to how to apply this data set to Sklearn here. So first we import some of the Sklearn libraries. So this is called label Binarizer. This is used to transform your data or basically one hot encode. We'll go through what is one hot encoding and then we next import is min maxscaler. Min maxscaler is again a library from Sklearn that is going to help you to rescale your data. Now why do you need rescaling your data? Right? So I was telling in the previous class, sometimes your columns, for example, somebody has, we have a salary column. So your salaries could be very, very Less or it could be somewhere in millions, right? So when you have data that is very, you know, spread across and you start with maybe a couple of hundreds and then it goes to millions. If your data points are spread in these extreme ranges, then taking. When you plug in this type of data to machine learning algorithm, it will never converge. It will have a very hard, you know, it will be having a lot of trouble analyzing your data and conversion, which is basically where it understands the distribution of data. So it's going to be extremely tough. So the best thing to do is rescale your data, which is whatever your data is, bring it to a range between 0 to 1, right? Something like that. And min max scalar is a tool that could help you with rescaling of data. The third tool here is the actual algorithm. What we are trying to import from here is K nearest neighbors. And this here we are importing k nearest neighbors. But we could also import something called Asian tree, random forest or something else. But in this case we are only focused on the K nearest neighborhood service. Okay? So till now the whole focus is using Sklearn, right? Sklearn pre processing. Under pre processing we see label pre processing, min max. And here if you look at this is separate package neighbors. Because this is focused on the algorithm. This, this package is totally focused on the pre processing libraries. And this package is focused only on your algorithm. Basically the K nearest neighborhood. Okay, now begin by importing data. Examine the columns and data. Of course, we want to examine the columns and data. Notice that the data contains the state area code phone number. Do you think these are good features to use when building a machine on a why or why not? Now, whenever you look at your data, there is an intuition, right? Hey, will this help us in predicting the model? You will say that, okay, why? How do you intuitively tell, for example, somebody gives you a cat, dog and a horse, right? You, you try, let's say remove horse, let's say penguin, right? So you immediately know, looking at your images, what will, what is going to help you with distinguishing your categories, right? So for at least if you want to distinguish cat and penguin, a tail is a very important feature, right? A tail. So that feature is extremely important. You want to keep that feature in your data set. Now, if you want to distinguish between cat and dog, then maybe the face is going to be extremely important. You will keep the face. So you might have lot of features in your data set, but which features to keep and which one to throw. Because if you're using unwanted features, then you are Adding noise also while to the prediction model. And you don't want to do that. So first thing is look at the data and intuitively understand which, which features might help you and which might not. Right? And we started, we start there, right? So here you are. Basically we are trying to load once you have loaded the data, right, you are everything is a data and then you do data head. So this is basically going to show you only here the description. And once that is done here data columns is basically showing us only the column names. Now once that is done, notice that some of the columns are categorical data and some are floats, right? These features will need to be numerically encoded using one of the methods. Now what is, what is categorical? What is categorical data? Right, so if you look at. Let's, let's try to take this. Right, so if we look at. Let me print these values a little bit here. Right? So if I go and say where do we have data? And then let me take this, okay, I will take all these columns from here, right? Okay, Take this. And you don't have to make this modification. It's just for explanatory purpose. When you are executing, you can execute the notebook as it is. No changes required. Okay. I just want to show you how the data is looking. And why do we need the transformation? Why do we need this one hot encoding or the transformer Here. Now here, let's just do head and this will only show this column. So here you see, this is a yes or no. Okay? This is not numerical. Okay, then let's do one more head. It's yes or no again John Pauls. Okay, now let's, let's try to see what happens to this data after we label it. You apply this label binarizer. Right? So because we are picking all these columns, we are picking init plan, we are picking voicemail plan, we are looking churn and once we send it to label binarizer, let's see what happens. So this is executed. What we can do is let's now what is happening in this for loop? It's basically looking at each column first. First time this for loop executes, it's going to take this init plan and apply this fit transform. Basically it's applying the encoding to this, this, this whole pandas data column. Second time it will apply to this data column. Third time, third time it's going to apply to this data column. Okay? Now once all these transformations are applied, let's try to see how this got transformed. So I'll say The same thing. We will take all these things, what we're seeing here, right? Okay, so I'm going to take this and I already told, I'm telling all my students and also in this class, please try to use a. There is, there is. I'm not putting any limitations here. Please use AI as much as you can. Because anyways, you're doing presentations, right? Don't, don't stop using AI that your code will be checked. Please use AI make fastest. I want you to really understand how to build the system. I want you to understand how to. You're connecting these dots. That's more important. Your coding part is okay, because the tools have come up. I'm not, we are not, we are not working on a Python course here. So more important is how you build this. Okay, one, two. And let's take this algorithm. Okay. I think so here. After the transformation, let's see what changed. See what happened here? The data was no and yes. It got changed into 0 and 1. Now your question is like okay, why, why are we changing this? Yes and 1 to 0 and 1. See, your data could be in strings and when you plug, when you are pre processing your data, you want to make sure what are we doing in pre processing? Again, in a very high level, we are basically making the data ready for your machine learning algorithm so that you can plug the data to machine learning algorithm. And to do that, remember that machine learning algorithm will not accept strings. It's all numerical data. And when you're playing with numerical data, remember what, what, what is going to help you? The numpy library. So you will do all the data processing and then you put in the matrix X put push these matrices to the machine learning algorithm. In this case it is K means next lab we are going to see addition tree. Later we are going to see random forest. We'll keep playing around with all this data and keep plugging in all these algorithms. Something important is XG boost, extremely important for your machine learning interviews. So we'll plug the data there as well and see how it works over a random forest. We'll compare its evaluation with random forest dictionary and all those things. So coming back here, this also got transformed, right? Let's see this, this also got transformed. So 1, 0, 1 0, simple thing. Okay, now let's see min max scaling. Now min max scaling. Before I do this, let's try to see if I can print my data here. So if I will go here and print data. So data dot head, right? And let me see what is my data so you look at the data. It's 028011, 25, 265 something right here. All this data is like this. Let's go and see once. So here we are creating a min max object scalar. Scalar. To rescale our data, we are creating the object. Once you have the object, you will use that object to fit your data. And what is your data? The whole data frame, the complete data frame, all the columns, all the rows, everything is given. You're fitting everything here and then it will give some data back. We'll see what the data means. So here we'll come here and now it's already executed. So let's see. And now see how did it change. So we have to complete the function here. So if you look at here account length, let's go back, it's all 128, 107. What do you see here? Account length 0.5. You see, you're going to see a lot of values between 0 and 1, between 0 and 1. And this can rescale. You can basically you're trying to put a range and your data should fit in within the range. It cannot just have a lot of variance. So you want bringing all your data between a simple range so that when you give. Now all your data is in zeros, ones, zeros and ones. See, everything is in zeros and ones, right? But here what was your data? Some, some, some numbers are in hundreds, some numbers are in 1 and 0, some numbers are in single digits, right? So your data is like not uniform, right? But here your data is uniform. Something your machine learning algorithms can comprehend and understand. That's the importance of rescaling your data. If you throw this data to the algorithm, right, you, no matter how good you do your rest of your data processing, no matter how you, you know, do your choose your best algorithm. Again, I've been telling this again and again that There are around 10 to 15 traditional machine learning algorithms. We generally play around with and then you have deep learning. But these algorithms are not going to work if you do not choose the right scaling. So that is also little important and we need to pay attention here. So once it is done that the data transformation is done, you can see everything in zeros and ones and we are done with the scaling part. We are still doing data processing. We first transformed and then we do the one hot encoding and then we did the scaling part. Now we go into next is something called as splitting the data set. Till now we only were doing data processing. Now we are getting ready to plug the data into the machine learning algorithm. For here what we do is split the data. And if you look at something called what is this loop doing? Look at this loop. So basically we are trying to tell in this code that from X pick up all the data columns, leave the column if it is churned. So what is the column here? So this is all the columns and this is a churn column. We want to leave the churn column because this is our target variable. This is what we want to predict. This is what we want, want the model to learn to predict. So what we do is we take all the columns except churn and put it in the X columns. And then we are going to take something in the Y data where data churned. This is basically we take all the target variables, target column data points and put it in Y. This single data point here, x data of 0 will map to y data of 0. X data of 1. The label is y date of 1, x data of 2, y data of 2. Right? So this is what we, this is how we split the data. Now once we have a data split, right next what we do is we go and create the object for k nearest neighbors, right here. We are not, as I said, we are not writing even a single line of code for your implementation of machine learning algorithm. We are not building the k nearest neighbor algorithm from the scratch. It's already implemented by Sklearn. You are just using it as a API. However, you are deciding the k value. You are telling the object, hey, create this k nearest algorithm. But the k value is 3. What is k value? I was trying to tell earlier, at the start of the class, k could be 3, k could be 7. Most of your machine learning algorithms will have parameters and you need to come up with the best value. Sometimes the value you come up with might not be the most efficient value. There are tools even available in Sklearn that can help you to identify the best values for your parameters for your specific machine learning algorithm. In this case, the parameter is neighbors and the value is 3. It could be different also. You can try. So now we created a instance of your knee rest neighbor. Once you have the instance of knarest neighbors, what we are trying to do here is we are doing KNN fit. A fit method is basically where you are sending in all the data, you are sending in all the data and you are trying to start building the models. So once the model is built, it's ready, it's trained on your data, it understands your data, it's ready to do the prediction, which is basically you give some X data point and it will able to predict Y. So you give the X and it's going to predict Y. Right? So here we are doing some testing. Now this is not the way you should test the model. This is just a sample code. We'll go into testing more detail because always here we just split the data. We are just putting it in X and Y. But the way to train is you always split your data into something called training data and something called as testing data. And you, you will fit your model only with the training data. And then you, once your model is ready, you'll start testing, testing it with the testing data. But in this case, right, we only have the training data, we don't have the testing data. Okay. And going forward, we'll, we'll do a proper split and we'll see how it works. So here we, once the model is trained, we are going to push X data and then we get Y predictions. Let me try to do one thing. Let me print the X data part here. So X underscore data. I'll just, I'll try to use something with generate AI so that, you know, just to show you that how simple is these things if you don't have to remember all the syntax show only PI data points. Okay. Yeah. So this is five, right? Okay, so this is your data points, right? And I could have done x of 00 as well. Right. So this is of x of 0 and column. The x of 0 is doing all this. We want to have of 1, right? Okay, this is your x of 1. This is a completely one data point up to here. And then I'm going to say Y underscore data. And what is Y underscore data? Sorry, y1 is zero. It's false, basically, if not churn. But this data point, the original for this data point, for all these features, the churned value is 0.0, which basically means that it's not churned. Right. Now if I take something called as now we fed the X data, we fed the Y data. So during K nearest neighbor, it is a supervised machine learning algorithm. We are showing each data point and we were saying that, hey, this is churned. Again for X2 we were saying, hey, this is not churned. Maybe X3 it is churned. But we are teaching in a supervised way. Right? Once the model is ready, what we do is we come down and we say here we again send X data. Now we get Y data. Now what is ypred? Actually when you push everything to the model, all X data points, what is the shape of this? 5,000, right? If I need to do X underscore data dot shape, right? What is this? There's also 5,000, right? These are all 5,000 data points. And remember, and there are 17 features. This is what it means, 5,000 points. Basically, if you count this 1, 2, 3, 4, 5, 6, these all come around 17. So 5,017, that's the dimension. Now these were the X data point, the the X data. What was the labels for them? Original labels? The original labels were Y underscore Y underscore data. And if you do the shape, this is also 5,000, right? Now when you now once you put everything into the model, right, what are we getting? We are getting ypred now how many points you are throwing to the model here to predict. You are basically saying, hey, look at all this X data. So if it is 5,000, how many labels it should have predict. Because it has to predict for each label, it has to for each data point, it has to predict the label whether churned or not. Now how many it will say if you see 5,000. So you're basically 5,000 labels were predicted. Now to tell your model whether it's working good or not, what are we actually doing? We need to compare. So here we basically need to compare head. So this is your original data first five label for the first five data points. These are the original labels, right? And if I want to do Y underscore pred right, what do I see? Hey. This is a NP array. So we can also do that to look at the array. Let's see. Generate the code first file. Okay, this is right. We were working with the indexes, remember? So let's take this. Okay, so now what do we see? We see this is the pandas data frame. Remember this is. This is not pandas. Right? Now we would say why this is not pandas. See, where did it change? See here your this function. This is a function, right? And here you can pass the pandas data frame, right? You can, it allows the input as a pandas data frame. This sklearn knn predict. Basically, if you call classifier knn classifier predict, you can pass the pandas data frame. But the output which you are getting is NPR nd array. It's npr. It's not pandas data frame. That's why it's a different so you can pass a pandas, but the function is implemented in a way that it's going to give you NPR Output. So now, so, so now if you look at it, what do we have? We have data head which is 1, 2, 3, 4, 5. So all the five. The first five labels for this data point were zeros, which is right, not churned. And here the predicted values are also zeros. Only for the first five data point. It means that at least for the first five data points the model is predicting really good. Now you want to find out the accuracy of the complete model. You basically do. You do a comparison of all the Y predicted and the Y original, which is this. And that will tell you how many. What is the accuracy? But let's take a minute here. But that will tell you accuracy. But again, I'm trying to tell you here the way we did it here, right? We are using, we are trying to test the model here. We trained the model, but we are trying to test the model on the same data. That is not right. We have to use testing data. So we will improve our code. But this, because this is starting. I'm just trying to keep it simple, right? Okay, so we got everything done here, right? Now let's try to see what is how to find the total accuracy. I was trying. How do you see? You are basically sending the Y data the actual original points. And you are seeing the Y prediction which is how the model predicted. You have both the data points. So for actually to in classification, if you want to measure accuracy, you need to compare what is was your Y original, the actual labels and what did the model predict the predictable labels. And you compare it and how many were same. The more it is same, the good is accurate, right? So this is the implementation of the accurate simple. And then you print it. It's 90%. So it's definitely model predicted something wrong. Now one thing interesting is you trained the model on X data, right? And you're predicting also on the X data. So you, you showed the model every all of your data points. Each time you hey, this data point is churned. Hey, this data point is not churned. Hey, this data point is churned, right? Each time you're doing that, even after that your, after your model learned everything, again you are showing the same data point. You're not showing something new for even showing the same data point. It's only 90% accuracy. Imagine what will be the accuracy when you show some data point which is which it has not seen in the training. And that is you actually evaluate. So here indirectly, this is you are only testing the training accuracy indirectly because only showing you are doing the evaluation only on the training data. So it's not going to help. But at least if you had testing data then you could could say that, okay, this is, you know, more reliably you could say that this is my accuracy. Okay, okay, perfect. Now what we do is this is done, right? So till now we loaded our data, we had, we were just trying to see what is the intuition, which features will try to help. We tried. So let's actually. Okay, let me actually continue and then we will do a recap here. So we did the accuracy part right now, now we are saying hey, not two, we need three neighbors, right? And we are doing this process again and once we do it, see accuracy 1.0, same thing we did. We call the K nearest neighbors. We fit the data, we did the prediction. Of course we're training. But it's a new model, new model, it's done, retraining is done from scratch. Why did the model accuracy change from 90% to 100%? What was the reason? So here the reason is that because this parameter phi worked well. That's why you have good accuracy. Now for the same, there is something called a speed. We don't have to go and just treat it as some other value parameter P and when you use that again, your accuracy fall to 92. So what does this show? If you have the same machine learning algorithm, if you have done the same data processing technique, if you fit the same data after processing all the data points, you're fitting the same data to the model. Just, just at the end, while you're training your model, you are, you are, you are choosing a different configuration value for your model, right? Your accuracy is going, your metrics is going to be different, right? That's, that's what we want to conclude from this last three blocks here. Very, very important. Even in decision tree, if you choose three decision trees, you will have one accuracy. Sorry, if you choose the depth as 3, you will have one accuracy, depth as 5, one accuracy, depth as 7, one accuracy. When you go to random forest, you choose three decision trees. Because remember, random forest is a collection of decision trees. 3 random forest, 1 accuracy, 5 random forest 1 and 7 random forest as well. Right? Now playing around these values, this is where your experience comes in. So people are going to ask you an interview, how did you train your model? I mean once you see the initial stage is Python, you know how we are doing data analysis, but you actually go and speak, they're going to ask you, okay, you know all these things. Now let's talk about more about experience. So what did you do for your model training? Did you keep plugging in five, four, three, that is how you plugged in the best parameters or did you use something better tools? And this is more interesting. Right? So and then if you say that, hey, I just plugged in some values, definitely it's not the answer they're expecting. So that's where we have grid search, Bayesian search, something different tools that can automate the best value, selecting the best value for each algorithm. Right. Okay. Now once we have all these things, fight the Keras model with different values of core score. So this is a simple for loop. See here what we are trying to do, this is not automation. This is for loop. Here also you're manually, you know, this is not the ideal way, but at least it tells you. Here we are fitting, trying to fit a k value from 1 to 21. We start with 1, we go to 21 and each time we keep getting the accuracy right. And if you see that you got different accuracy, so let it run. It's going to take some time, right? So because how many more can anybody tell me? How many models are we building here? 20, 20 exactly. Thanks. Yeah, that's right. Okay, this ran. Okay. And then let's see C1 and now let's see. If you look at it, we are just plotting. What are we plotting? We are seeing for each k value how the accuracy is changing. Right. So we started with one, right. The accuracy is looks to be good. All right. For one it is like almost one, right. Which is good. As you keep changing, keep changing, the number of case are increasing, the number of case are increasing, your accuracy is completely dropping. So this is at least one conclusion you can make for this data set for k nearest neighbors. So again you don't have to write all these for loops and everything. There are better ways to do it. And we're going to procedure. Okay, so this, this notebook is very, very simple notebook. It does skip some details, but the intention is to keep it very, very, very, very simple. And this is only for to getting, getting started. So this is your first algorithm. You plugged in some data and you are seeing some accuracy values. This, this is, this should be super, super clean and easy to understand. And yeah, and from here we will, we'll take more, you know, a little complexity might increase, but not in today's class. In next week we'll go one by one. So what we'll do is 7:20, we'll take a quick break, 7:30 and we'll come back at 7:30. Okay. I will finish the rest of the laps. Okay, so we are back and let's continue. Okay. Let's reconnect this. Let me make sure that I'm sharing my screen. Okay. Okay. Okay. You should be able to share. Okay. Okay. Now we will work on something called as. Remember I was trying to talk about something. We need to split our data. We need to do cross validation. We haven't done that but we will do that with another algorithm. You know, linear regression and one of the major. So these two things are common with your regression or classification. But the important thing is linear regression is a regression problem. Right. So now what we will trying to do here is you. The first thing, let's focus on the data. So the data you need here is. Let's come here. Let me also put this. And we need to download from here. Let's click on the data part and here ensure that not on mute. Okay, download. All right. And will be downloaded. And once you download you will have the test and train. So you need to extract and you will have test and train. Okay so what you need to do here is once you have the test and train, you need to come here and upload both test and train. Both the test and train. It's not a single CSV file. You're uploading both the test and train. Okay, that's all. Everything else is straightforward. Okay. So now with this what we are trying to do here is we'll be working with a data set based on housing prices. And so it was conqueror propelled for educational use and more and expanded alternative to the well known Boston housing another data set. The version of data set had some missing values filled for convenience. We didn't. I don't think I have done something like that here yet. There are an extended number of features so they've been described in the below. Now the predictor is what are we predicting based on the features? We are predicting the sales price. So based on the property we want to predict sales price. And let's see what are the features available to us. The sales. We have sales type that is one of the feature sale condition or neighborhood street alley. You give all these things and you want to find what is the, you know, what is the sale price of the property. Now this is extremely. If you build something, I mean this data is okay, but if you take some real world data sets and you're trying to predict house prices or how the house prices might change and if this is good, it's going to be really useful in the real estate industry and maybe you know, it becomes a really good app like Zillow. Now what we do here is we first load the data. We import all the. All the processing libraries. If you look at the processing libraries, what do you have? Sklearn. So we are one hot encoding label encode. Now one thing that is different here is we are also importing the library called one hot encoder. Last time we did label encoder. Now we are also using the library called train test split right. And we are using linear regression. We are not using k nearest neighbors, right? And then we are using something called a mean square. Why are we using mean square error? See, accuracy is something to measure the classification models. Error. This is a metric. Mean square is a metric. For this regression models, the metrics are going to change based on your problem, right? Here you have a classification. You want to find accuracy. Here you know how well you are coming out with the value. So we have mean squared error. How far is your value from the value? Original value, right. And in recommendation engine it could be different. If you're building a recommendation engine, how well is your users interacting with your recommendations? Right. The the metrics are going to change. Then another pre processing we are going to take a standard scalar. We know. So these are all scalar. Different types of scalar, different type of scaling techniques. We are loading now once we are done with that, let's import that and did this and we load both the data. So we load the train data set, right? We load the test data set, right? And we basically concat both the test and train. Once we load it and we bring it to a single data frame, right? And we print the shape. So that is done. And then we do the value counts. Once that is done, we can remove this value counts. Now as discussed this particularly we have dealt with data many columns and each column gets encoded correctly. And this is important, we want to encode each column correctly. Now to do that let's. Let's try to see. We did something called as mask. Now let's see now if you. So here if I do. So basically it's trying to tell, you know which is string and which is not string, right? So sales condition. Sales type. If you look at sales type earlier, what is sales type? Type of sale. And we didn't do a data head, right? So let's do. We can do a data head also. So what is sales type? This is. So it's a categorical variable, right? String. So lot area. What is lot area? It's a number. So you say it's not a string, it's false. So basically we are getting true and false. This mask is nothing but giving us true and false values. Okay, now when you do data dot column, dot mask, you are basing this columns and we are getting category value. Let's, let's try to see what does this do. If you see print categorical columns, this is basically giving you an index, right? So here, street, alley, lot shape, this is all the categorical columns. But it's more than that. So let me actually do one thing. So also want to demonstrate a simple thing. Now let's see what does AI tell us about this, this line of code. If you have some problems, always, you know, you don't have to look at a complete documentation with AI available. Looking at the documentation helps. But look, let's take a look at mask. It creates a boolean mass for each column in the data frame. It checks if the data type D is object. The result is a series of boolean values. The true indicate the columns data type object and a false otherwise. So whatever we're checking, we were checking when we had this. Right? Whatever we're checking, we're checking against the object. And if it was object, we were saying false or true, right? So basically object here is basically we're checking whether it's string or not. So mask is basically going to return true or false for each. Now then, it uses boolean mask created in the previous step to select the column names from data dot column. This results in pandas index object contain names of all columns in the data frame that have an object data file. These columns are assumed to be categorical columns. So whatever columns here we got, these are all of dtype object. So these are all string columns. Is there lot area here? Look at lot area. There is no lot area, right? There is no lot area here, see, because it, it eliminated all the numerical columns. So there so how many columns? We have 81 columns, right? It must have eliminated lot of columns for here, right? So yeah. So it just. So what is the, what is the summary here? We just got the categorical columns. This line of code is basically helping us to get the categorical column columns. Why do we need categorical columns? Remember, for categorical columns you have to do encoding. Now whether what type of encoding? We'll see. But you cannot just put the strings or objects into the machine learning code. You have to change those things, right? So let's see those things. Okay? Determine how many extra columns would be created. No need to encode if there is only one value number of unmod one. Okay. Okay. Before we do that, let's try to print the data. So this is saying if you apply these one hot encoding and we'll see what will happen. What happens in one hot encoding? But once you apply one hot encoding your data, the number of columns in your data will increase. So what is your original Data columns here? 81 data columns. Right? But if I apply one hot encoding. Right. The number of columns in my data will increase. And why it will increase? What is the intuition? We will see. So let it there two, not eight. That's fine. Now what we will do is. Let's see this code here we have label encoder and one hot encoder. Okay. We will pick both label encoder one hot encoder int. So if integer here we are encoding it as integer. We are removing the original column. We are encoding it as a one hot encoding. So let's see what happens after one hot encoding. So if I do this here, once this is all done, it will take some time. Okay. If I do beta.head so and I will do a data. So I'll do a data head is here as well. Okay. Did anything change here? So here we had 61. 2. Those things are remaining same. Here we had RL. This RL is same lot front 65. 65. So nothing, nothing got changed. 8, 4, 5 pair nan. Right. All looks same normal. Yeah, definitely. The transformation was not done on the data. It was done where if you carefully look, we copied all the data from data. When you do data copy, you are basically copying all the data frame into this and this ohc. Now once we have the data osc, what we are trying to do is we are doing some kind of encodings. Once those encodings are done, the final data frame we have with the encoding is data hsc. So it's not. We are not applying the encoding directly on the original data frame. We apply the encoding but on a different data frame. That is data oc. Right? So we come here, right? And let's see and look at this. Now the 6060 is okay. What is this changing? RL is getting changed. Where is M? No, where is. Yeah. So everything became numbers. Some. Some things. All these. So if you look at. So let's start with the easy part. Id MD Class 60. What happened to M zoning? It went somewhere. Okay, we need to see what happened to that lot area was numerical. So it remained the lot Frontage is remain same street is. So all these things something. I think we got dropped because we are also Dropping something in the code, I guess. Right? Yeah, we're dropping these columns, something, but we're changing. So if we look at. Let's see, one by one, right, that's much more easier. So we'll take some of the columns here, take some of the corners. You see if this column is still there. It's not there. We'll take something that was utilities is there utilities. So we'll take utilities. Some columns got drop utilities is there and try to take five. Okay. Okay. We don't have. We need to put this. Okay, so what was the original column? All pub. All pub. Right. Does this. Let's also see one more thing. Code. This is all the first five, but we only want to look at the unique values. The all pubnos are nan. Right, and now let's look at this. So how is this changing utilities is not there. Why it's not there. See, utilities got changed into something called utilities_0 utility under 1 utility_2. Right. So basically you're encoding the value all pub is either 1 00. So what I will try to show is how the transformation is happening. It you ask a complex query, even it takes so much time, it's not necessary. That's the problem with AI. Like it's not always. Instead of that, we have utilities of head five and we have unique here. This unique. And if I do. Yeah. What I could do is. What I could do is. So here, if you look at utilities 10 it. Okay, I can just do it. I don't have to use AI. But if it was something like this. Right? So when you apply one hot encoding, right? These are two things. So what is one hot encoding? Nothing. But this is one, right? You make it zero, right? So not exactly that. So, so suppose you had something like. So in your data point you had something like this, right? And then you had. So you have many, many rows, right? Remember you have many rows. This is the column values. Suppose you had something like this, right? Sometimes it's much easier to do manually. Okay, so here, if you apply one hot encoding, what will happen? Right? Since we only have two values, we do One way to do one hot encoding is one way to do encode. This is change into numbers is you assign the ID, right? So you assign ID 1, 2, right? And this data will be changed into this, right? Right. Now there are certain problems we'll discuss what are the problems with this? But there's one way to encode. But when you apply one hot encoding, that is not what the conversion output is the conversion output is something like this. So there are two unique values. The con it will be something like this. So whenever, so it will internally. So you know, 1 and 0. And basically this will become. So this is utilities, right? This will become utilities_0, right? And this will become utilities_1, right? And instead of. And then I'll just copy this right, instead of this, okay? So this will be zero this time. This will be one because, right? This is one hot one. Okay? And whenever you have again, right, this is one hot encoding and this normal encoding. So here we are trying to do one, we are applying one hot encoding, right? So if you go back, I just want to delete this. Okay? So all this data, basically your utilities got turned into utilities 1, utilities 2. And now you can understand what is this encoding. This is. We will, we'll clearly discuss why it is more helpful for machine learning algorithm later. But one is you simply assign some numbers instead of breaking. So when you like like this. And this will not add more columns, right? Sorry, if you, if you did something like this, this is not adding more components, but when you are encoding the same thing like this, you are adding more columns, right? That is the problem with one hot encoding, right? So for here, for many columns, for many categorical columns, objects, you might have done normal, the code might have done normal encoding. And for some it would have done category one hot encoding. We are going to discuss the major difference between that. So for now let's leave it, it's just encoding. Okay? So now once we are done with it, okay, so we have encoded our data, then data encoding is done. Now what we do next is what is our predictor value? Predictor column, that is Y column, right? Y column is our predictor column. So we take the Y predictor column and that sales price. And then because the Y column, right? And all the rest of the columns, we put it in the feature columns, right? So X data is our all feature columns. Y underscore data is the Y column, right? So we have all our data here, ready? Ready. Now we take all this data and pass it to this function. Now this function is called train test split. Now train test split. What it does is it takes your X data, it takes your Y data. And the best easy way to again understand this is go here you can just what what this code means and it will tell you the parameter values. So here with X data, this is data frame or nonp containing your feature data, right? The series of numpy array containing your target variable. The dependent Variable you want to predict Test size is 0.3. This specify the proportion of data that should be allocated to the test set. See, we want to split the data into 70, 30. In this case, that why we are giving 0.3. If it is a 0.2, it means 8020 random state equal to 42. What does random 42 mean? This is used as seed number generator. See, whenever you're splitting your data, it will randomly split the data and it might take a seed and based on that seed, it will split the data. Suppose if you want to have the again the same split for the same function. If you give 42, you might get the same data, same random split, right? So seed is important. Okay, now once we did all this splitting, we split the data, right? We did the trained test split and now you have the one hot encoding. Also what we are we now when the data is split, we are doing the one hot encoding of the one hot encoded data. See here which data did we take? We took the X data. Here we are taking the one hot encoded data. This split is for the original data. This split is for the one hot encoded data, right? Now compare the indexes to ensure they are identical, right? That's fine. You don't have to worry about it. It will be identical. Basically we're seeing the indexes are in both the frames are or not. And then this line of code here, you see this fill in, fill in, don't worry about it. Basically what happened is when I was trying to do something, whatever I was trying to work on this and when I was trying. Let me explain this code. What, what we are trying to do is here, we are building a regression model. That's why you're using a linear regression model model, right? So you're sending your training values and Y values. So if this is your house features, this is how the price is. We are basically training the model. Hey, this is how what are the features of a property? And this is generally the price. You will show maybe thousands of properties and it should learn tomorrow you give new features. Hey, this is a street. There is a. The neighborhood looks like this. It should be able to predict the price of the property. That's what we are trying to gain understand from this model. So we let it, let it fit train and when then we, we again send the train values and it does the train prediction. Then we send test values, it does the Y prediction and we see what is the error. Right? Now again here instead of accuracy, we are focused on the error. More is error. It Means it's doing bad, right? So and so here what we did is, what we did is we directly pushed the data which was not one hot encoded, right? We pushed the X data, not the one hot encoded. Here we are putting the one hot encoded data and we are seeing the error. Now what is the main difference? See, this is just to show you if you don't do your transformations intuitively, your model performance will, will be different. It's not just the model selection, it's also the model configuration. It's also how you do data processing. In data processing, if you apply scaling, if you don't apply scaling, right, There will be difference in your model. If you, if you apply hot encoding. What type of encoding? If you apply encoding, you don't apply encoding. What would be the difference? Right? So there are a lot of things. If you don't choose a set of right values for each step, the, the you know you, you could go wrong. So every in the whole pipeline everything needs to be carefully done. Now let's let. And while we were doing this there were some null values. So it was throwing some errors. So this, this line of code is just putting some null value. So just ignore it. It's not of that much importance here. And once we run this, this is some, this is, you know, for the train. This is the error and the, the test itself. If you apply one out can what is the error? If you don't apply one out doing what is the error? We will focus more on understanding the metrics very very carefully. So for now just understand that we are just applied a function. We got some error value more. Is the error bad? The. The modeling is bad, right? So this is a predictive model here. What we did is. What did we just learn is train split. We didn't learn anything about the cross validation. Let me just make sure there was no cross validation here. There's no cross validation. Okay. Okay, let me miss anything. Okay, this is error. Now it's a continuation here. What so once it is all encoded. Now we want to scale the data, right? So we are going to scale the data. Sometimes you can. You have to scale the data. I remember why last lab I was trying to test tell why we have to scale the data. So we might apply scaling there we just. It was a. Scaling is very easy, right? You just took the Pandas data frame and put it in the min max scalar and scale. It's the most easiest way to do. There are other scaling techniques as well. If you look at the documentation now here what we are trying to do here is get the. So we have two training sets. We don't have only one training set. We have one with one not encoding and one with not coding. I just want to make a note. When you are building your pipeline for the first time, it doesn't have to be so much complicated, right? The previous notebook, which was extremely simple, that is a good tutorial for you to just, you know, plug in your data and build a model. This notebook is little bit one step ahead where we are showing that if you do this way, this is the accuracy. If you do this way, that is the accuracy. And here we are also trying to show there are different types of scaling techniques. One is the standard scalar. You can look at the documentation, how the scaling differs. Sometimes your focus is to bring the range between 0 to 1. Sometimes the focus is to bring the mean and variance, right? That mean between 0 to 1. That, that is. So every scaling technique is little bit different. Different. So here what we are trying to show is there are three different types of scaling and there are two different type of transformations. So basically how many two into three? That is six, right? There's six possible combinations. So you do one hot encoding and you apply this scaling technique. What is accuracy, what is the error? If you take one hot encoding and you apply min max, what is the error? You take one hot encoding max absolute encoding, what is the error? Right. So if you do for all the combinations, we are iterating over all the different data transformations and all the different type of scaling techniques and just to see what is the best error because we don't know what is the best one, right? You have to find the best modeling technique there. It's not one that's a problem in machine learning. It's not one technique that is best suit for your data. You have to do the permutation for your data set. When we look there are going to be different value and we select whatever the best error comes, that is going to be our best configuration. If you look at from the start to top, you could have different configurations for your data processing. You could have different configuration based on the algorithm you are selecting. You could have different configuration for your the parameter values for your algorithm, right? So based on all these things, you might get in the different kinds of accuracies and whatever the best is, you choose that configuration values. Okay, so this is also. You can just go and then do a print and it should be done. Let me just take a look at the last one here. This is a decision tree, right? This Is another. We'll quickly go over it. So this is the final lab in the series. Very quickly I will go. What we are trying to do here is we load the data, right? By now you know all these things. We have some metrics over here. Don't have to worry about precision recall. We have to still discuss warzone matrix means. Instead of KN and linear regression, we are using decision tree and random forest, right? And we have some visualization libraries, right. We load the data set from the Sklearn. It's a bind data set. We select all our features into one data frame called feature columns. Then we are doing a split train test. Split. But the function we are going using is stratified shuffles. It's also shuffle, but a little bit variation. Look at the documentation. Whenever you have question, you can always look at the documentation more clearly. Just copy this guy. Put sklearn, right? It will come up the documentation. Read the documentation more clearly. It has example, right? That's how you become more and more. It takes time. But if you're not reading the documentation and you are just clicking here and then you are like, I don't really want to know. Then in the interview they want to know. So. So it's better to know here because these are the things they might ask. You know, why did you use stratified server? Is there any others Shuffle split? Why to use this shuffle split? In which scenario? These are the questions which shows that you don't know machine learning at Surface. You, you have experience, right? You, you. You are experienced candidate. And experience doesn't have to be always the number of years. It means that the number of models you built and the model, you know, that's experience tool. The number of models, the number of, you know, the projects you did, that's experience. It doesn't matter you are doing it for 10 years or you are doing it for 10 months. If you're doing all those things in 10 months, what you are doing in 10 years, it doesn't matter. There's a number of projects you've done. Okay, so here we. We split again. The X train and X test. And then we do the decision tree classifier. So instead of knee rest neighbor, you use decision tree classifier. And once we use a decision tree classifier, we just are printing our accuracy. These are the measure for accuracy. Don't worry about Precision Recall F1. These are other metrics. I said now what did I miss here? I just want to make sure this is just splitting the data Here we are focusing on creating the X Train Y, train X test Y test so that we can put it in the decision tree classifier. You train the classifier. We, we basically are printing what is the default depth of the decision tree. Why Decision tree has some depth, right. And what is the node count? It prints after training. What is the decision tree? So the decision tree built after fitting the model was 4. That the depth was 4 and the number of nodes in the decision tree was 15. That's something good to know. And this is the performance of the decision tree. Now we say, hey, can we go ahead and do the same thing? We have the X train and Y train data, right? Instead of decision tree, why not use random forest? So we are putting this in the random forest instead of decision tree, right. And then we, then we have. Yeah, then we are measuring that performance results here. So almost similar, you if for your test. If you look at your test, random forest is working fine. More than your decision tree, right? More than your decision. More than your decision tree. You look at 0.96. But Random Forest is becoming much more better. Again, you could always argue, hey, if I had increased the depth or if I had increased the node or something like that, maybe random forest it would be better. We cannot say and I agree, right? It just the default values. Yes, random forest is working better. But if you improve your values in decision trees, maybe you get decision tree itself could get these values. That's why I'm trying to say the configuration matters. Now here you know the, the parameter grid search, what it is trying to do is we are setting different values of the depth, right? And and basically we're change for the decision parameters we are setting. We are saying that use different values of the depth. And we are basically giving a range. And this is the range we are trying to set. And then we are setting something called a grid search, right? And grid search is the tool I was trying to sell. It will automatically whatever values you are going to set. Suppose you say, remember the for loop for k nearest neighbors, which we were doing. We were building 21, 20 models, right? For each K value K1 to 20 and it kept building the model, you wrote a for loop. Instead of doing all that thing, you are setting these values and you are giving it to the grid search it will automatically create. Suppose all these values are like 30 values. 30 decision trees would be automatically made. So you're setting everything right and you're saying what metrics do you want to look? So the metrics is something called the accuracy. And you fit the grid search with the training Data and after everything is run gr best estimator. See if it was trying to build 20 models gr best estimator is basically telling the best model out of 20 decision trees which is the best model which is a. What is the configuration for that? Right. And what is the accuracy for that, right. So if we run this it's going to take some time because it's building all those models, right? So the out of all the models of decision tree the best grid search the depth with the configuration of depth 3 it was best and tree count 13 were best. So trained test cycle is 97.0.8. But there is a catch here. The cash here is if you look above all right here, here our depth was 4 and 15. For 4 and 15 we got 0.9. But here we are saying the best model is depth 3 and 13. So there is something bug here. But I hope you understand what I was trying to say that how the grid search works here. Now we covered couple of things till now. Okay. We started with Pandas, Python Pandas and numpy, right. And then we will matplot Seaborn at Visualization Exploratory data analysis labs. Right. Then we today we started looking into machine learning very basics. SK learn, right. SK Learn data processing tools, right. SK learn, right. Basically data processing comes again. Data processing tools. We have encoding techniques. SK Learn evaluation tools. SK learn ML algorithm implementation. Right. And everything together is basically building your pipeline. So you need to know all those things. And this is a little bit complicated if you're doing for the first time. But as you keep doing it, keep doing you will understand lot of code. You are just reusing it, right? The, the problem area is always this how you map. And we are going to focus on. We are going to say take some real world data sets. Next time it won't be toy data sets. I'm planning to do something, something big. So we will, we'll take a real world data set and we will try to focus what is the noise? What are the issues with much more industry standard data sets and what are the challenges out there. And we'll sit down and in the lab and we'll focus step by step. We'll complete one good project hands on in class if we have the time in the data science. And we'll do that. So one more thing before I, you know, just give me a minute. So just give me a minute. So I just updated one more thing in the end. So here in this lab if anybody dropped it, that's the reason if you have to Watch the recording. So if you dropped out of class, please wait till then or watch the recording. You know, this is the last. I just added it. You. You can go here. The reason I added it, his cross validation is missing. And I want to show that at least we have some code for cross validation. So here I'm going to very go quickly. Don't want to, you know, go again on some things we have done. So here this is doing classification on wine data set from Sklearn. So we have been seeing that importing all these libraries. But one thing is here look at right there's something called PCA we haven't discussed pca. PCA is a feature engineering technique. For this class we haven't talked about feature engineering, so don't worry about feature engineering. But feature engineering is technique where you basically extract some features from your data and you select what features are important. Or you have 17 features, you want to bring it down to 10 features because more features you add to the model is more noise. PCI is a feature reduction technique. It's algorithm and we'll go over the class later. Most of the libraries you have seen here this is classification report and confusion matrix is something new. So let me quickly go over this. Loading the data set we just checking if it's in the memory or not. Checking for null values. Heat map, correlation map is all the block plot for various different type of, you know, columns. Sometimes we have been seeing scatter plots but scatter plots could also be 3D plots. If you want to look at 3D plots, right? Look why 3D plots will be matter. Just focus search more on the 3D plots in which use cases it might be helpful. Here we are taking all the data right and scaling it using standard scalar. We just saw that multiple scalar techniques, scaling techniques. But we here we are only looking at standard scalar putting all the data we getting X scaled right and there is algorithm called pca. We could give PCA and our data and it will give us some data as well. But that is something reduced feature data. It's good to know what it does and we'll focus it later. For now we skip the PCA part completely. See here we are not giving. Once we have the XPCA data we are not focused on. We are. We are only giving the X scale data. So whatever you got after your scaling that data you are giving your. We are not working on the PCA data. It's just for. I wanted to demo something but let's leave that for now. So here we have X scale data, right? This only X scale data. And we are giving to the train test split function we are aware of now. And once we have the train test split function we have something called as pipeline. Now Sklearn also gives you a way to build models that is called pipeline where you will give random class classifier and you can select See. Random forest also has its own configuration and you know, number of trees you want to have, whether you want to 10, 50, 100 trees, what is the depth of the trees. You put all those things right and then you create a pipeline using a skillarn. And here in the grid search you're sending the pipeline, you're setting whatever parameter you have selected for random forest, you're selecting the metric you want to test against your model. But what is the CV equal to 5? That's the reason I opened this notebook. CV equal to 5 means cross validation equal to 5. It means that 5 would do five fold cross validation on this training data set. Now I was explaining cross validation other date basically means that don't show all your data points or if you're 100k data points, don't show 100k data points to the model at once during training split that maybe 20, 20, 20, right? And each time show something and train it and take the test hold out some data set each time look at the slides again, look at cross validation document implementation more. It's extremely important. I'm going to share this video with my big data class as well. And for them there is an announcement here as that you know for your assignment three. This is, this is extremely important you do cross validation. I mean try to do the cross validation and CV equal to 5. There are, there's more code, there is more simple code if you want to look at cross validation. If you have any issues with cross validations, let me know. Now CV equal to 5 basically here means that you know you have to find five cross validations each time you do a split on a subset of data. You're training your model, measuring the accuracy, going for the second subset, you take over the five subsets and measure the accuracy. And final accuracy is more reliable measure of accuracy. Instead of showing all the data once. And this measure will tell you hey, looks like if, if the measure is good, if the average accuracy is good, it means that most likely your model is going to work well in the testing as well. So it's not going to overfit. Overfit means works well in training, doesn't work well in testing. If we get a bad Accuracy here, then overfitting. We understand it's overfitting. So what we can do is immediately we can test with another model, another algorithm, another configuration. Or you can have a set of algorithms and you can have different configuration values for each algorithm in the parent grid. You can create that code. It's very easy to create. And also you can use AI in encouraging students to use AI right. And test different models in cross validation. The model with this grid search the best fit will give you the best model. It could be kns neighbor random forest with different configuration. But the output when you do grid search best estimator it will give the best model that pass the test. And also in the cross validation, you take that and then you can retrain it with the complete data and then do the remaining testing. Right now why do to retrain? See when you're training the model, the cross validation basically you're trying to do is you to approximate whether it will work in testing. But once you know this model is going to work, then train the model on the complete data set. Whatever the best model, train it on the complete data set. You can skip this for the assignment this for the big data class. But in general you have to retrain it. But if you don't want to retrain on the complete data set, that's fine. And then once you have it right, you, you, you take this grid search and you have the best estimator random forest. You can also plot hyper. This thing, this is called where did it go? There was a. There was a graph which was basically going with the number of parameters. Like if you had like five trees, how well was the model converging? If you had 10 trees, how well your model was converging. Once you run it, it will reprint it. And you can see that. And finally there is some confusion matrix and classification report. Confusion matrix basically tells us like okay, it tells you there are three classes, zero, one and two. For these three classes, 12 classes of label zero were actually zero. And it also got predicted as zero. So it means for, for label zero accuracy was really good by, by this model. Also there are 14 labels which were label one and they got also classified correctly as predicted correctly as label one. So for you see the heat map is really good. This is all darker. It means it's almost 100% mentioned here. And this is the classification report. This is more visualization the confusion matrix. There's more on the classification report where we are seeing the accuracy and everything. And Precision Recall F1. We don't know a precision Recall if something we need to discuss. So again this, I hope at least these four notebooks gave you some kind of understanding of the sklearn, the, that the machine learning pipeline, the different concepts. Handson. Right now we are ready with the basics of machine learning. But now in our class, in the data science class, next time when we come, we are going to go deep. We have, we, we, we need to do it. Here is we will go a little bit deeper into. I will show you how these algorithms are getting builded, are getting built and then we'll focus on a real time data science project, right. Where we will take some real world data and whatever we have learned so far and with also the algorithms which will go a little bit deeper how they're built with all that knowledge, we will apply that into a real world data set and that will be our next class lab. So again please, please come. The next class lab is going to be extremely important and very, very, very interesting. So, so come to that class. Don't skip it. And we have only a couple of classes left, so it's in your interest, you know, to come and ask questions. Right. And I will be posting your assignment too and project as well. Yeah. Again, my office hours are extended from Monday to Thursday, 4:30 to 5:25pm Please make use of these office hours and anytime if you have any questions or concerns, just let me. Yeah, Professor, I have a quick question. I saw that you have lab 3 posted due this Saturday. Is that, could you also move that one till next week? Yes, sure. Thank you so much, I appreciate that. No, no problem. Yeah, so I'm going to post it and if you have any issues with this notebook, let me know. Most of all the notebooks are completely executing. If you have. We just saw that. But if there is small errors, that's fine. I'm okay with the partial execution. But all the notebooks we just saw are mostly executing. So I will Change it to 26th. Okay. Any other questions before we go? So for the assignment three we will have, we will have. I mean three or lab three. I'm sorry, I am, I am mixing up with another class. Yeah, we can, you can ask, we can finish this class and I can send you my zoom link and you can ask there. Great. For the assignment 1 then for this class. Okay. Yeah, sure. So this we will, we will learn about IQR first, right? Yes. You are learning about iqr, how to implement iqr? Yes, I mean we will have in our class we will learn first, right? No, so IQR is something it's a very simple algorithm. So I've already put posted a video also there. So if you. Okay, yeah so if you come here. So this is the algorithm and there is a video here. You can go through the video, it almost shows how to do it right. And the IQR is a very, very, you know, popular algorithm. All you need to do is use some Python. You, you're welcome to use AI. The algorithm is right here. Implement IQR using Python also look at the video, it will become clear and if you have any questions, you can let me know. I can help you. Anyone who wants to you can come in my office hours and I can help you with that. Okay. So I, I couldn't make this week's office or except the Monday work was so busy. This work. Is there any possibility that I can talk to you tomorrow? Yeah, yeah, you can send me an email and we can schedule. Thank you so much. No problem. Yeah. Any other questions? Okay then we'll see you next week and yeah, please, please, please attend. Yeah. Thank you. Thank you so much. Have a good evening. Thank you. It. 